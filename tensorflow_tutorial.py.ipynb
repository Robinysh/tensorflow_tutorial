{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n",
      "40.0\n",
      "40.0\n",
      "40.0\n",
      "40.0\n",
      "Tensor(\"zeros:0\", shape=(2, 2), dtype=float32)\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "w = tf.constant(5.0)\n",
    "x = tf.constant(6.0)\n",
    "b = tf.constant(10.0)\n",
    "\n",
    "#Important: c is a computation graph that connects w, x and b,\n",
    "#and outputs w*x + b. \n",
    "#c is not 40. It is a connection between nodes.\n",
    "c = w*x + b \n",
    "with tf.Session() as sess: \n",
    "    print(sess.run(w))  # 5.0\n",
    "    print(sess.run(c))  # 40.0\n",
    "    print(c.eval()) # 40.0\n",
    "    \n",
    "sess = tf.Session()\n",
    "print(c.eval(session=sess))  #40.0\n",
    "print(sess.run(c)) # 40.0\n",
    "\n",
    "t = tf.zeros((2,2))\n",
    "with sess.as_default():\n",
    "    print(t) #Tensor(\"zeros:0\", shape=(2, 2), dtype=float32) \n",
    "    print(t.eval()) # [[ 0. 0.]\n",
    "                    #  [ 0. 0.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constant and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14159\n",
      "[ 9 25]\n",
      "Tensor(\"Const_4:0\", shape=(), dtype=int16)\n",
      "Tensor(\"sub:0\", shape=(), dtype=int16)\n",
      "180\n"
     ]
    }
   ],
   "source": [
    "#0-d tensors\n",
    "mammal = tf.constant(\"Elephant\", tf.string)\n",
    "ignition = tf.constant(451, tf.int16)\n",
    "floating = tf.constant(3.14159265359, tf.float32)\n",
    "\n",
    "#2-d tensors\n",
    "xor = tf.constant([[False, True],[True, False]], tf.bool)\n",
    "cool_numbers  = tf.constant([3.14159, 2.71828], tf.float32)\n",
    "squarish_squares = tf.constant([ [4, 9], [16, 25] ], tf.int32)\n",
    "\n",
    "pi = cool_numbers[0]\n",
    "my_column_vector = squarish_squares[:, 1]\n",
    "ss_shape = tf.shape(squarish_squares) #=> [2,2]\n",
    "\n",
    "print(pi.eval(session=sess))\n",
    "print(my_column_vector.eval(session=sess))\n",
    "\n",
    "print(ignition) #Tensor(\"Const_10:0\", shape=(), dtype=int16)\n",
    "ignition -= 271\n",
    "print(ignition) #Tensor(\"sub_1:0\", shape=(), dtype=int16)\n",
    "print(ignition.eval(session=sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.4 3.3]\n",
      " [1.1 2.2]]\n",
      "[[[0 0 0]\n",
      "  [0 0 0]]]\n"
     ]
    }
   ],
   "source": [
    "#A name has to be given to variables. The name can be used in tensorboard, but for this tutorial the name is just for syntax purposes. \n",
    "#Inputs: get_variable(name, shape=None, dtype=None, initializer=None)\n",
    "w1 = tf.get_variable(\"floatpointfloat\", [2,2],  initializer=tf.constant_initializer([[4.4, 3.3], [1.1, 2.2]]))\n",
    "w2 = tf.get_variable(\"zeros\", [1, 2, 3], dtype=tf.int32, initializer=tf.zeros_initializer)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #Variables have to be initialized before use.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(w1)) #[[4.3, 3.2], [1.1, 2.2]]\n",
    "    print(sess.run(w2)) #[[[0,0,0],[0,0,0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_cant_be_changed = tf.constant(101)\n",
    "i_can_be_changed = tf.get_variable(\"can_be_changed\", [1], dtype=tf.int32, initializer=tf.constant_initializer([101]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101]\n",
      "<tf.Variable 'can_be_changed:0' shape=(1,) dtype=int32_ref>\n",
      "[5]\n"
     ]
    }
   ],
   "source": [
    "assign_op = i_can_be_changed.assign([5])\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(i_can_be_changed))\n",
    "    sess.run(assign_op)\n",
    "    print(i_can_be_changed)\n",
    "    print(sess.run(i_can_be_changed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'assign'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-11eae9e334b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0massign_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi_cant_be_changed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#AttributeError: 'Tensor' object has no attribute 'assign'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'assign'"
     ]
    }
   ],
   "source": [
    "assign_op = i_cant_be_changed.assign([5]) #AttributeError: 'Tensor' object has no attribute 'assign'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# lower level api for creating variable object\n",
    "# mostly used for scalars\n",
    "i_am_zero_variable = tf.Variable(0)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(i_am_zero_variable.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs and Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1., 2., 3.], dtype=float32), array([1., 4., 9.], dtype=float32)]\n",
      "[ 0.  0. 25.]\n"
     ]
    }
   ],
   "source": [
    "# Define a placeholder that expects a vector of three floating-point values, and a computation that depends on it.\n",
    "x = tf.placeholder(tf.float32, shape=[3]) #If the shape is not specified, you can feed a tensor of any shape.\n",
    "y = tf.square(x)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  # Feeding a value changes the result that is returned when you evaluate `y`.\n",
    "  print(sess.run([x, y], {x: [1.0, 2.0, 3.0]}))  # [array([1., 2., 3.], dtype=float32), array([1., 4., 9.], dtype=float32)]\n",
    "  print(sess.run(y, {x: [0.0, 0.0, 5.0]}))  # [0., 0., 25.]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape [3]\n\t [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[3], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\t [[Node: Square/_1 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_5_Square\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'Placeholder', defined at:\n  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/lib/python3.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/usr/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 523, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1758, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/usr/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n    handler_func(fileobj, events)\n  File \"/usr/lib/python3.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python3.7/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/lib/python3.7/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/lib/python3.7/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/lib/python3.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-e012f40a63e9>\", line 2, in <module>\n    x = tf.placeholder(tf.float32, shape=[3]) #If the shape is not specified, you can feed a tensor of any shape.\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\", line 1735, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 4925, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape [3]\n\t [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[3], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\t [[Node: Square/_1 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_5_Square\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape [3]\n\t [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[3], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\t [[Node: Square/_1 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_5_Square\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7bd74b1ec0d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# a `tf.placeholder()` when evaluating a tensor that depends on it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1291\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape [3]\n\t [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[3], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\t [[Node: Square/_1 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_5_Square\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'Placeholder', defined at:\n  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/lib/python3.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/usr/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 523, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1758, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/usr/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n    handler_func(fileobj, events)\n  File \"/usr/lib/python3.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python3.7/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/lib/python3.7/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/lib/python3.7/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/lib/python3.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-e012f40a63e9>\", line 2, in <module>\n    x = tf.placeholder(tf.float32, shape=[3]) #If the shape is not specified, you can feed a tensor of any shape.\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\", line 1735, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 4925, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape [3]\n\t [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[3], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\t [[Node: Square/_1 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_5_Square\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "# Raises `tf.errors.InvalidArgumentError`, because you must feed a value for\n",
    "# a `tf.placeholder()` when evaluating a tensor that depends on it.\n",
    "with tf.Session() as sess:\n",
    "    sess.run(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape () for Tensor 'Placeholder:0', which has shape '(3,)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-4f33f7522bfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# of placeholder `x`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m37.0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1074\u001b[0m                              \u001b[0;34m'which has shape %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[0;32m-> 1076\u001b[0;31m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1077\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape () for Tensor 'Placeholder:0', which has shape '(3,)'"
     ]
    }
   ],
   "source": [
    "# Raises `ValueError`, because the shape of `37.0` does not match the shape\n",
    "# of placeholder `x`.\n",
    "with tf.Session() as sess:\n",
    "    sess.run(y, {x: 37.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight:[[0.9996424]], Bias:[[0.02246104]]\n",
      "Final Loss:8.756192299008703e-11\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define number of data and learning rate\n",
    "n_samples = 1000\n",
    "learning_rate = 2E-5 \n",
    "\n",
    "# Generate some input data\n",
    "x_data = np.linspace(0,40*np.pi, n_samples)\n",
    "f = lambda x: x + 20*np.sin(x/10) \n",
    "y_data = f(x_data)\n",
    "\n",
    "# Reshape it to 1-dimension inputs\n",
    "x_data = np.reshape(x_data, (n_samples,1))\n",
    "y_data = np.reshape(y_data, (n_samples,1))\n",
    "\n",
    "# For logging the lost function\n",
    "loss_log = []\n",
    "\n",
    "# Define placeholders for input\n",
    "X = tf.placeholder(tf.float32, shape=(n_samples, 1))\n",
    "y = tf.placeholder(tf.float32, shape=(n_samples, 1))\n",
    "\n",
    "# Define variables to be learned\n",
    "w = tf.get_variable(\"weights\", (1, 1),\n",
    "    initializer=tf.random_normal_initializer())\n",
    "b = tf.get_variable(\"bias\", (1,1),\n",
    "    initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "# Define structure of the loss\n",
    "y_pred = w*X + b\n",
    "# Mean Squared Error\n",
    "loss = tf.reduce_mean(y - y_pred)**2\n",
    "\n",
    "# Operator to minimize the loss\n",
    "train_opt = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize Variables in graph\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Gradient descent 500 times (500 is arbitrary)\n",
    "    for i in range(500):\n",
    "        # Do gradient descent step\n",
    "        _, loss_val = sess.run([train_opt, loss], feed_dict={X: x_data, y: y_data})\n",
    "        loss_log.append(loss_val)\n",
    "    pred = sess.run(y_pred, feed_dict={X: x_data}) \n",
    "    weight, bias = sess.run([w, b])\n",
    "    print('Weight:{}, Bias:{}'.format(weight, bias))\n",
    "    print('Final Loss:{}'.format(loss_log[-1]))\n",
    "\n",
    "# Plot the loss function\n",
    "plt.plot(np.arange(len(loss_log)),loss_log)\n",
    "plt.title('Linear Regresson Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.figure()\n",
    "# Plot the data and predictions\n",
    "plt.title('Linear Regresson Data and Fit')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.plot(x_data, y_data)\n",
    "plt.plot(x_data, pred)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higher Level Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight:[[0.9753293]], Bias:[1.5501037]\n",
      "Final Loss:1.8384538683080542e-11\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3Xd4FFX3wPHvIQUILfQSCL13iHQUpXfF3kAs2HvB8r4/sYsVFSwoKlZERKUKCKj0Kp1AEkJJAgQICQkh/f7+mOE1YCAh7O7sJufzPHmSnZ2Ze2Ynu2fnzsw9YoxBKaWUOlsJpwNQSinlnTRBKKWUypMmCKWUUnnSBKGUUipPmiCUUkrlSROEUkqpPGmCUACISE8R2eV0HKpoE5F6ImJExP8ClwsVkRQR8XNXbOrfNEEUMyKyV0T6nD3dGLPMGNPUiZjOJiLjRCTT/kBIFJGVItLV6bicICK3iUi2/VqkiEi0iHwhIk0uYB1fisjL7ozTVez/z1O5tjdFRGoZY/YbY8oaY7Lt+f4QkTudjreo0wShHHWeb5I/GGPKAlWApcCPHm7fm6yyX4sKQB/gFLBBRFo5G5bbDLWTwemfOKcDKq40QSgARKSXiMTkerxXRJ4QkS0ikiQiP4hIqVzPDxGRTbm+4bfJ9dzTIhIlIskiskNErsr13G0iskJE3hWRBGDc+eIyxmQB3wIhIlK1gO13EJG/7fZ/tGN/Ofd2ishYETkEfFGA9Y0VkVh7fbtEpLc9vZOIrBeREyJyWETeybXMMBHZbq/vDxFpXtDX9jyvRbYxJsoYcx/wZ+7Xzt7OQ/b6/hKRlvb0McDNwFP2t/HZ+e2js9nbucreloMiMlFEAnM9b0TkHhGJEJHjIjJJRMR+zk9E3hKRoyKyBxic33aeI4b/dU2JyCtAT2CivU0TC7NOVQDGGP0pRj/AXqBPHtN7ATFnzbcWqAVUAnYC99jPdQDigc6AHzDKnr+k/fy19nIlgOuBk0BN+7nbgCzgQcAfKJ1HLOOAb+y/A4HXgaOAf37t2/PvAx4GAoARQAbwcq7tzALG2/OXzmd9TYEDQC17+XpAQ/vvVcCt9t9lgS72303sbe5rx/AUEAkE5vfa5vFa3AYsz2P67cDhsx6Xs2OeAGzK9dyXp7c/17Rz7qM82uoIdLH3Vz073kdyPW+AOUAwEAocAQbYz90DhAN17G1das/vf4H/n/VyLwf8Adzp9PupqP/oEYQ6n/eNMXHGmARgNtDOnn4X8IkxZo2xvtVOBdKxPkQwxvxoL5djjPkBiAA65VpvnDHmA2NMljHm1Dnavk5EErG6U+4CrjHW0UR+7Z/+IHvfGJNpjJmJ9WGcWw7wvDEm3W7/fOvLxvrQbSEiAcaYvcaYKHs9mUAjEalijEkxxqy2p18PzDXGLDLGZAJvYSWibgV4bQsqDusDFwBjzOfGmGRjTDpWgm0rIhXOtXAB9lHueTcYY1bb+2sv8Alw2VmzvW6MSTTG7MdKAqe35zpggjHmgL2trxVg236xj1YSReSXAsyv3EQThDqfQ7n+TsX6lgxQF3g815s4EesbYi0AERmZq7smEWiFdS7htAMFaHu6MSYYqA5sw/oWe9r52q8FxBpjco9CeXZ7R4wxaQVZnzEmEngE60M3XkSmiUgte7k7sI4WwkVknYgMsafXwjqKAcAYk2PHEJKrzXO9tgUVAiTA/7pxXre7jE5gfQuHM1/zMxRgH+Wet4mIzLG7sE4Ar+Yx77m2pxZnvv77yN+Vxphg++fKAsyv3EQThCqMA8Arud7EwcaYIGPM9yJSF/gUeACobH/IbwMk1/IFHkLYGHMUuBsYJyI182sfOIh1viJ3e3XOXm1Bt8eO4TtjTA+sRGKwuqcwxkQYY24EqtnTZohIGaxv93VPr9yOpQ4QW9DtLoCrgGX23zcBw7FOYFfA6o6Bf17zM7a3gPsot4+wuokaG2PKA8+eZ96zHeTM1z+0gMvlR4eh9gBNEMVTgIiUyvVzoVfyfArcIyKdxVJGRAaLSDmgDNab9wiAiIzG+nZaaMaYcGABVl9+fu2vwuoWesA+oTmcc3SdFGR7RKSpiFwhIiWBNKwur9OXWt4iIlXtI4REe13ZwHRgsIj0FpEA4HGsLquVF/M62EcK9UXkA6xzKS/YT5Wz138MCML6hp/bYaBBrscXuo/KASeAFBFpBtx7AWFPBx4SkdoiUhF4+gKWPZ+zt0m5gSaI4mke1gfd6Z9xF7KwMWY9Vr/9ROA41gnY2+zndgBvY31QHwZaAytcEPObwBgRqZZP+xlYJ6bvwPrQvgXrBGp6YbYH6/zD6ZPkh7COFp61nxsAbBeRFOA94AZjTJoxZpfd7gf2ckOxLt3MKOS2d7XbOIF1crY8cIkxZqv9/FdYXTexwA5g9VnLT8E6h5IoIr8UYh89gXWUkoyVTH+4gNg/xUrum4GNwMwLWPZ83gOusa+aet9F61RnkTO7apUqekRkDfCxMeYLp2NRypfoEYQqckTkMhGpYXcxjQLaAL85HZdSvsYX7iJV6kI1xer7LgtEYV0ie9DZkJTyPdrFpJRSKk/axaSUUipPPt3FVKVKFVOvXj2nw1BKKZ+yYcOGo8aYqvnN59MJol69eqxfv97pMJRSyqeISEHuaNcuJqWUUnnTBKGUUipPmiCUUkrlSROEUkqpPGmCUEoplSdNEEoppfKkCUIppVSeNEEopZSPee/3CDbsS3B7O5oglFLKh2yNSeLd33ezPOKY29vSBKGUUj7k7UW7aFg6hds7nbPkuMtoglBKKR+xPvooNSJ/YJ7fY5Rb9Zbb2/PpsZiUUqq4MEd2Ueb723k9YBvZtXpAx9Fub1MThFJKebOsdFj+Luavt6mZHcDyluPoce0jIOL2pjVBKKWUt9q3CmY/DEd3saxkL94IGMXMEcM9khzAjecgRORzEYkXkW25pr0pIuEiskVEfhaR4FzPPSMikSKyS0T6uysupZTyeqcSYfYj8MUAyDrFxp6fMSppDCP7hFHS389jYbjzJPWXwICzpi0CWhlj2gC7gWcARKQFcAPQ0l7mQxHx3KuglFLewBjY/gtM6gQbp0LXB8i5ZxXPbq1O/SpluLpDbY+G47YEYYz5C0g4a9pCY0yW/XA1cHprhwPTjDHpxphoIBLo5K7YlFLK6yTFwPc3wo+joFwNuGsp9H+FObtOEH4omUf6NMbfz7MXnjp5DuJ24Af77xCshHFajD3tX0RkDDAGIDQ01J3xKaWU++Vkw9pPYclLYHKg3yvQ+R7w8ycrO4cJi3bTtHo5hrap5fHQHEkQIvIckAV8e3pSHrOZvJY1xkwGJgOEhYXlOY9SSvmEQ9tg9kMQuwEa9YHBb0PFev97eubfsew5epJPbu1IiRKeOTGdm8cThIiMAoYAvY0xpz/gY4A6uWarDcR5OjallPKIzFPw53hY+QGUCoarp0Crq8+4Oik9K5v3fo+gbe0K9GtR3ZEwPZogRGQAMBa4zBiTmuupWcB3IvIOUAtoDKz1ZGxKKeURUUthzqNwPBra3wJ9X4KgSv+a7Yd1B4hNPMVrI1ojHrqs9WxuSxAi8j3QC6giIjHA81hXLZUEFtkbvNoYc48xZruITAd2YHU93W+MyXZXbEop5XEnj8HC/8Dm76BSQxg1G+pfmuespzKy+WBJJJ3qV6JnY/ePuXQubksQxpgb85g85TzzvwK84q54lFLKEcbAlumw4BlIS4JLn4SeT0BAqXMu8sXKaI4kpzPppg6OHT2A3kmtlFLukxBtdSftWQq1L4Gh70P1Fudd5PjJDD76I4rezarRqf6/u548SROEUkq5WnYmrJoEf7wOJfxh0FsQdgeUyP8+hklLIzmZnsXYgc08EOj5aYJQSilXit0Asx6Gw1uh2RAY9CaUL9g9DAcSUvlq1T6u6VibJtXLuTnQ/GmCUEopV0hPgaWvwJqPoWx1uP4baD70glbx7qLdiMCjfZu4KcgLowlCKaUu1u4FMPdxa7iMS+6A3v8HpSpc0Cp2xJ3g502x3H1pQ2pWKO2mQC+MJgillCqs5MPw21jY/jNUbQ63L4DQzoVa1eu/hVO+VAD39mro4iALTxOEUkpdqJwc+PtrWPRfyEyDK/4D3R4G/8BCrW5F5FH+2n2E5wY1p0LpABcHW3iaIJRS6kIc2Q1zHoF9K6BeTxgyAao0KvTqcnIMr83fSUhwaW7tWteFgV48TRBKKVUQWemwfAIsewsCgmDYRGuojIu8kW3Gxhi2xZ7gvRvaUSrAu8rgaIJQSqn85Cr9SatrYMBrULbaRa82OS2TN37bRYfQYIa19fxw3vnRBKGUUudyKhEWvwDrP4cKoXDzDGjc12Wrn7Q0iqMp6UwZFebokBrnoglCKaXOZgzsnAXznoKT8dD1Abj8WQgs47Im9h07yefLoxnRIYS2dYJdtl5X0gShlFK5JcXAvCdh1zyo0QZumga12ru8mVfn7cTfTxg7wPkhNc5FE4RSSoFV+nPdZ7D4Rbv058vQ+V7wc/3H5MqooyzYfpgn+zelevlzj+rqNE0QSimVu/Rnw94w5J0zSn+6UmZ2Di/M2kHtiqW5o0d9t7ThKpoglFLFV+Yp+PMNWPm+VfpzxGfQ+pqLvnT1fL5YEc2uw8l8cmtHr7us9WyaIJRSxdOeP2D2I1bpz3a3QL+8S3+6UmziKd5dFEGf5tXp37KGW9tyBU0QSqniJTUBFjxnl/5sACNnQYPLPNL0C7O2AzBu2PmLBnkLTRBKqeLBGNj6I/z2tFX6s+cTcOkTEOCZkVMX7TjMwh2HeXpgM2pXDPJImxdLE4RSquhLiIa5j0HUkgKX/nSl1Iwsxs3aTpPqZb3+xHRumiCUUkVXdhasngRLX8tV+vN2KOHZk8Pj54cTl3SKH+/uSoBf/mVHvYUmCKVU0RS70bp09dBWaDrYKv1ZIcTjYazec4ypq/Yxuns9wuq59yS4q7ktlYnI5yISLyLbck2rJCKLRCTC/l3Rni4i8r6IRIrIFhHp4K64lFJFXHoK/PYMfNYbUo5YpT9v/M6R5JCakcVTM7ZQt3IQT/X33jumz8WdRxBfAhOBr3JNexpYbIx5XUSeth+PBQYCje2fzsBH9m+llIckpmawYd9x/t6fyL6EVGKPp3IyPZvMnBwC/UpQqUwgNSqUomn1cjSvWZ6OdStSpqSXdULsXmida0g6AGF3QJ/nL7j0pyuNnx/OgeOp/DCmK6UDvfueh7y4be8aY/4SkXpnTR4O9LL/ngr8gZUghgNfGWMMsFpEgkWkpjHmoLviU0pBSnoWv26KZc7mg6yOPoYx4F9CCKlYmpDg0lQrVwp/PyEtM4fjqRmsiDzKzI2xgDVfh7oV6dO8GsPbhTg7ZETyYevqpO0zoWozu/RnF+fiwaoSd7prqVN93+paOs3T6b/66Q99Y8xBETk9oHoIcCDXfDH2NE0QSrlBfHIaU5ZF893a/SSnZdGgahkevLwR3RpVoW3t4PN+2z1+MoNtcUmsiDzGsogjvDovnNfmh9O9YRVu6RJK3xY18CvhoaGrjbFKfy78j3VX9OX/ge6FL/3pKsdS0nn0h000rFrGJ7uWTvOW48O8/ptMnjOKjAHGAISGhrozJqWKnLTMbKYsj+bDpZGcysxmUOua3NmzAW1rVyhwPYKKZQLp2bgqPRtX5emBzdhzJIVf/o7lp42x3PPNRupWDuL27vW5/pI67h1K4miEdSf0vuVQtwcMnQBVGruvvQIyxvDkjC0knsrky9GdfLJr6TRPJ4jDp7uORKQmEG9PjwHq5JqvNhCX1wqMMZOByQBhYWF5JhGl1L/9vf84j/+4mT1HTtK3RXWeGdiMBlXLXvR6G1Qty2P9mvJQ78Ys3HGYT5ft4flZ2/nkzyge7tOYqzvUxt+Vl3ZmZcCKCfDXmy4t/ekqX67cy5LweF4Y1pIWtco7Hc5F8XSCmAWMAl63f/+aa/oDIjIN6+R0kp5/UMo1cnIM7y2O4IMlEdQoX4qvbu/EpU2qurwdf78SDGpdk0Gta7Iy8ihvLNjF2J+28smfe3h6YDP6tqh+8VXT9q+2Sn8eCYdWV8OA111S+tNVNu4/zmvzwunTvBoju9Z1OpyLJtZ5YTesWOR7rBPSVYDDwPPAL8B0IBTYD1xrjEkQ679mIjAASAVGG2PW59dGWFiYWb8+39mUKraSTmXy6A+bWBIez4gOIYwb1pLypQI80rYxhkU7DvPmgl1ExKdwWZOqPD+0ReGOWtKS4PdxdunPOjD4HWjSz+UxX4z4E2kM+WA5pQL8mPVAd4KDnD0Pcj4issEYE5bvfO5KEJ6gCUKpc9t/LJXbvljL/oRUnh/aglu61HWk7nFmdg5frdrHhEW7ScvK5s6eDXjwikYEBRagA8MY2DnbqvB2Mt4q4HP5s1Dy4rvGXCkjK4cbP13NjrgT/Hx/N5rV8O6upYImCG85Sa2UcqFdh5K5dcoaMrJz+PbOznRuUNmxWAL8SnBHj/oMbVuT8fN38dEfUczeHMerV7U+f1dXUqxd+nMu1GgNN34PId53D60xhudnbWPDvuNMvKm91yeHC+E7g4IopQpk04FErvtkFSIw/e6ujiaH3KqVK8Xb17Vl+t1dCfQvwcjP1/LY9E0cP5lx5ow52bBmMkzqbA2u1/cluOsPr0wOABOXRPL92gM8cHkjhrSp5XQ4LqVHEEoVIdvjkhg5ZQ3BQYF8e2dn6lTyvmGlO9WvxLyHejJxSSQf/xnFn7uO8PywlgxtUxOJ3wGzHoLY9W4v/ekKP64/wNuLdjOiQwiP92vidDgupwlCqSIi6kgKI6espWxJf767q7NX1xwoFeDHE/2bMrhNTZ7+aQtPfr8GvyULGJT8I+Kh0p8X67dtB3lm5lZ6Nq7C6yPaOHJ+x900QShVBBxKSuOWz9YgAt/c6d3JIbfmNcszc2AWKTP+S4WkA/xsLiPtkhe4rlU7/Lz4A/e3bQd54Lu/aVsnmI9u6Uigf9HsrS+aW6VUMZKakcWdX63jxKlMvrq9s0tufvOI1AT45T78vh5OhdIBxF/1IzNDn+OZBXFc9eEKtsUmOR1hnuZsieOB7/6mTe0KfDn6Esp624CFLlR0t0ypYiAnx/DoD5vYEXeCz0aF+cadu/8q/fk4XPok1QJK81Ubw6zNcbw0ZyfDJi7ntm71eaxfE6/4EDbGMGV5NK/M20nH0Ip8MfoSynnonhKnOP+qK6UK7a2Fu1iw/TD/HdKCK5pVdzqc/B3fC3Meg6jFEBIGQ9+DGq3+97SIMLxdCL2aVuON38L5YmU087cd5PmhLejfsoZj/fyZ2Tm8PGcHU1ftY2CrGrx7fTv3jjPlJTRBKOWjFu04zId/RHFjpzrc3r2e0+GcX3YWrP4Qlr5qlf4c+CZccsc5S39WKB3AK1e15uqOtXl25lbu+WYjnepV4tnBzWlXJ9ijocclnuLB7/9mw77j3NmjPs8Oak4JT41W6zC9k1opH3QgIZXB7y8jtHIQM+7p5t3fZmM3WuMnHdoCTQdZdaEvoLpbVnYO09YdYMLvuzmaksHQtrV4ol8T6lYu48agrS6lXzbF8sLsHWRm5fDa1W0Y1rZo3Oegd1IrVUSlZ2Vz/3cbMcCHN3X03uSQnmIdMaz5CMpUg+u+huZDL/jSVX+/EtzSpS5Xtg/hkz+j+HTZHuZuiWNIm1rc26shzWu6/rxLxOFkXpq7k792H6F9aDBvX9vWd07+u5AmCKV8zPj5u9gSk8THt3QktLKXXs66eyHMfRyS9rus9GfZkv483q8pt3apy2fLo/l29T5mbY6jZ+Mq3HBJKH1aVKOk/8Uly4jDyXz85x5+/juGoEB/XhjWklu61PVcASQvo11MSvmQFZFHufmzNYzsWpcXh7fKfwFPS4m3rk7a9hNUaQrD3ndb6c/E1Ay+XrWP79fuJy4pjYpBAfRrUYMrmlejR6MqBa6XHZ+cxtLweGZujGVNdAKB/iUY1bUu9/ZqRKUy3jsi68XQ0VyVKmKSTmUyYMJflA70Y+6DPb2rUtnZpT8vfdIu/VnS7U1n5xiWRx5lxoYY/giPJzk9ixICjauVo1VIBepUKk2N8qUoHehHCRFS0rM4mpzOnqMn2R6XxO7DKQDUqVSamzrV5bqw2lQu6/64naTnIJQqYsbN2k58cjoz7+3mXcnhaKR1Enrfcqjb3bp01YOlP/1KCJc1qcplTaqSkZXDur0JrIlOYEtMIssijhCfnJ7nctXLl6RVrQoMa1uL3s2r06xGuSI5XMbF0AShlA+Yt/UgP/8dyyN9GtPWw5d5nlNWBqx4zy79WQqGfQDtboESzg3QEOhfgu6NqtC9UZX/TcvIyiE+OY30rByycwxlS/pTqUyg957c9yKaIJTycompGfzfr9toHVKB+y9v5HQ4lv1rYPZDVunPliOs0p/lvPNGvUD/Ej4zNpW30QShlJd7dd5Ojqda4ywF+Dk8fFpaEvz+AqyfYpX+vGk6NOnvbEzKbTRBKOXFVkYdZfr6GO65rKHz4yztmPVP6c8u93tl6U/lWpoglPJSaZnZPDtzK3UrB/FIH8+d9P2XpFiY/xSEz/Hq0p/K9TRBKOWlPlgSwd5jqXxzR2dnTqjmZMP6z60upZws6PsidLkP/Ir2CKbqH5oglPJCkfEpfPLnHkZ0CKFH4yr5L+Bqh7dbl67GrIMGl8OQd6FSfc/HoRylCUIpL2OM4YXZ2ykd6Mezg5p7tvHMNOuy1RUTrKExRnwKra/16tKfyn0cSRAi8ihwJ2CArcBooCYwDagEbARuNcZkOBGfUk5atOMwyyKO8n9DWlDFk3f0Rv8Fsx+BhChodzP0exmCKnmufeV1PH7NnIiEAA8BYcaYVoAfcAMwHnjXGNMYOA7c4enYlHJaWmY2L83dQZPqZbm1a13PNJqaAL/cD1OHgsmBkb/ClR9qclCO1aT2B0qLiD8QBBwErgBm2M9PBa50KDalHPPpX3s4kHCKcUNbuv+eB2Ngy48w8RLYMg16PAb3rYIGvdzbrvIZHu9iMsbEishbwH7gFLAQ2AAkGmOy7NligDwriojIGGAMQGhoqPsDVspDYhNPMemPSAa1rkG3Rm4+MX1G6c+OMPTXM0p/KgXOdDFVBIYD9YFaQBlgYB6z5jnMrDFmsjEmzBgTVrVqVfcFqpSHvTpvJwDPDW7hvkays2DlB/BhVziwBga+AXcs0uSg8uTESeo+QLQx5giAiMwEugHBIuJvH0XUBuIciE0pR2zYl8DcLQd5pE9jQoJLu6eRuL9h1kNW6c8mA2HwW1ChtnvaUkWCEwliP9BFRIKwuph6A+uBpcA1WFcyjQJ+dSA2pTzOGMMrc3dSrVxJxlzawPUNZJy0Sn+u/tAu/fkVNB+ml66qfDlxDmKNiMzAupQ1C/gbmAzMBaaJyMv2tCmejk0pJ/y27RAb9yfy+ojWBAW6+C0Zscg615C0H8Juh97PQ2kvGS5ceT1H7oMwxjwPPH/W5D1AJwfCUcoxGVk5jP8tnCbVy3JtWB3Xrfjs0p+jf4O6XV23flUs6J3USjnouzX72HsslS9uuwS/Ei7o8jEG/v7GLv2ZCr2ehR6PeKT0pyp6NEEo5ZCkU5m8tziCbg0r06upC67IOxoJcx6BvcsgtJtV+rNqk4tfryq2NEEo5ZCP/ojieGomzw5qfnG1kM8u/Tn0fWh/q6OlP1XRoAlCKQfEJp7i8xXRXNU+hFYhFQq/ogNrrUtXj+yEllfBgPFeW/pT+R5NEEo5YMKi3QA83q+QXUBpSbD4RVg3BcqHwI0/QNMBLoxQKU0QSnlc1JEUftoYw23d6lO7YtCFr2DnbKv0Z8ph6HIvXP6clv5UbqEJQikPe2fRbkoF+HHf5Q0vbMETcVZiCJ8D1VvDDd9a4ygp5SaaIJTyoO1xSczdcpD7L29Y8FoPOTmwfoqW/lQepwlCKQ96Z+FuypfyZ0zPAh49HN5hl/5ca5f+fAcquWE4DqXyoAlCKQ/ZuP84i8PjebJ/UyoE5fPt/+zSn1dNhjbX6fhJyqM0QSjlIW8t2EWVsoHc1q3e+WeMXmYdNSREQdsbod8rUKayR2JUKjdNEEp5wIrIo6yMOsZ/h7SgTMlzvO1SE2DRf62hMirWh1t/gYaXezZQpXLRBKGUmxljeHPBLmpWKMXNnfOogmiMNaje/LFw6jj0eBQuGwsBbqoLoVQBaYJQys0W74xn04FEXhvRmlIBfmc+eXwfzH0MIn+3LlkdqaU/lffQBKGUG+XkGN5auIu6lYO4pmOu6m3ZWbDmI6uQj5SwSn9ecieU8Dv3ypTyME0QSrnR3K0HCT+UzITr2xHgZw+eF7cJZj8EBzdr6U/l1TRBKOUmWdk5vLtoN02ql2Vo21pnlf6sCtdOhRbD9dJV5bU0QSjlJjM3xrLn6Ek+ubUjflGLYc6jVunPjqOhzzgt/am8niYIpdwgPSub9xZHcGmtHPrtfA62zdDSn8rnaIJQyg2mrdlPt+T5vJrzA7LzlJb+VD4p3wQhIg8A3xpjjnsgHqV8XtrBXbT8fTSjArZjanS1S382dTospS5YQWoS1gDWich0ERkgF1UbUakiLCsD/noT/0970CQnmuhuryK3zdPkoHxWvgnCGPMfoDEwBbgNiBCRV0XkAgez/4eIBIvIDBEJF5GdItJVRCqJyCIRibB/Vyzs+pXyuANrYfJlsORlFud05D+1P6d+v/u1LrTyaQX67zXGGOCQ/ZMFVARmiMgbhWz3PeA3Y0wzoC2wE3gaWGyMaQwsth8r5d3STsDcJ2BKP0g7wa/N3+butAe5a2A3pyNT6qLlmyBE5CER2QC8AawAWhtj7gU6AldfaIMiUh64FOuIBGNMhjEmERgOTLVnmwpceaHrVsqjds6BSZ1h3WfQ+R6Oj17GczvqMKBlDVrXruB0dEpdtIJcxVQFGGGM2Zd7ojEmR0SGFKLNBsAR4AsRaQtsAB4GqhtjDtrrPigi1QqxbqXc7+zSn9d/A7U78vH8nZzMyOKxfk2cjlApl8g3QRhj/u88z+0sZJsdgAeNMWtE5D0uoDtJRMYAYwBCQ/MYGVMpdzmj9Gcm9HkBut4PfgHEn0hj6sq9XNkuhCbVyzkdqVIu4cQD2rWMAAAbDklEQVQZtBggxhizxn48AythHBaRmgD27/i8FjbGTDbGhBljwqpWreqRgJUifid83h/mPQG1O8J9q6z7Guy60JOWRpKZbXi4d2OHA1XKdTx+o5wx5pCIHBCRpsaYXUBvYIf9Mwp43f79q6djU+pfMtNg2VuwfAKULAdXfQJtrj9j/KSY46l8t3Y/14XVpl6VMg4Gq5RrOXUn9YPAtyISCOwBRmMdzUwXkTuA/cC1DsWmlKWApT8/WByJIDx4hR49qKLFkQRhjNkEhOXxVG9Px6LUv6QmwKL/g7+/hor1zlv6M/roSWZsjGFk17rUCtYKcKpo0bGYlDrtdOnP3562kkT3R6zSn4FB51zk3UW7CfQrwX29GnkwUKU8QxOEUmCX/nwcIhdBrQ5w689Qo/V5Fwk/dILZW+K457KGVC2ng/CpokcThCresrNgzcew9BVAYMB46HRXgUp/vrNwN2UD/bn70gbuj1MpB2iCUMXXGaU/B8CgtyC4ToEW3XQgkYU7DvNY3yYEBwW6OVClnKEJQhU/GSfhj9dg1YcQVBmu/RJaXFng0p/GGF6fv5PKZQK5vUd998aqlIM0QajiJfJ3q/Rn4n7oeJtd+vPCBg7+Y/cRVu9J4IVhLSlbUt9CqujS/25VPKQcgQXPwNYfoUoTGD0f6l74iKvZOYbx88MJrRTEjZ10qBdVtGmCUEWbMbDpW1j4H0hPgcuehp6PFbr056+bYgk/lMz7N7Yn0F9rPaiiTROEKrqORVl3Qu9dBqEXX/ozLTObtxfupnVIBYa0runCQJXyTpogVNGTlQEr34c/3wD/UjBkAnQYddHV3b5ZvY/YxFO8cU0bSpTQyruq6NMEoYqWA+usS1fjd1hXJg0cD+VqXPRqk05lMnFpJD0bV6F7oyouCFQp76cJQhUNaSdg8YtWdbfyteDGadB0oMtW//GfUSSmZvL0wGYuW6dS3k4ThPJ94XOtutDJB6Hz3XDFf6yhuV0k5ngqny+P5sp2tWhZS0uJquJDE4TyXScOwvwnYedsqN7qf6U/Xe21+eGIwFMD9OhBFS+aIJTvycmBDZ9bpT+zM6yb3bo+8L/qbq60NjqBuVsO8nDvxjqctyp2NEEo3xK/07p09cAaaNALhrwLldwzWF5OjuHFOdupWaEU91zW0C1tKOXNNEEo31CA0p+uNmNjDNtiTzDh+naUDsx/dFelihpNEMr77V1uHTUci4Q2N0D/V6CMey81TUnP4s0Fu2gfGszwdrXc2pZS3koThPJe/yr9+TM0vMIjTU9aGsmR5HQ+HRmGuPEoRSlvpglCeR9jYPtMmD+2wKU/XSnqSAqfLdvDiPYhtKsT7JE2lfJGmiCUd0ncb5X+jFgItdrDLTOhZhuPNW+M4b+/bKN0gB/PDGrusXaV8kaaIJR3yM6CtZ/AkpexSn++Dp3GFKj0pyvN2hzHyqhjvHRlK60zrYo9TRDKeQc3w6yH4OAmaNwfBr9d4NKfrpR0KpOX5uykbe0K3KS1HpRyLkGIiB+wHog1xgwRkfrANKASsBG41RiT4VR8ygMusvSnq721YBcJJ9P5cvQl+OlorUrhZMWTh4GduR6PB941xjQGjgN3OBKV8ozI3+HDrrDyA2h/CzywFlpe5Vhy2Lj/ON+s2cfIrvVoFaLjLSkFDiUIEakNDAY+sx8LcAUww55lKnClE7EpN0s5Aj/dBd9cDX6BcNs8GPb+BdeFdqW0zGye/HEzNcuX4vF+TRyLQylv41QX0wTgKeD0kJuVgURjTJb9OAYIyWtBERkDjAEIDdV+Yp9hDGz6DhY+55LSn6404fcIoo6c5KvbO1GulOvHc1LKV3k8QYjIECDeGLNBRHqdnpzHrCav5Y0xk4HJAGFhYXnOU5wYY8gxeHef+bEomPMIRP8FdbpYpT+recfIqJsOJDL5ryhuuKQOlzap6nQ4SnkVJ44gugPDRGQQUAooj3VEESwi/vZRRG0gzoHYvFZaZjbr9x5n3d4EtsQksj8hlYNJaZzKzMYYKBVQguDSgYRWCqJx9bK0rFWBbg0rU7dykHN3Amdn/lP60y/QGlivw20XXfrTVU53LVUvX4pnB+s9D0qdzeMJwhjzDPAMgH0E8YQx5mYR+RG4ButKplHAr56OzdsYY1gbncD09TEs3H6I5PQsSgg0qV6OJtXL0atpNYIC/fAvUYKTGVkknMxg79GTzN4cx7dr9gMQElyaQa1rcGX7EFrULO+5ZHFG6c/hMGA8lK/pmbYLaPxv4UTEp/DF6Esor11LSv2LN90HMRaYJiIvA38DUxyOxzHGGH7fGc/EpZFsPpBIuZL+DGhVg0GtaxJWr2K+/eTGGKKPnmRF1DH+3BXPlyv38umyaJpWL8eobvUY0SGEUgFuugEt7QQseQnWfmqV/rzhe2g2yD1tXYSl4fF8sWIvo7rW5fKm1ZwORymvJMb4bjd+WFiYWb9+vdNhuFT4oRO8MGsHq/Yco27lIO7s2YBrOtS+qOGmj5/MYN62g3y/dj/bYk9QqUwgI7vW5fYe9V37zdnNpT9dJT45jYETllG1XEl+ub+7+5KlUl5KRDYYY8LynU8ThHfIys7hwz+ieH9xBGVL+fNY3ybc1CkUfz/X9def7rL6dFk0v+88THBQAPde1pCRXetdXL2DEwdh/lOwcxZUa2ldtlo73/89R2TnGG77Yi3r9iYw+4EeNK7ufQlMKXcraILwpi6mYisu8RT3fbuRTQcSGda2Fi8Ma0nFMoEub0dE6NygMp0bVGZrTBJvLdzFa/PD+XxFNGMHNOOq9iEXdo4iJwc2fAG/j7NKf/Z+Hro96JbSn64y4ffdLIs4yqtXtdbkoFQ+9AjCYev2JnDvNxtIy8zh1RGtGdbWs8Vp1kYn8Mq8nWw+kEhY3YqMG9ayYHcS5y79Wf8y6wqlyt5dlnPB9kPc/fUGrgurzfir22idB1VsaReTD5ixIYZnZm6hdsUgPh3ZkUbVnPlGm5NjmLEhhvG/hXM8NYObOofyZP9mVCidx5FAZhosexuWvwsly0L/V6HtjY4NkVFQkfEpXDlpBQ2rluGHu7vqeQdVrGkXk5f7fHk0L87ZQY9GVZh0c4e8P4w9pEQJ4bpL6tC/VQ3eXbSbr1btZeH2w7w4vBUDWtX4Z8a9y2H2I3AswqoH3f9Vt5f+dIWEkxnc9dV6SvqX4KNbOmpyUKqAvOOOpWLm/cURvDhnBwNa1mDKbWGOJofcKpQOYNywlvx6fw8qly3JPd9s4J6vN3Ak/hDMehC+HGyda7hlJoyY7BPJ4VRGNndMXUdc4ikmj+xIreDSToeklM/QIwgP+/jPKN5ZtJurO9Rm/NWtXXqVkqu0rl2BWQ9059O/oti95Cskaio5koJ0exjp9bTHSn9erOwcw8PT/mbTgUQ+urkDHetWcjokpXyKJggP+nbNPl6fH87QtrV445o2Xj1+UkByDPfFPQd+C4gKaMzIlLFU2NuR104Y6nn/gQPZOYbHp29i4Y7DjBvaggGtvOsubqV8gfd9fS2i5m09yH9+2cYVzarxznVtvTc55GRbBXwmdbHOOfR/jfpjV3PrVUPZFpdEvwl/MXFJBBlZOU5Hek7ZOYYnf9zML5vieLJ/U27rXt/pkJTySXoE4QFbYhJ5bPom2tcJ5sObOxDghd1KgFX6c/bDEPc3NO5nl/4MpQRwY6dQrmhWjRfn7OCthbv5ZVMcr17Vmk71vavbJj0rmyd+3MLszXE83rcJ91/eyOmQlPJZXvpJVXQcSkrjrq/WU7lMSSaPDPPOK2gyUmHhf2Hy5ZAUC9d8ATdNh+Az621UL1+KSTd14IvbLuFURjbXfbKKp3/aQmKqd1SGTTqVyajP1zJ7cxxjBzTjwd6NnQ5JKZ+mRxBulJaZzZ1frSMlLYuf7utGlbLOF8f5l8jFMOdRSNwHHUZB3xfyre52ebNqLHrsUt5bHMFny6JZtOMwYwc24+oOtR3rOttzJIV7vtlA9NGTvHt9W65qX9uROJQqSvQIwo3GzdrOttgTvHdDe5rVKO90OGc6eRRmjoFvRhSq9GdQoD/PDGzOnAd7ULdyEE/N2MKQD5azPOKomwP/t7lbDjJs4gqOJKczdXQnTQ5KuYgeQbjJzI0xTFt3gPsvb0ifFtWdDucfxsDm72HBs3bpz7HQ4zEIKFWo1TWvWZ6f7u3GnC0HGf9bOLdMWUOvplV5rG8T2tQOdnHwZ0o4mcGLs7fzy6Y42ocGM+mmDnqfg1IupENtuEHE4WSGTVxBm9oV+PbOzt5zr4ObS3+mZ2UzdeVeJi6J5ERaFj0bV+H+yxvRuX4ll457lJmdw7R1B3h30W6S0zK5r1cj7r+8EYH+XvI6K+XldCwmh6RlZjNs4nISTmYw96GeVC9fuG/mLpWdCSs/gD/HW91JfcZBx9FuK/2ZnJbJN6v3M2X5Ho6mZNCsRjluuKQOV7YPITio8KPUnkzP4pdNsUz+aw/7jqXSqV4lXryypfd13ynl5TRBOOSVuTv4dFk0U2/vxGVNqjodDsSsh1kPQfx2aD4MBr7hsdKfaZnZzNwYy/dr97M1NokAP6FLg8r0bVGdrg0q07BqWUrkc1I7NSOLlZHHWLTjMPO2HSQ5LYtWIeV5rG8TLm9aTUdkVaoQdLA+B6zec4zPlkdza5e6zieH9GRY/BKsnQzlasIN30GzwR4NoVSAHzd1DuWmzqFsi03i102x/L4znv/7dTsA5Ur606xmOWoFl6ZG+VIE+pdAgBNpWcQnpxFxOIWoIynkGGve3s2rcWvXunQIraiJQSkP0CMIF0lOy2TAhGUE+pdg7kM9CAp0MPeGz4N5T8CJOOg0xir9Wcp7umH2HElh4/5ENh9IZNfhZA4lpXHoRBqZ2TkYA+VK+VO1XEnqVy5Dq5AKhNWrSOf6lfUcg1IuokcQHvbSnB0cTDrFjHu7OZcckg/BvCf/Kf153VdeWfqzQdWyNKhalms66uWoSnkzTRAusCziCNPXx3Bfr4Z0CC3YfQQulZMDG7+EReMgKw16/x90e8irS38qpbyfJoiLdCojm+d+3kaDKmV4yImhHeLD7dKfq6H+pTBkgteX/lRK+QaPJwgRqQN8BdQAcoDJxpj3RKQS8ANQD9gLXGeMOe7p+C7Ue4sj2J+QyrQxXTw7zlJWulX6c9k7VunPKz/yidKfSinf4cRZvyzgcWNMc6ALcL+ItACeBhYbYxoDi+3HXm17XBKfLtvD9WF16NKgsuca3rsCPupu3dfQ8iq4fx20u0mTg1LKpTx+BGGMOQgctP9OFpGdQAgwHOhlzzYV+AMY6+n4Cio7x/DMzK1UDArgmUGuuxv5vE4dh0X/Bxu/skZaveUnaNTHM20rpYodR89BiEg9oD2wBqhuJw+MMQdFpNo5lhkDjAEIDQ3NaxaP+G7NPrbEJPHeDe0u6u7gAjEGtv8M88dC6jHrBHSvpyGwjHvbVUoVa44lCBEpC/wEPGKMOVHQG5+MMZOByWDdB+G+CM/t+MkM3lq4m24NKzOsbS33NpZ4AOY+DhELoGY7uGUG1Gzr3jaVUgqHEoSIBGAlh2+NMTPtyYdFpKZ99FATiHcitoJ4a+EuUtKzeH5oS/fd0ZuTbd0Fvfgl63H/16yb3vz0wjOllGc4cRWTAFOAncaYd3I9NQsYBbxu//7V07EVxLbYJL5bu59RXevRtEY59zRycAvMfsgq/dmoLwx551/V3ZRSyt2c+DraHbgV2Coim+xpz2IlhukicgewH7jWgdjOyxjDC7O3UzEokEf7NnF9Axmp8OfrsHIiBFWCaz6HliP06iSllCOcuIppOXCuT7zenozlQs3aHMe6vcd5fURrKpR28V3KZ5T+HAl9XrCShFJKOUQ7tAvoZHoWr80Lp3VIBa4Nq+PCFR+1qrtt+QEqN4JRc6B+T9etXymlCkkTRAFN/msPh06kMenm9vjlU8OgQIyBzdPs0p/JcOlT0PPxQpf+VEopV9MEUQDxJ9L4dNkeBreuSce6Luj2ORZldSdF/wl1OtulP5tf/HqVUsqFNEEUwLu/R5CZncNTA5pe3IrOLv05+B23lv5USqmLoQkiHxGHk/lh3X5Gdq1H3coXcedyzAbr0tXD26D5UBj4psdKfyqlVGFogsjH6/PDKRPoX/ihvNOTYcnLsOYTq/Tn9d9C8yGuDVIppdxAE8R5rIo6xuLweMYOaEalMoUYb2nXfGuYjBNx0OkuuOK/XlX6UymlzkcTxDnk5Bhem7+TWhVKMbp7vQtbOPkQzH8KdvwK1VrAtVOhziVuiVMppdxFE8Q5zN4Sx5aYJN65rm3BCwGdXfrziv9aI6/6u3m0V6WUcgNNEHnIyMrhrYW7aFGzPFe2CynYQkd2WaU/96/S0p9KqSJBE0Qepq8/wIGEU3wxuhUl8rspLivdKvu57G2r9OfwD7W6m1KqSNAEcZa0zGw+WBJBWN2K9GpS9fwz71tpHTUc3Q2tr4P+r0LZfJZRSikfoQniLF+v2sfhE+m8f0P7c9d6OHUcFj0PG6daw3Df/BM01tKfSqmiRRNELinpWXz0ZxQ9G1ehc4PK/57hjNKfR6Hbg9DrGS39qZQqkjRB5PL58mgSTmbwRL88htRIPADznoDdv1klP2/+EWq183yQSinlIZogbImpGXz61x76tahO2zrB/zxxRulPY51n6HS3lv5UShV5+iln+/jPPaRkZPF47qOHQ1th1kMQt9Eq/Tn4bahY17kglVLKgzRBAPHJaXy5MpphbWtZdaYzUq0RV1d+YFV1u3oKtLpaL11VShUrmiCAD5dGkZlteLRPE4haYtVqOL4X2t8KfV/U0p9KqWKp2CeImOOpfLtmH6PblaHeX4/Blmla+lMppdAEwfu/7+YqWcYze6ZBxgm49Eno+YSW/lRKFXvFOkHsj9zGsK3308N/G1TpZJX+rN7C6bCUUsorFM8EkZ0JqyZSY/GrVBQ/knuPp1z3MVr6UymlcvG6T0QRGSAiu0QkUkSedksjm76D38exOKstX3ecTrme92hyUEqps3jVEYSI+AGTgL5ADLBORGYZY3a4tKF2N/Pu2pN8frghy/t0demqlVKqqPC2r82dgEhjzB5jTAYwDRju6kY2xibz3r663H1pAyoEBbh69UopVSR4W4IIAQ7kehxjT/sfERkjIutFZP2RI0cK3VDPxlUY3b1+oZdXSqmiztsSRF63KpszHhgz2RgTZowJq1q1cLUXOoRW5Os7OlOmpFf1sCmllFfxtgQRA9TJ9bg2EOdQLEopVax5W4JYBzQWkfoiEgjcAMxyOCallCqWvKqPxRiTJSIPAAsAP+BzY8x2h8NSSqliyasSBIAxZh4wz+k4lFKquPO2LiallFJeQhOEUkqpPGmCUEoplSdNEEoppfIkxpj85/JSInIE2FfIxasAR10YjhN8fRt8PX7w/W3w9fjB97fBifjrGmPyvdPYpxPExRCR9caYMKfjuBi+vg2+Hj/4/jb4evzg+9vgzfFrF5NSSqk8aYJQSimVp+KcICY7HYAL+Po2+Hr84Pvb4Ovxg+9vg9fGX2zPQSillDq/4nwEoZRS6jw0QSillMpTsUwQIjJARHaJSKSIPO10PPkRkToislREdorIdhF52J5eSUQWiUiE/bui07Gej4j4icjfIjLHflxfRNbY8f9gD/HutUQkWERmiEi4vS+6+uA+eNT+H9omIt+LSClv3g8i8rmIxIvItlzT8nzNxfK+/b7eIiIdnIv8H+fYhjft/6MtIvKziATneu4Zext2iUh/Z6K2FLsEISJ+wCRgINACuFFEWjgbVb6ygMeNMc2BLsD9dsxPA4uNMY2BxfZjb/YwsDPX4/HAu3b8x4E7HImq4N4DfjPGNAPaYm2Lz+wDEQkBHgLCjDGtsIbUvwHv3g9fAgPOmnau13wg0Nj+GQN85KEY8/Ml/96GRUArY0wbYDfwDID9vr4BaGkv86H9meWIYpcggE5ApDFmjzEmA5gGDHc4pvMyxhw0xmy0/07G+mAKwYp7qj3bVOBKZyLMn4jUBgYDn9mPBbgCmGHP4u3xlwcuBaYAGGMyjDGJ+NA+sPkDpUXEHwgCDuLF+8EY8xeQcNbkc73mw4GvjGU1ECwiNT0T6bnltQ3GmIXGmCz74Wqs6plgbcM0Y0y6MSYaiMT6zHJEcUwQIcCBXI9j7Gk+QUTqAe2BNUB1Y8xBsJIIUM25yPI1AXgKyLEfVwYSc71JvH0/NACOAF/Y3WSfiUgZfGgfGGNigbeA/ViJIQnYgG/tBzj3a+6r7+3bgfn23161DcUxQUge03ziWl8RKQv8BDxijDnhdDwFJSJDgHhjzIbck/OY1Zv3gz/QAfjIGNMeOIkXdyflxe6rHw7UB2oBZbC6Zc7mzfvhfHztfwoReQ6rC/nb05PymM2xbSiOCSIGqJPrcW0gzqFYCkxEArCSw7fGmJn25MOnD6Ht3/FOxZeP7sAwEdmL1aV3BdYRRbDd1QHevx9igBhjzBr78QyshOEr+wCgDxBtjDlijMkEZgLd8K39AOd+zX3qvS0io4AhwM3mnxvSvGobimOCWAc0tq/cCMQ6ITTL4ZjOy+6vnwLsNMa8k+upWcAo++9RwK+ejq0gjDHPGGNqG2PqYb3eS4wxNwNLgWvs2bw2fgBjzCHggIg0tSf1BnbgI/vAth/oIiJB9v/U6W3wmf1gO9drPgsYaV/N1AVIOt0V5W1EZAAwFhhmjEnN9dQs4AYRKSki9bFOuK91IkYAjDHF7gcYhHXlQBTwnNPxFCDeHliHmVuATfbPIKx+/MVAhP27ktOxFmBbegFz7L8bYP3zRwI/AiWdji+f2NsB6+398AtQ0df2AfACEA5sA74GSnrzfgC+xzpfkon17fqOc73mWN0zk+z39Vasq7W8dRsisc41nH4/f5xr/ufsbdgFDHQydh1qQymlVJ6KYxeTUkqpAtAEoZRSKk+aIJRSSuVJE4RSSqk8aYJQSimVJ00QSiml8qQJQimlVJ40QSjlQiJyiT3GfykRKWPXXmjldFxKFYbeKKeUi4nIy0ApoDTW+E2vORySUoWiCUIpF7PH+FoHpAHdjDHZDoekVKFoF5NSrlcJKAuUwzqSUMon6RGEUi4mIrOwhjWvD9Q0xjzgcEhKFYp//rMopQpKREYCWcaY7+xawitF5ApjzBKnY1PqQukRhFJKqTzpOQillFJ50gShlFIqT5oglFJK5UkThFJKqTxpglBKKZUnTRBKKaXypAlCKaVUnv4faHeNB7y7R0IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_samples = 1000\n",
    "learning_rate = 2E-2\n",
    "\n",
    "x_data = np.linspace(0,40*np.pi, n_samples)\n",
    "f = lambda x: x + 20*np.sin(x/10) \n",
    "y_data = f(x_data)\n",
    "\n",
    "x_data = np.reshape(x_data, (n_samples,1))\n",
    "y_data = np.reshape(y_data, (n_samples,1))\n",
    "\n",
    "loss_log = []\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(n_samples, 1))\n",
    "y = tf.placeholder(tf.float32, shape=(n_samples, 1))\n",
    "\n",
    "# Create an object of dense layer \n",
    "dense = tf.layers.Dense(1) # 1 for output dimension\n",
    "y_pred = dense(X)\n",
    "loss = tf.reduce_mean(y - y_pred)**2\n",
    "\n",
    "# ADAM Optimizer: a more advanced version of gradient descent\n",
    "train_opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(500):\n",
    "      _, loss_val = sess.run([train_opt, loss], feed_dict={X: x_data, y: y_data})\n",
    "      loss_log.append(loss_val)\n",
    "    pred = sess.run(y_pred, feed_dict={X: x_data}) \n",
    "    weight, bias = sess.run([dense.kernel, dense.bias])\n",
    "    print('Weight:{}, Bias:{}'.format(weight, bias))\n",
    "    print('Final Loss:{}'.format(loss_log[-1]))\n",
    "\n",
    "plt.plot(np.arange(len(loss_log)),loss_log)\n",
    "plt.title('Linear Regresson Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.figure()\n",
    "\n",
    "plt.title('Linear Regresson Data and Fit')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.plot(x_data, y_data)\n",
    "plt.plot(x_data, pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minibatch and rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Loss 0.151392\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum from RNN [[15.100087]]\n",
      "True sum 15\n",
      "Sum from RNN [[12.841793]]\n",
      "True sum 16\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def generate_data(amount):\n",
    "    x = np.random.uniform(low=-10, high=10, size=(amount,5,1))\n",
    "    y = np.sum(x, 1)\n",
    "    return x, y\n",
    "\n",
    "def read_batch(stream, batch_size):\n",
    "    head = 0\n",
    "    while True:\n",
    "        batch = []\n",
    "        for item in stream:\n",
    "            batch.append(item[head:head+batch_size])\n",
    "        yield batch\n",
    "        head += batch_size\n",
    "        head %= len(stream[0])-batch_size\n",
    "\n",
    "class SumRNN(object):\n",
    "    def __init__(self):\n",
    "        self.lstm_size = 32\n",
    "        self.learning_rate = 0.01\n",
    "        self.data_amount = 10000\n",
    "        self.batch_size = 100\n",
    "        self.loss_logs = []\n",
    "        self.input_data, self.target_data = generate_data(self.data_amount)\n",
    "        self.create_model()\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def create_rnn(lstm_size, data):\n",
    "        #Create a LSTMCell with hidden size lstm_size\n",
    "        rnn_cell = tf.contrib.rnn.LSTMCell(lstm_size) \n",
    "        #Get the zero state of rnn_cell\n",
    "        initial_state = rnn_cell.zero_state(tf.shape(data)[0], tf.float32)\n",
    "        #Create a rnn with the cell.\n",
    "        #Dynamic means the input length can be a variable\n",
    "        outputs, state = tf.nn.dynamic_rnn(rnn_cell, data, \n",
    "                                           initial_state=initial_state,\n",
    "                                           dtype=tf.float32)\n",
    "        #Get the last output only and pass it through a dense layer\n",
    "        outputs = tf.layers.dense(outputs[:,-1], 1)\n",
    "        return outputs\n",
    "\n",
    "    def create_placeholders(self):\n",
    "        self.inputs = tf.placeholder(dtype=tf.float32, shape=[None, None, 1])\n",
    "        self.targets = tf.placeholder(dtype=tf.float32, shape=[None, 1])\n",
    "        \n",
    "    @staticmethod\n",
    "    def calculate_loss(predictions, targets):\n",
    "        #L2 loss\n",
    "        loss = tf.reduce_mean(tf.square(predictions-targets))\n",
    "        return loss\n",
    "    \n",
    "    def create_model(self):\n",
    "        self.create_placeholders()\n",
    "        self.outputs = self.create_rnn(self.lstm_size, self.inputs)\n",
    "        self.loss = self.calculate_loss(self.outputs, self.targets)\n",
    "        self.train_opt = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
    "        \n",
    "    @staticmethod\n",
    "    def plot_results(loss_logs):\n",
    "        plt.plot(np.arange(len(loss_logs)), loss_logs)\n",
    "        plt.title('Loss')\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.show()\n",
    "        \n",
    "    def train(self):\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            #Create a iterator that feed minibatches\n",
    "            batch_feeder = read_batch([self.input_data, self.target_data], self.batch_size)\n",
    "            for _ in range(500):\n",
    "                batch_inputs, batch_targets = next(batch_feeder)\n",
    "                feed_dict = {self.inputs: batch_inputs,\n",
    "                             self.targets: batch_targets}\n",
    "                loss, _ = sess.run([self.loss, self.train_opt], feed_dict=feed_dict)\n",
    "                self.loss_logs.append(loss)\n",
    "            print('Final Loss', loss)\n",
    "            #The noisy loss comes from the minibatch\n",
    "            self.plot_results(self.loss_logs)\n",
    "\n",
    "            #Test the rnn\n",
    "            self.infer(sess, [1,2,3,4,5])\n",
    "            #The rnn does not generalize well to lengths other than 5\n",
    "            self.infer(sess, [1,2,3,4,5,1])\n",
    "\n",
    "                \n",
    "    def infer(self, sess, test_input):            \n",
    "            test_input = np.reshape(test_input, (1,len(test_input),1))\n",
    "            print('Sum from RNN', sess.run(self.outputs, feed_dict={self.inputs:test_input}))\n",
    "            print('True sum', test_input.sum())\n",
    "            \n",
    "model = SumRNN()\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-275c20351226>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'True sum'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSumRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-275c20351226>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_amount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_amount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-275c20351226>\u001b[0m in \u001b[0;36mgenerate_data\u001b[0;34m(amount)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial)\u001b[0m\n\u001b[1;32m   1928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1929\u001b[0m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[0;32m-> 1930\u001b[0;31m                           initial=initial)\n\u001b[0m\u001b[1;32m   1931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "#TODO\n",
    "###\n",
    "# Calculates the sum of a list with variable length\n",
    "###\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def generate_data(amount):\n",
    "    lengths = np.random.randint(low=1, high=10, size=amount)\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in lengths:\n",
    "        x.append(np.random.uniform(low=-10, high=10, size=(i,1)).tolist())\n",
    "        y.append(np.sum(x))\n",
    "    return x, y\n",
    "\n",
    "def generate_shuffle_key(length):\n",
    "    shuffle_key = np.arange(length)\n",
    "    np.random.shuffle(shuffle_key)\n",
    "    return shuffle_key\n",
    "\n",
    "def pad_data(item):\n",
    "    \"\"\"\n",
    "    Pads a 2d array with zeros.\n",
    "    Input:\n",
    "    [[1,2],\n",
    "     [3,4,5,6],\n",
    "     [7,8,9]]\n",
    "    Output:\n",
    "    [[1,2,0,0],\n",
    "     [3,4,5,6],\n",
    "     [7,8,9,0]], (2,4,3)\n",
    "    \"\"\"\n",
    "    lengths = [len(i) for i in item]\n",
    "    maxlen = max(lengths)\n",
    "    x = np.zeros([len(item), maxlen], dtype=np.int32)\n",
    "    for i, x_i in enumerate(x):\n",
    "        x_i[:lengths[i]] = item[i]\n",
    "    return x, lengths\n",
    "\n",
    "\n",
    "    \n",
    "def read_batch(stream, batch_size):\n",
    "    head = 0\n",
    "    stream[0] = pad_data(stream[0])\n",
    "    while True:\n",
    "        batch = []\n",
    "        for item in stream:\n",
    "            batch.append(item[head:head+batch_size])\n",
    "        yield batch\n",
    "        head += batch_size\n",
    "        if head+batch_size > len(stream[0]):\n",
    "            head = 0\n",
    "            shuffle_key = generate_shuffle_key(len(stream[0]))\n",
    "            for i, _ in enumerate(stream):\n",
    "                stream[i] = stream[i][shuffle_key]\n",
    "                \n",
    "\n",
    "class SumRNN(object):\n",
    "    def __init__(self):\n",
    "        self.lstm_size = 32\n",
    "        self.learning_rate = 0.01\n",
    "        self.data_amount = 10000\n",
    "        self.batch_size = 100\n",
    "        self.input_data, self.target_data = generate_data(self.data_amount)\n",
    "        self.create_model()\n",
    "    \n",
    "        self.loss_logs = []\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_rnn(lstm_size, data):\n",
    "        rnn_cell = tf.contrib.rnn.LSTMCell(lstm_size)\n",
    "        initial_state = rnn_cell.zero_state(tf.shape(data)[0], tf.float32)\n",
    "\n",
    "        # 'state' is a tensor of shape [batch_size, cell_state_size]\n",
    "        outputs, state = tf.nn.dynamic_rnn(rnn_cell, data,\n",
    "                                       initial_state=initial_state,\n",
    "                                       dtype=tf.float32)\n",
    "        outputs = tf.layers.dense(outputs[:,-1], 1)\n",
    "        return outputs\n",
    "\n",
    "    def create_placeholders(self):\n",
    "        #self.inputs = tf.placeholder(dtype=tf.float32, shape=[self.batch_size, None, 1])\n",
    "        self.inputs = tf.placeholder(dtype=tf.float32, shape=[None, None, 1])\n",
    "        self.targets = tf.placeholder(dtype=tf.float32, shape=[None, 1])\n",
    "        \n",
    "    @staticmethod\n",
    "    def calculate_loss(predictions, targets):\n",
    "        loss = tf.reduce_mean(tf.square(predictions-targets))\n",
    "        return loss\n",
    "    \n",
    "    def create_model(self):\n",
    "        self.create_placeholders()\n",
    "        self.outputs = self.create_rnn(self.lstm_size, self.inputs)\n",
    "        self.loss = self.calculate_loss(self.outputs, self.targets)\n",
    "        self.train_opt = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
    "        \n",
    "    def plot_results(self):\n",
    "        plt.plot(np.arange(len(self.loss_logs)),self.loss_logs)\n",
    "        plt.title('Loss')\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.show()\n",
    "        \n",
    "    def train(self):\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            batch_feeder = read_batch([self.input_data, self.target_data], self.batch_size)\n",
    "            for _ in range(500):\n",
    "                batch_inputs, batch_targets = next(batch_feeder)\n",
    "                feed_dict = {self.inputs: batch_inputs,\n",
    "                             self.targets: batch_targets}\n",
    "                loss, _ = sess.run([self.loss, self.train_opt], feed_dict=feed_dict)\n",
    "                self.loss_logs.append(loss)\n",
    "            print(loss)\n",
    "            self.plot_results()\n",
    "\n",
    "            self.infer(sess, [1,2,3,4,5])\n",
    "            self.infer(sess, [1,2,3,4,5,1])\n",
    "\n",
    "                \n",
    "    def infer(self, sess, test_input):            \n",
    "            test_input = np.reshape(test_input, (1,len(test_input),1))\n",
    "            print('Sum from RNN', sess.run(self.outputs, feed_dict={self.inputs:test_input}))\n",
    "            print('True sum', test_input.sum())\n",
    "            \n",
    "model = SumRNN()\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BROKEN\n",
    "#TODO\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" A clean, no_frills character-level generative language model.                                                                                                                          \n",
    "\n",
    "CS 20: \"TensorFlow for Deep Learning Research\"\n",
    "cs20.stanford.edu\n",
    "Danijar Hafner (mail@danijar.com)\n",
    "& Chip Huyen (chiphuyen@cs.stanford.edu)\n",
    "Lecture 11\n",
    "\"\"\"\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import random\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def safe_mkdir(path):\n",
    "    \"\"\" Create a directory if there isn't one already. \"\"\"\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "def vocab_encode(text, vocab):\n",
    "    return [vocab.index(x) + 1 for x in text if x in vocab]\n",
    "\n",
    "def vocab_decode(array, vocab):\n",
    "    return ''.join([vocab[x - 1] for x in array])\n",
    "\n",
    "def read_data(filename, vocab, window, overlap):\n",
    "    lines = [line.strip() for line in open(filename, 'r').readlines()]\n",
    "    while True:\n",
    "        random.shuffle(lines)\n",
    "\n",
    "        for text in lines:\n",
    "            text = vocab_encode(text, vocab)\n",
    "            for start in range(0, len(text) - window, overlap):\n",
    "                chunk = text[start: start + window]\n",
    "                chunk += [0] * (window - len(chunk))\n",
    "                yield chunk\n",
    "\n",
    "def read_batch(stream, batch_size):\n",
    "    batch = []\n",
    "    for element in stream:\n",
    "        batch.append(element)\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    yield batch\n",
    "\n",
    "class CharRNN(object):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.path = 'data/' + model + '.txt'\n",
    "        if 'trump' in model:\n",
    "            self.vocab = (\"$%'()+,-./0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "                    \" '\\\"_abcdefghijklmnopqrstuvwxyz{|}@#➡📈\")\n",
    "        self.len_generated = 200\n",
    "        self.gstep = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "    def create_rnn(self, seq):\n",
    "        layers = [tf.nn.rnn_cell.GRUCell(size) for size in self.hidden_sizes]\n",
    "        cells = tf.nn.rnn_cell.MultiRNNCell(layers)\n",
    "        batch = tf.shape(seq)[0]\n",
    "        zero_states = cells.zero_state(batch, dtype=tf.float32)\n",
    "        self.in_state = tuple([tf.placeholder_with_default(state, [None, state.shape[1]]) \n",
    "                                for state in zero_states])\n",
    "        # this line to calculate the real length of seq\n",
    "        # all seq are padded to be of the same length, which is num_steps\n",
    "        length = tf.reduce_sum(tf.reduce_max(tf.sign(seq), 2), 1)\n",
    "        self.output, self.out_state = tf.nn.dynamic_rnn(cells, seq, length, self.in_state)\n",
    "\n",
    "    def create_model(self):\n",
    "        seq = tf.one_hot(self.seq, len(self.vocab))\n",
    "        self.create_rnn(seq)\n",
    "        self.logits = tf.layers.dense(self.output, len(self.vocab), None)\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits[:, :-1], \n",
    "                                                        labels=seq[:, 1:])\n",
    "        self.loss = tf.reduce_sum(loss)\n",
    "        # sample the next character from Maxwell-Boltzmann Distribution \n",
    "        # with temperature temp. It works equally well without tf.exp\n",
    "        self.sample = tf.multinomial(tf.exp(self.logits[:, -1] / self.temp), 1)[:, 0] \n",
    "        self.opt = tf.train.AdamOptimizer(self.lr).minimize(self.loss, global_step=self.gstep)\n",
    "\n",
    "    def train(self):\n",
    "        saver = tf.train.Saver()\n",
    "        start = time.time()\n",
    "        min_loss = None\n",
    "        with tf.Session() as sess:\n",
    "            writer = tf.summary.FileWriter('graphs/gist', sess.graph)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/' + self.model + '/checkpoint'))\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            \n",
    "            iteration = self.gstep.eval()\n",
    "            stream = read_data(self.path, self.vocab, self.num_steps, overlap=self.num_steps//2)\n",
    "            data = read_batch(stream, self.batch_size)\n",
    "            while True:\n",
    "                batch = next(data)\n",
    "\n",
    "                batch_loss, _ = sess.run([self.loss, self.opt], {self.seq: batch})\n",
    "                if (iteration + 1) % self.skip_step == 0:\n",
    "                    print('Iter {}. \\n    Loss {}. Time {}'.format(iteration, batch_loss, time.time() - start))\n",
    "                    self.online_infer(sess)\n",
    "                    start = time.time()\n",
    "                    checkpoint_name = 'checkpoints/' + self.model + '/char-rnn'\n",
    "                    if min_loss is None:\n",
    "                        saver.save(sess, checkpoint_name, iteration)\n",
    "                    elif batch_loss < min_loss:\n",
    "                        saver.save(sess, checkpoint_name, iteration)\n",
    "                        min_loss = batch_loss\n",
    "                iteration += 1\n",
    "\n",
    "    def online_infer(self, sess):\n",
    "        \"\"\" Generate sequence one character at a time, based on the previous character\n",
    "        \"\"\"\n",
    "        for seed in ['Hillary', 'I', 'R', 'T', '@', 'N', 'M', '.', 'G', 'A', 'W']:\n",
    "            sentence = seed\n",
    "            state = None\n",
    "            for _ in range(self.len_generated):\n",
    "                batch = [vocab_encode(sentence[-1], self.vocab)]\n",
    "                feed = {self.seq: batch}\n",
    "                if state is not None: # for the first decoder step, the state is None\n",
    "                    for i in range(len(state)):\n",
    "                        feed.update({self.in_state[i]: state[i]})\n",
    "                index, state = sess.run([self.sample, self.out_state], feed)\n",
    "                sentence += vocab_decode(index, self.vocab)\n",
    "            print('\\t' + sentence)\n",
    "\n",
    "def main():\n",
    "    model = 'trump_tweets'\n",
    "    safe_mkdir('checkpoints')\n",
    "    safe_mkdir('checkpoints/' + model)\n",
    "\n",
    "    lm = CharRNN(model)\n",
    "    lm.create_model()\n",
    "    lm.train()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.Print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def tf_print(op, tensors, message=''): \n",
    "    #redirective output from stderr to stdout for demonstrative purpose\n",
    "    def print_message(x):\n",
    "        sys.stdout.write(message + \" %s\\n\" % x)\n",
    "        return x\n",
    "\n",
    "    prints = [tf.py_func(print_message, [tensor], tensor.dtype) for tensor in tensors]\n",
    "    with tf.control_dependencies(prints):\n",
    "        op = tf.identity(op)\n",
    "    return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y:  9.0\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(shape=[], dtype=tf.float32, name='x')\n",
    "y = tf.square(x)\n",
    "y = tf_print(y, [y], message='y: ')\n",
    "z = tf.sqrt(y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(z, feed_dict={x:-3}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for missing connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "dense/kernel:0 not changed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-aaf2a98ee2d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mfeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mtest_all_trainables_changed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-aaf2a98ee2d2>\u001b[0m in \u001b[0;36mtest_all_trainables_changed\u001b[0;34m(sess, train_op, feed)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbefore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mafter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Make sure something changed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'{} not changed'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainable_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: dense/kernel:0 not changed"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def test_all_trainables_changed(sess, train_op, feed):\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #Get a list of values of all trainable variables\n",
    "    before = sess.run(tf.trainable_variables())\n",
    "    #Gradient descent the variables once\n",
    "    _ = sess.run(train_op, feed)\n",
    "    #Get the values after being trained once\n",
    "    after = sess.run(tf.trainable_variables())\n",
    "    #Get a list of names of all trainable variable\n",
    "    trainable_names = [v.name for v in tf.trainable_variables()]\n",
    "    #Check for each trainable variable\n",
    "    for i, (b, a) in enumerate(zip(before, after)):\n",
    "        # Make sure something changed.\n",
    "        assert (b != a).any(), '{} not changed'.format(trainable_names[i])\n",
    "\n",
    "learning_rate = 0.01\n",
    "hidden_units = 8\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 5])\n",
    "y = tf.placeholder(tf.float32, [None, 2])\n",
    "\n",
    "# Stacked dense layers example\n",
    "output = tf.layers.dense(x, hidden_units)\n",
    "output = tf.nn.relu(output)\n",
    "output = tf.layers.dense(x, hidden_units)\n",
    "output = tf.nn.relu(output)\n",
    "output = tf.layers.dense(x, hidden_units)\n",
    "output = tf.nn.relu(output)\n",
    "output = tf.layers.dense(x, 2)\n",
    "output = tf.nn.relu(output)\n",
    "\n",
    "loss = tf.reduce_mean((y - output)**2)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    feed = {x:[[1,2,3,4,5]], y:[[2,4]]}\n",
    "    test_all_trainables_changed(sess, train_op, feed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert NaN into inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.0007992  0.00949341 0.01229813 0.00716891 0.00225313]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.00099776 0.00943848 0.01227802 0.00673182 0.0024643 ]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.00122693 0.00931066 0.01230152 0.00635852 0.00266232]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.0014853  0.00913952 0.01236539 0.00602461 0.00284451]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.00177232 0.00892803 0.01246845 0.00572553 0.00301226]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.00208765 0.00866213 0.01261188 0.00546111 0.0031665 ]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.00243936 0.00833574 0.0128089  0.00524187 0.00330879]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.00283356 0.00794545 0.01305805 0.00506346 0.00343639]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.00327458 0.007493   0.01334589 0.00491525 0.00354506]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.00377057 0.00698245 0.01365503 0.00479042 0.00363245]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.00433633 0.0064154  0.01396669 0.00468592 0.00369911]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.00499533 0.00579448 0.01426213 0.00460059 0.00374883]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.00578049 0.00512913 0.01452293 0.00453325 0.00378862]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.00672892 0.0044437  0.01472991 0.00448027 0.00382752]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.00786413 0.00378265 0.01486542 0.00443438 0.00387378]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.00916824 0.00319834 0.01492634 0.00438584 0.00392847]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.01056747 0.00272668 0.0149386  0.00432547 0.00397982]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.01195421 0.00237331 0.01494937 0.00424631 0.00400753]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.0132329  0.00212002 0.01499812 0.00414404 0.00399557]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.01435069 0.00194003 0.01509764 0.00401727 0.00394031]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.01529534 0.00180917 0.01523796 0.00386691 0.00384906]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.01607797 0.00170968 0.01539917 0.0036953  0.0037341 ]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.01671764 0.00162995 0.01556026 0.00350563 0.00360738]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.01723138 0.00156283 0.01570222 0.00330225 0.00347748]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.01763003 0.00150417 0.01580851 0.00309064 0.00334953]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.01791679 0.00145224 0.01586556 0.00287534 0.00322669]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.01808829 0.00140727 0.01586233 0.00265816 0.00311061]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.01814569 0.00136987 0.01578884 0.0024408  0.0030016 ]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.01811293 0.00133892 0.01563784 0.00222931 0.00289945]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.01804175 0.0013109  0.01541133 0.00203402 0.00280426]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.0179986  0.00128078 0.01512439 0.00186587 0.00271657]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.01804446 0.00124385 0.01480201 0.0017324  0.00263679]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.01821782 0.00119762 0.01447116 0.00163553 0.0025646 ]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.01853094 0.0011426  0.01415345 0.00157227 0.00249893]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.01897607 0.00108148 0.01386129 0.00153719 0.00243827]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.01953252 0.00101793 0.01359779 0.00152418 0.00238107]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.02017199 0.00095568 0.01335899 0.00152699 0.00232593]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.02086218 0.00089801 0.0131364  0.00153899 0.00227181]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.02157056 0.0008475  0.0129188  0.00155303 0.00221812]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.0222685  0.00080595 0.01269372 0.00156185 0.00216488]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.02293526 0.00077443 0.0124495  0.00155892 0.00211263]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.02356007 0.00075337 0.01217703 0.00153957 0.00206193]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.02414197 0.0007425  0.01187034 0.00150166 0.00201288]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.02468787 0.00074087 0.01152634 0.00144566 0.00196515]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.02520945 0.00074709 0.0111446  0.00137413 0.00191855]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.02571919 0.0007596  0.01072753 0.00129087 0.00187351]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.0262275  0.00077696 0.0102808  0.00120018 0.00183093]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.02674331 0.00079793 0.00981366 0.00110615 0.0017915 ]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.02727276 0.0008214  0.00933861 0.00101221 0.00175534]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.02780781 0.00084648 0.00887135 0.00092101 0.00172218]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "####\n",
    "#Give the index of the minimum element of a list\n",
    "####\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "learning_rate = 0.01\n",
    "hidden_units = 8\n",
    "input_vector_size = 5\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, input_vector_size])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "# Stacked dense layers example\n",
    "output = tf.layers.dense(x, hidden_units)\n",
    "output = tf.nn.tanh(output)\n",
    "output = tf.layers.dense(output, hidden_units)\n",
    "output = tf.nn.tanh(output)\n",
    "output = tf.layers.dense(output, input_vector_size)\n",
    "output = tf.nn.softmax(output, axis=0)\n",
    "\n",
    "y_onehot = tf.one_hot(y, input_vector_size)\n",
    "cross_entropy_loss = tf.reduce_mean(-tf.reduce_sum(y_onehot*tf.log(output+1e-10), reduction_indices=[1]))\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cross_entropy_loss)\n",
    "\n",
    "def generate_data(amount):\n",
    "    x = np.random.uniform(low=-10, high=10, size=(amount, input_vector_size))\n",
    "    y = np.argmin(x, axis=-1)\n",
    "    return x, y\n",
    "\n",
    "loss = []\n",
    "\n",
    "x_data, y_data = generate_data(100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for _ in range(50):\n",
    "        feed = {x:x_data, y:y_data}\n",
    "        loss_, _, probabilities = sess.run([cross_entropy_loss, train_op, output], feed)\n",
    "        loss.append(loss_)\n",
    "        feed = {x:[x_data[0]]}\n",
    "        print('\\n x', x_data[0])\n",
    "        print('y', y_data[0])\n",
    "        print('output', probabilities[0])\n",
    "\n",
    "plt.plot(np.arange(len(loss)),loss)\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n"
     ]
    }
   ],
   "source": [
    "x = output\n",
    "output = tf_print(output, [output], message='1')\n",
    "output = tf.layers.dense(output, hidden_units)\n",
    "output = tf_print(output, [output], message='2')\n",
    "output = tf.nn.tanh(output)\n",
    "output = tf_print(output, [output], message='3')\n",
    "output = tf.layers.dense(output, hidden_units)\n",
    "output = tf_print(output, [output], message='4')\n",
    "output = tf.nn.tanh(output)\n",
    "output = tf_print(output, [output], message='5')\n",
    "output = tf.layers.dense(output, 5)\n",
    "output = tf.nn.softmax(output, axis=0)\n",
    "output = tf_print(output, [output], message='6')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for _ in range(50):\n",
    "        feed = {x:[[float('NaN'),1,2,3,4], [6,2,6,2,5],[5,2,6,7,8]]}\n",
    "        probabilities = sess.run([output], feed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "####\n",
    "#Give the index of the minimum element of a list\n",
    "####\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "learning_rate = 0.01\n",
    "hidden_units = 32\n",
    "input_vector_size = 5\n",
    "\n",
    "def get_grads_and_vars(cost):\n",
    "    tvars = tf.trainable_variables() #Get a list of all trainable variables\n",
    "    grads = tf.gradients(cost, tvars) #Get the gradients of all trainable variables wrt cost\n",
    "    grads_and_vars = zip(grads, tvars)\n",
    "    return grads_and_vars\n",
    "\n",
    "def create_summary(loss, grads_and_vars):                                             \n",
    "    tf.summary.scalar(\"loss\", loss) #Create scalar plot of loss\n",
    "    for g, v in grads_and_vars:                                                       \n",
    "        tf.summary.histogram(v.name, v) #Plot histrogram of weights and bias                       \n",
    "        tf.summary.histogram(v.name + '_grad', g) #Plot histogram of gradients          \n",
    "    return tf.summary.merge_all() #Merge the summary operators into a single summary operator\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, input_vector_size], name='x')\n",
    "y = tf.placeholder(tf.int32, [None], name='y')\n",
    "\n",
    "# Stacked dense layers example\n",
    "output = tf.layers.dense(x, hidden_units)\n",
    "output = tf.nn.tanh(output)\n",
    "output = tf.layers.dense(output, hidden_units)\n",
    "output = tf.nn.tanh(output)\n",
    "output = tf.layers.dense(output, input_vector_size)\n",
    "output = tf.nn.softmax(output, axis=1)\n",
    "\n",
    "y_onehot = tf.one_hot(y, input_vector_size)\n",
    "cross_entropy_loss = tf.reduce_mean(-tf.reduce_sum(y_onehot*tf.log(output+1e-10), reduction_indices=[1]))\n",
    "\n",
    "grads_and_vars = get_grads_and_vars(cross_entropy_loss)\n",
    "#This line is changed because gradients are already calculated\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_op = optimizer.apply_gradients(grads_and_vars)\n",
    "\n",
    "summary_op = create_summary(cross_entropy_loss, grads_and_vars)\n",
    "writer = tf.summary.FileWriter('train', sess.graph) #Create a writer for tensorboard\n",
    "\n",
    "\n",
    "def generate_data(amount):\n",
    "    x = np.random.uniform(low=-10, high=10, size=(amount, input_vector_size))\n",
    "    y = np.argmin(x, axis=-1)\n",
    "    return x, y\n",
    "\n",
    "x_data, y_data = generate_data(100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(50):\n",
    "        feed = {x:x_data, y:y_data}\n",
    "        #Set to trace all metadata for tensorboard\n",
    "        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "        run_metadata = tf.RunMetadata()\n",
    "        _, summary = sess.run([train_op, summary_op], feed,\n",
    "                               options=run_options,\n",
    "                               run_metadata=run_metadata)\n",
    "        #Get the summaries and log it as the 'i'th run\n",
    "        writer.add_summary(summary, i)\n",
    "        #Get metadata such as memory usage and computation time\n",
    "        writer.add_run_metadata(run_metadata, 'step %d' % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
