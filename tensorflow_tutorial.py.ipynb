{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n",
      "40.0\n",
      "40.0\n",
      "40.0\n",
      "40.0\n",
      "Tensor(\"zeros:0\", shape=(2, 2), dtype=float32)\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "w = tf.constant(5.0)\n",
    "x = tf.constant(6.0)\n",
    "b = tf.constant(10.0)\n",
    "\n",
    "#Important: c is a computation graph that connects w, x and b,\n",
    "#and outputs w*x + b. \n",
    "#c is not 40. It is a connection between nodes.\n",
    "c = w*x + b \n",
    "with tf.Session() as sess: \n",
    "    print(sess.run(w))  # 5.0\n",
    "    print(sess.run(c))  # 40.0\n",
    "    print(c.eval()) # 40.0\n",
    "    \n",
    "sess = tf.Session()\n",
    "print(c.eval(session=sess))  #40.0\n",
    "print(sess.run(c)) # 40.0\n",
    "\n",
    "t = tf.zeros((2,2))\n",
    "with sess.as_default():\n",
    "    print(t) #Tensor(\"zeros:0\", shape=(2, 2), dtype=float32) \n",
    "    print(t.eval()) # [[ 0. 0.]\n",
    "                    #  [ 0. 0.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constant and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14159\n",
      "[ 9 25]\n",
      "Tensor(\"Const_4:0\", shape=(), dtype=int16)\n",
      "Tensor(\"sub:0\", shape=(), dtype=int16)\n",
      "180\n"
     ]
    }
   ],
   "source": [
    "#0-d tensors\n",
    "mammal = tf.constant(\"Elephant\", tf.string)\n",
    "ignition = tf.constant(451, tf.int16)\n",
    "floating = tf.constant(3.14159265359, tf.float32)\n",
    "\n",
    "#2-d tensors\n",
    "xor = tf.constant([[False, True],[True, False]], tf.bool)\n",
    "cool_numbers  = tf.constant([3.14159, 2.71828], tf.float32)\n",
    "squarish_squares = tf.constant([ [4, 9], [16, 25] ], tf.int32)\n",
    "\n",
    "pi = cool_numbers[0]\n",
    "my_column_vector = squarish_squares[:, 1]\n",
    "ss_shape = tf.shape(squarish_squares) # [2,2]\n",
    "\n",
    "print(pi.eval(session=sess)) # 3.14159\n",
    "print(my_column_vector.eval(session=sess)) #[ 9 25]\n",
    "\n",
    "print(ignition) # Tensor(\"Const_10:0\", shape=(), dtype=int16)\n",
    "ignition -= 271\n",
    "print(ignition) # Tensor(\"sub_1:0\", shape=(), dtype=int16)\n",
    "print(ignition.eval(session=sess)) # 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.4 3.3]\n",
      " [1.1 2.2]]\n",
      "[[[0 0 0]\n",
      "  [0 0 0]]]\n"
     ]
    }
   ],
   "source": [
    "#A name has to be given to variables.\n",
    "#The name can be used in tensorboard or variables manipulations\n",
    "#Inputs: get_variable(name, shape=None, dtype=None, initializer=None)\n",
    "w1 = tf.get_variable(\"floatpointfloat\", [2,2],  initializer=tf.constant_initializer([[4.4, 3.3], [1.1, 2.2]]))\n",
    "w2 = tf.get_variable(\"zeros\", [1, 2, 3], dtype=tf.int32, initializer=tf.zeros_initializer)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #Variables have to be initialized before use.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(w1)) #[[4.3, 3.2], [1.1, 2.2]]\n",
    "    print(sess.run(w2)) #[[[0,0,0],[0,0,0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# lower level api for creating variable object\n",
    "# mostly used for scalars\n",
    "i_am_zero_variable = tf.Variable(0)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(i_am_zero_variable.eval()) #0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_cant_be_changed = tf.constant(101)\n",
    "i_can_be_changed = tf.get_variable(\"can_be_changed\", [1],\n",
    "                                   dtype=tf.int32,\n",
    "                                   initializer=tf.constant_initializer([101]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101]\n",
      "<tf.Variable 'can_be_changed:0' shape=(1,) dtype=int32_ref>\n",
      "[5]\n"
     ]
    }
   ],
   "source": [
    "assign_op = i_can_be_changed.assign([5])\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(i_can_be_changed))\n",
    "    sess.run(assign_op)\n",
    "    print(i_can_be_changed)\n",
    "    print(sess.run(i_can_be_changed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'assign'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-11eae9e334b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0massign_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi_cant_be_changed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#AttributeError: 'Tensor' object has no attribute 'assign'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'assign'"
     ]
    }
   ],
   "source": [
    "assign_op = i_cant_be_changed.assign([5]) #AttributeError: 'Tensor' object has no attribute 'assign'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106\n"
     ]
    }
   ],
   "source": [
    "i_cant_be_changed = tf.constant(101)\n",
    "#A new tensor will be created.\n",
    "#The original tensor has not been modified\n",
    "i_cant_be_changed += 5\n",
    "with tf.Session() as sess:\n",
    "    print(i_cant_be_changed.eval()) #106"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs and Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1., 2., 3.], dtype=float32), array([1., 4., 9.], dtype=float32)]\n",
      "[ 0.  0. 25.]\n"
     ]
    }
   ],
   "source": [
    "# Define a placeholder that expects a vector of three floating-point values\n",
    "x = tf.placeholder(tf.float32, shape=[3]) #If the shape is None, you can feed a tensor of any shape.\n",
    "y = tf.square(x)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Value of 'y' depends on the value fed to x.\n",
    "    print(sess.run([x, y], {x: [1.0, 2.0, 3.0]}))  # [array([1., 2., 3.], dtype=float32),\n",
    "                                                   #  array([1., 4., 9.], dtype=float32)]\n",
    "    print(sess.run(y, {x: [0.0, 0.0, 5.0]}))       # [0., 0., 25.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape [3]\n\t [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[3], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\t [[Node: Square/_1 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_5_Square\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'Placeholder', defined at:\n  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/lib/python3.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/usr/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 523, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1758, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/usr/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n    handler_func(fileobj, events)\n  File \"/usr/lib/python3.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python3.7/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/lib/python3.7/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/lib/python3.7/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/lib/python3.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-e012f40a63e9>\", line 2, in <module>\n    x = tf.placeholder(tf.float32, shape=[3]) #If the shape is not specified, you can feed a tensor of any shape.\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\", line 1735, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 4925, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape [3]\n\t [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[3], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\t [[Node: Square/_1 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_5_Square\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape [3]\n\t [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[3], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\t [[Node: Square/_1 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_5_Square\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7bd74b1ec0d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# a `tf.placeholder()` when evaluating a tensor that depends on it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1291\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape [3]\n\t [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[3], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\t [[Node: Square/_1 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_5_Square\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'Placeholder', defined at:\n  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/lib/python3.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/usr/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 523, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1758, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/usr/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n    handler_func(fileobj, events)\n  File \"/usr/lib/python3.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python3.7/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/lib/python3.7/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/lib/python3.7/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/lib/python3.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-e012f40a63e9>\", line 2, in <module>\n    x = tf.placeholder(tf.float32, shape=[3]) #If the shape is not specified, you can feed a tensor of any shape.\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\", line 1735, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 4925, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape [3]\n\t [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[3], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\t [[Node: Square/_1 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_5_Square\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "# Raises `tf.errors.InvalidArgumentError`, because you must feed a value for\n",
    "# a `tf.placeholder()` when evaluating a tensor that depends on it.\n",
    "with tf.Session() as sess:\n",
    "    sess.run(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape () for Tensor 'Placeholder:0', which has shape '(3,)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-4f33f7522bfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# of placeholder `x`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m37.0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1074\u001b[0m                              \u001b[0;34m'which has shape %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[0;32m-> 1076\u001b[0;31m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1077\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape () for Tensor 'Placeholder:0', which has shape '(3,)'"
     ]
    }
   ],
   "source": [
    "# Raises `ValueError`, because the shape of `37.0` does not match the shape\n",
    "# of placeholder `x`.\n",
    "with tf.Session() as sess:\n",
    "    sess.run(y, {x: 37.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight:[[0.99986756]], Bias:[[0.00831092]]\n",
      "Final Loss:1.3144789423602532e-10\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define number of data and learning rate\n",
    "n_samples = 1000\n",
    "learning_rate = 2E-5 \n",
    "\n",
    "# Generate some input data\n",
    "x_data = np.linspace(0,40*np.pi, n_samples)\n",
    "f = lambda x: x + 20*np.sin(x/10) \n",
    "y_data = f(x_data)\n",
    "\n",
    "# Reshape it to 1-dimension inputs\n",
    "x_data = np.reshape(x_data, (n_samples,1))\n",
    "y_data = np.reshape(y_data, (n_samples,1))\n",
    "\n",
    "# For logging the lost function\n",
    "loss_log = []\n",
    "\n",
    "# Define placeholders for input\n",
    "X = tf.placeholder(tf.float32, shape=(n_samples, 1))\n",
    "y = tf.placeholder(tf.float32, shape=(n_samples, 1))\n",
    "\n",
    "# Define variables to be learned\n",
    "w = tf.get_variable(\"weights\", (1, 1),\n",
    "    initializer=tf.random_normal_initializer())\n",
    "b = tf.get_variable(\"bias\", (1,1),\n",
    "    initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "# Define structure of the loss\n",
    "y_pred = w*X + b\n",
    "# Mean Squared Error\n",
    "loss = tf.reduce_mean(y - y_pred)**2\n",
    "\n",
    "# Operator to minimize the loss\n",
    "train_opt = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize Variables in graph\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Gradient descent 500 times (500 is arbitrary)\n",
    "    for i in range(500):\n",
    "        # Do gradient descent step\n",
    "        _, loss_val = sess.run([train_opt, loss], feed_dict={X: x_data, y: y_data})\n",
    "        loss_log.append(loss_val)\n",
    "    pred = sess.run(y_pred, feed_dict={X: x_data}) \n",
    "    weight, bias = sess.run([w, b])\n",
    "    print('Weight:{}, Bias:{}'.format(weight, bias))\n",
    "    print('Final Loss:{}'.format(loss_log[-1]))\n",
    "\n",
    "# Plot the loss function\n",
    "plt.plot(np.arange(len(loss_log)),loss_log)\n",
    "plt.title('Linear Regresson Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.figure()\n",
    "# Plot the data and predictions\n",
    "plt.title('Linear Regresson Data and Fit')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.plot(x_data, y_data)\n",
    "plt.plot(x_data, pred)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higher Level Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight:[[0.9753293]], Bias:[1.5501037]\n",
      "Final Loss:1.8384538683080542e-11\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_samples = 1000\n",
    "learning_rate = 2E-2\n",
    "\n",
    "x_data = np.linspace(0,40*np.pi, n_samples)\n",
    "f = lambda x: x + 20*np.sin(x/10) \n",
    "y_data = f(x_data)\n",
    "\n",
    "x_data = np.reshape(x_data, (n_samples,1))\n",
    "y_data = np.reshape(y_data, (n_samples,1))\n",
    "\n",
    "loss_log = []\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(n_samples, 1))\n",
    "y = tf.placeholder(tf.float32, shape=(n_samples, 1))\n",
    "\n",
    "# Create an object of dense layer \n",
    "dense = tf.layers.Dense(1) # 1 for output dimension\n",
    "y_pred = dense(X)\n",
    "loss = tf.reduce_mean(y - y_pred)**2\n",
    "\n",
    "# ADAM Optimizer: a more advanced version of gradient descent\n",
    "train_opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(500):\n",
    "      _, loss_val = sess.run([train_opt, loss], feed_dict={X: x_data, y: y_data})\n",
    "      loss_log.append(loss_val)\n",
    "    pred = sess.run(y_pred, feed_dict={X: x_data}) \n",
    "    weight, bias = sess.run([dense.kernel, dense.bias])\n",
    "    print('Weight:{}, Bias:{}'.format(weight, bias))\n",
    "    print('Final Loss:{}'.format(loss_log[-1]))\n",
    "\n",
    "plt.plot(np.arange(len(loss_log)),loss_log)\n",
    "plt.title('Linear Regresson Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.figure()\n",
    "\n",
    "plt.title('Linear Regresson Data and Fit')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.plot(x_data, y_data)\n",
    "plt.plot(x_data, pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minibatch and rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Loss 0.151392\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum from RNN [[15.100087]]\n",
      "True sum 15\n",
      "Sum from RNN [[12.841793]]\n",
      "True sum 16\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "###\n",
    "# Calculates the sum of a list\n",
    "###\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def generate_data(amount):\n",
    "    x = np.random.uniform(low=-10, high=10, size=(amount,5,1))\n",
    "    y = np.sum(x, 1)\n",
    "    return x, y\n",
    "\n",
    "def read_batch(stream, batch_size):\n",
    "    head = 0\n",
    "    while True:\n",
    "        batch = []\n",
    "        for item in stream:\n",
    "            batch.append(item[head:head+batch_size])\n",
    "        yield batch\n",
    "        head += batch_size\n",
    "        head %= len(stream[0])-batch_size\n",
    "\n",
    "class SumRNN(object):\n",
    "    def __init__(self):\n",
    "        self.lstm_size = 32\n",
    "        self.learning_rate = 0.01\n",
    "        self.data_amount = 10000\n",
    "        self.batch_size = 100\n",
    "        self.loss_logs = []\n",
    "        self.input_data, self.target_data = generate_data(self.data_amount)\n",
    "        self.create_model()\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def create_rnn(lstm_size, data):\n",
    "        #Create a LSTMCell with hidden size lstm_size\n",
    "        rnn_cell = tf.contrib.rnn.LSTMCell(lstm_size) \n",
    "        #Get the zero state of rnn_cell\n",
    "        initial_state = rnn_cell.zero_state(tf.shape(data)[0], tf.float32)\n",
    "        #Create a rnn with the cell.\n",
    "        #Dynamic means the input length can be a variable\n",
    "        outputs, state = tf.nn.dynamic_rnn(rnn_cell, data, \n",
    "                                           initial_state=initial_state,\n",
    "                                           dtype=tf.float32)\n",
    "        #Get the last output only and pass it through a dense layer\n",
    "        outputs = tf.layers.dense(outputs[:,-1], 1)\n",
    "        return outputs\n",
    "\n",
    "    def create_placeholders(self):\n",
    "        self.inputs = tf.placeholder(dtype=tf.float32, shape=[None, None, 1])\n",
    "        self.targets = tf.placeholder(dtype=tf.float32, shape=[None, 1])\n",
    "        \n",
    "    @staticmethod\n",
    "    def calculate_loss(predictions, targets):\n",
    "        #L2 loss\n",
    "        loss = tf.reduce_mean(tf.square(predictions-targets))\n",
    "        return loss\n",
    "    \n",
    "    def create_model(self):\n",
    "        self.create_placeholders()\n",
    "        self.outputs = self.create_rnn(self.lstm_size, self.inputs)\n",
    "        self.loss = self.calculate_loss(self.outputs, self.targets)\n",
    "        self.train_opt = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
    "        \n",
    "    @staticmethod\n",
    "    def plot_results(loss_logs):\n",
    "        plt.plot(np.arange(len(loss_logs)), loss_logs)\n",
    "        plt.title('Loss')\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.show()\n",
    "        \n",
    "    def train(self):\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            #Create a iterator that feed minibatches\n",
    "            batch_feeder = read_batch([self.input_data, self.target_data], self.batch_size)\n",
    "            for _ in range(500):\n",
    "                batch_inputs, batch_targets = next(batch_feeder)\n",
    "                feed_dict = {self.inputs: batch_inputs,\n",
    "                             self.targets: batch_targets}\n",
    "                loss, _ = sess.run([self.loss, self.train_opt], feed_dict=feed_dict)\n",
    "                self.loss_logs.append(loss)\n",
    "            print('Final Loss', loss)\n",
    "            #The noisy loss comes from the minibatch\n",
    "            self.plot_results(self.loss_logs)\n",
    "\n",
    "            #Test the rnn\n",
    "            self.infer(sess, [1,2,3,4,5])\n",
    "            #The rnn does not generalize well to lengths other than 5\n",
    "            self.infer(sess, [1,2,3,4,5,1])\n",
    "\n",
    "                \n",
    "    def infer(self, sess, test_input):            \n",
    "            test_input = np.reshape(test_input, (1,len(test_input),1))\n",
    "            print('Sum from RNN', sess.run(self.outputs, feed_dict={self.inputs:test_input}))\n",
    "            print('True sum', test_input.sum())\n",
    "            \n",
    "model = SumRNN()\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-275c20351226>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'True sum'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSumRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-275c20351226>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_amount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_amount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-275c20351226>\u001b[0m in \u001b[0;36mgenerate_data\u001b[0;34m(amount)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial)\u001b[0m\n\u001b[1;32m   1928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1929\u001b[0m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[0;32m-> 1930\u001b[0;31m                           initial=initial)\n\u001b[0m\u001b[1;32m   1931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "#TODO\n",
    "###\n",
    "# Calculates the sum of a list with variable length\n",
    "###\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def generate_data(amount):\n",
    "    lengths = np.random.randint(low=1, high=10, size=amount)\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in lengths:\n",
    "        x.append(np.random.uniform(low=-10, high=10, size=(i,1)).tolist())\n",
    "        y.append(np.sum(x))\n",
    "    return x, y\n",
    "\n",
    "def generate_shuffle_key(length):\n",
    "    shuffle_key = np.arange(length)\n",
    "    np.random.shuffle(shuffle_key)\n",
    "    return shuffle_key\n",
    "\n",
    "def pad_data(item):\n",
    "    \"\"\"\n",
    "    Pads a 2d array with zeros.\n",
    "    Input:\n",
    "    [[1,2],\n",
    "     [3,4,5,6],\n",
    "     [7,8,9]]\n",
    "    Output:\n",
    "    [[1,2,0,0],\n",
    "     [3,4,5,6],\n",
    "     [7,8,9,0]], (2,4,3)\n",
    "    \"\"\"\n",
    "    lengths = [len(i) for i in item]\n",
    "    maxlen = max(lengths)\n",
    "    x = np.zeros([len(item), maxlen], dtype=np.int32)\n",
    "    for i, x_i in enumerate(x):\n",
    "        x_i[:lengths[i]] = item[i]\n",
    "    return x, lengths\n",
    "\n",
    "\n",
    "    \n",
    "def read_batch(stream, batch_size):\n",
    "    head = 0\n",
    "    stream[0] = pad_data(stream[0])\n",
    "    while True:\n",
    "        batch = []\n",
    "        for item in stream:\n",
    "            batch.append(item[head:head+batch_size])\n",
    "        yield batch\n",
    "        head += batch_size\n",
    "        if head+batch_size > len(stream[0]):\n",
    "            head = 0\n",
    "            shuffle_key = generate_shuffle_key(len(stream[0]))\n",
    "            for i, _ in enumerate(stream):\n",
    "                stream[i] = stream[i][shuffle_key]\n",
    "                \n",
    "\n",
    "class SumRNN(object):\n",
    "    def __init__(self):\n",
    "        self.lstm_size = 32\n",
    "        self.learning_rate = 0.01\n",
    "        self.data_amount = 10000\n",
    "        self.batch_size = 100\n",
    "        self.input_data, self.target_data = generate_data(self.data_amount)\n",
    "        self.create_model()\n",
    "    \n",
    "        self.loss_logs = []\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_rnn(lstm_size, data):\n",
    "        rnn_cell = tf.contrib.rnn.LSTMCell(lstm_size)\n",
    "        initial_state = rnn_cell.zero_state(tf.shape(data)[0], tf.float32)\n",
    "\n",
    "        # 'state' is a tensor of shape [batch_size, cell_state_size]\n",
    "        outputs, state = tf.nn.dynamic_rnn(rnn_cell, data,\n",
    "                                       initial_state=initial_state,\n",
    "                                       dtype=tf.float32)\n",
    "        outputs = tf.layers.dense(outputs[:,-1], 1)\n",
    "        return outputs\n",
    "\n",
    "    def create_placeholders(self):\n",
    "        #self.inputs = tf.placeholder(dtype=tf.float32, shape=[self.batch_size, None, 1])\n",
    "        self.inputs = tf.placeholder(dtype=tf.float32, shape=[None, None, 1])\n",
    "        self.targets = tf.placeholder(dtype=tf.float32, shape=[None, 1])\n",
    "        \n",
    "    @staticmethod\n",
    "    def calculate_loss(predictions, targets):\n",
    "        loss = tf.reduce_mean(tf.square(predictions-targets))\n",
    "        return loss\n",
    "    \n",
    "    def create_model(self):\n",
    "        self.create_placeholders()\n",
    "        self.outputs = self.create_rnn(self.lstm_size, self.inputs)\n",
    "        self.loss = self.calculate_loss(self.outputs, self.targets)\n",
    "        self.train_opt = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
    "        \n",
    "    def plot_results(self):\n",
    "        plt.plot(np.arange(len(self.loss_logs)),self.loss_logs)\n",
    "        plt.title('Loss')\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.show()\n",
    "        \n",
    "    def train(self):\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            batch_feeder = read_batch([self.input_data, self.target_data], self.batch_size)\n",
    "            for _ in range(500):\n",
    "                batch_inputs, batch_targets = next(batch_feeder)\n",
    "                feed_dict = {self.inputs: batch_inputs,\n",
    "                             self.targets: batch_targets}\n",
    "                loss, _ = sess.run([self.loss, self.train_opt], feed_dict=feed_dict)\n",
    "                self.loss_logs.append(loss)\n",
    "            print(loss)\n",
    "            self.plot_results()\n",
    "\n",
    "            self.infer(sess, [1,2,3,4,5])\n",
    "            self.infer(sess, [1,2,3,4,5,1])\n",
    "\n",
    "                \n",
    "    def infer(self, sess, test_input):            \n",
    "            test_input = np.reshape(test_input, (1,len(test_input),1))\n",
    "            print('Sum from RNN', sess.run(self.outputs, feed_dict={self.inputs:test_input}))\n",
    "            print('True sum', test_input.sum())\n",
    "            \n",
    "model = SumRNN()\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ij,j->i:  [26 62]\n",
      "ij,j->ij:  [[ 3  8 15]\n",
      " [12 20 30]]\n",
      "ii->:  21\n"
     ]
    }
   ],
   "source": [
    "#Almost the same as np.einsum (See: https://github.com/tensorflow/tensorflow/issues/4722)\n",
    "#Does einsum summation convention\n",
    "import tensorflow as tf\n",
    "a = tf.constant([[1,2,3], [4,5,6]])\n",
    "b = tf.constant([3,4,5])\n",
    "#Tensor contraction a_ij*b^j -> c_i or matrix multiplication\n",
    "c = tf.einsum('ij,j->i', a, b)\n",
    "#broadcasted multiplication a_ij*b^j -> c_ij (matrix multiplication without summing)\n",
    "d = tf.einsum('ij,j->ij', a, b)\n",
    "#Trace of a\n",
    "e = tf.einsum('ii->', a)\n",
    "with tf.Session() as sess:\n",
    "    print('ij,j->i: ', c.eval())  #ij,j->i:  [26 62]\n",
    "    print('ij,j->ij: ', d.eval()) #ij,j->ij:  [[ 3  8 15]\n",
    "                                  #            [12 20 30]]\n",
    "    print('ii->: ', e.eval())     #ii->:  21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Restore Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in path: /tmp/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Create some variables.\n",
    "v1 = tf.get_variable(\"v1\", shape=[3], initializer = tf.zeros_initializer)\n",
    "v2 = tf.get_variable(\"v2\", shape=[5], initializer = tf.zeros_initializer)\n",
    "\n",
    "inc_v1 = v1.assign(v1+1)\n",
    "dec_v2 = v2.assign(v2-1)\n",
    "\n",
    "# Add an op to initialize the variables.\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# Add ops to save and restore ALL the variables.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Later, launch the model, initialize the variables, do some work, and save the\n",
    "# variables to disk.\n",
    "with tf.Session() as sess:\n",
    "  sess.run(init_op)\n",
    "  # Do some work with the model.\n",
    "  inc_v1.op.run()\n",
    "  dec_v2.op.run()\n",
    "  # Save the variables to disk.\n",
    "  save_path = saver.save(sess, \"/tmp/model.ckpt\")\n",
    "  print(\"Model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "Model restored.\n",
      "v1 : [1. 1. 1.]\n",
      "v2 : [-1. -1. -1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Create some variables.\n",
    "v1 = tf.get_variable(\"v1\", shape=[3])\n",
    "v2 = tf.get_variable(\"v2\", shape=[5])\n",
    "\n",
    "# Add ops to save and restore ALL the variables.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Later, launch the model, use the saver to restore variables from disk, and\n",
    "# do some work with the model.\n",
    "with tf.Session() as sess:\n",
    "  # Restore variables from disk.\n",
    "  saver.restore(sess, \"/tmp/model.ckpt\")\n",
    "  print(\"Model restored.\")\n",
    "  # Check the values of the variables\n",
    "  print(\"v1 : %s\" % v1.eval()) #v1 : [1. 1. 1.]\n",
    "  print(\"v2 : %s\" % v2.eval()) #v2 : [-1. -1. -1. -1. -1.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trump Tweets Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" A clean, no_frills character-level generative language model.\n",
    "\n",
    "CS 20: \"TensorFlow for Deep Learning Research\"\n",
    "cs20.stanford.edu\n",
    "Danijar Hafner (mail@danijar.com)\n",
    "& Chip Huyen (chiphuyen@cs.stanford.edu)\n",
    "Lecture 11\n",
    "\"\"\"\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import random\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def safe_mkdir(path):\n",
    "    \"\"\" Create a directory if there isn't one already. \"\"\"\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "def vocab_encode(text, vocab):\n",
    "    return [vocab.index(x) for x in text if x in vocab]\n",
    "\n",
    "def vocab_decode(array, vocab):\n",
    "    return ''.join([vocab[x] if x>=0 else 'PAD' for x in array])\n",
    "\n",
    "def read_data(filename, vocab, window, overlap):\n",
    "    lines = [line.strip() for line in open(filename, 'r').readlines()]\n",
    "    while True:\n",
    "        random.shuffle(lines)\n",
    "\n",
    "        for text in lines:\n",
    "            text = vocab_encode(text, vocab)\n",
    "            for start in range(0, len(text), overlap):\n",
    "                chunk = text[start: start + window]\n",
    "                chunk += [-1]*(window-len(chunk))\n",
    "                yield chunk\n",
    "\n",
    "def read_batch(stream, batch_size):\n",
    "    batch = []\n",
    "    for element in stream:\n",
    "        batch.append(element)\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    yield batch\n",
    "\n",
    "class CharRNN(object):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.path = 'data/' + model + '.txt'\n",
    "        if 'trump' in model:\n",
    "            self.vocab = (\" $%'()+,-./123456790:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "                    \"'\\\"_abcdefghijklmnopqrstuvwxyz{|}@#\")\n",
    "        else:\n",
    "            self.vocab = (\" $%'()+,-./0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "                    \"\\\\^_abcdefghijklmnopqrstuvwxyz{|}\")\n",
    "\n",
    "        self.seq = tf.placeholder(tf.int32, [None, None])\n",
    "        self.temp = tf.constant(.5)\n",
    "        self.hidden_sizes = [128, 256]\n",
    "        self.batch_size = 128\n",
    "        self.lr = 0.0003\n",
    "        self.skip_step = 50 # number of training per infer\n",
    "        self.num_steps = 50 # number of chars for RNN unrolled\n",
    "        self.len_generated = 200\n",
    "        self.gstep = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "    def create_rnn(self, seq):\n",
    "        layers = [tf.nn.rnn_cell.GRUCell(size) for size in self.hidden_sizes]\n",
    "        cells = tf.nn.rnn_cell.MultiRNNCell(layers)\n",
    "        batch = tf.shape(seq)[0]\n",
    "        zero_states = cells.zero_state(batch, dtype=tf.float32)\n",
    "        self.in_state = tuple([tf.placeholder_with_default(state, [None, state.shape[1]]) \n",
    "                                for state in zero_states])\n",
    "\n",
    "        length = tf.reduce_sum(tf.reduce_max(seq, 2), 1)\n",
    "        self.output, self.out_state = tf.nn.dynamic_rnn(cells, seq, length, self.in_state)\n",
    "\n",
    "    def create_model(self):\n",
    "        seq = tf.one_hot(self.seq, len(self.vocab))\n",
    "        self.create_rnn(seq)\n",
    "        self.logits = tf.layers.dense(self.output, len(self.vocab), None)\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits[:, :-1], \n",
    "                                                        labels=seq[:, 1:])\n",
    "        self.loss = tf.reduce_sum(loss)\n",
    "        self.sample = tf.multinomial(self.logits[:, -1] / self.temp, 1)[:, 0] \n",
    "        self.opt = tf.train.AdamOptimizer(self.lr).minimize(self.loss, global_step=self.gstep)\n",
    "\n",
    "    def train(self):\n",
    "        saver = tf.train.Saver()\n",
    "        start = time.time()\n",
    "        min_loss = None\n",
    "        with tf.Session() as sess:\n",
    "            #To log the graph of the neural network to tensorboard\n",
    "            writer = tf.summary.FileWriter('graphs/gist', sess.graph)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            #Restore model\n",
    "            ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/' + self.model + '/checkpoint'))\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            \n",
    "            iteration = self.gstep.eval()\n",
    "            stream = read_data(self.path, self.vocab, self.num_steps, overlap=self.num_steps//2)\n",
    "            data = read_batch(stream, self.batch_size)\n",
    "            while True:\n",
    "                batch = next(data)\n",
    "                batch_loss, _ = sess.run([self.loss, self.opt], {self.seq: batch})\n",
    "                if (iteration + 1) % self.skip_step == 0:\n",
    "                    print('Iter {}. \\n    Loss {}. Time {}'.format(iteration, batch_loss, time.time() - start))\n",
    "                    self.online_infer(sess)\n",
    "                    start = time.time()\n",
    "                    checkpoint_name = 'checkpoints/' + self.model + '/char-rnn'\n",
    "                    if min_loss is None:\n",
    "                        saver.save(sess, checkpoint_name, iteration)\n",
    "                    elif batch_loss < min_loss:\n",
    "                        saver.save(sess, checkpoint_name, iteration)\n",
    "                        min_loss = batch_loss\n",
    "                iteration += 1\n",
    "\n",
    "    def online_infer(self, sess):\n",
    "        \"\"\" Generate sequence one character at a time, based on the previous character\n",
    "        \"\"\"\n",
    "        for seed in ['Hillary', 'I', 'R', 'T', '@', 'N', 'M', '.', 'G', 'A', 'W']:\n",
    "            sentence = seed\n",
    "            state = None\n",
    "            for _ in range(self.len_generated):\n",
    "                batch = [vocab_encode(sentence[-1], self.vocab)]\n",
    "                feed = {self.seq: batch}\n",
    "                if state is not None: # for the first decoder step, the state is None\n",
    "                    for i in range(len(state)):\n",
    "                        feed.update({self.in_state[i]: state[i]})\n",
    "                index, state = sess.run([self.sample, self.out_state], feed)\n",
    "                sentence += vocab_decode(index, self.vocab)\n",
    "            print('\\t' + sentence)\n",
    "\n",
    "def main():\n",
    "    model = 'trump_tweets'\n",
    "    safe_mkdir('checkpoints')\n",
    "    safe_mkdir('checkpoints/' + model)\n",
    "\n",
    "    lm = CharRNN(model)\n",
    "    lm.create_model()\n",
    "    lm.train()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.Print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def tf_print(op, tensors, message=''): \n",
    "    #redirective output from stderr to stdout for demonstrative purpose\n",
    "    def print_message(x):\n",
    "        sys.stdout.write(message + \" %s\\n\" % x)\n",
    "        return x\n",
    "\n",
    "    prints = [tf.py_func(print_message, [tensor], tensor.dtype) for tensor in tensors]\n",
    "    with tf.control_dependencies(prints):\n",
    "        op = tf.identity(op)\n",
    "    return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y:  9.0\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(shape=[], dtype=tf.float32, name='x')\n",
    "y = tf.square(x)\n",
    "y = tf_print(y, [y], message='y: ')\n",
    "z = tf.sqrt(y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(z, feed_dict={x:-3}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for missing connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "dense/kernel:0 not changed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-aaf2a98ee2d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mfeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mtest_all_trainables_changed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-aaf2a98ee2d2>\u001b[0m in \u001b[0;36mtest_all_trainables_changed\u001b[0;34m(sess, train_op, feed)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbefore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mafter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Make sure something changed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'{} not changed'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainable_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: dense/kernel:0 not changed"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def test_all_trainables_changed(sess, train_op, feed):\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #Get a list of values of all trainable variables\n",
    "    before = sess.run(tf.trainable_variables())\n",
    "    #Gradient descent the variables once\n",
    "    _ = sess.run(train_op, feed)\n",
    "    #Get the values after being trained once\n",
    "    after = sess.run(tf.trainable_variables())\n",
    "    #Get a list of names of all trainable variable\n",
    "    trainable_names = [v.name for v in tf.trainable_variables()]\n",
    "    #Check for each trainable variable\n",
    "    for i, (b, a) in enumerate(zip(before, after)):\n",
    "        # Make sure something changed.\n",
    "        assert (b != a).any(), '{} not changed'.format(trainable_names[i])\n",
    "\n",
    "learning_rate = 0.01\n",
    "hidden_units = 8\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 5])\n",
    "y = tf.placeholder(tf.float32, [None, 2])\n",
    "\n",
    "# Stacked dense layers example\n",
    "output = tf.layers.dense(x, hidden_units)\n",
    "output = tf.nn.relu(output)\n",
    "output = tf.layers.dense(x, hidden_units)\n",
    "output = tf.nn.relu(output)\n",
    "output = tf.layers.dense(x, hidden_units)\n",
    "output = tf.nn.relu(output)\n",
    "output = tf.layers.dense(x, 2)\n",
    "output = tf.nn.relu(output)\n",
    "\n",
    "loss = tf.reduce_mean((y - output)**2)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    feed = {x:[[1,2,3,4,5]], y:[[2,4]]}\n",
    "    test_all_trainables_changed(sess, train_op, feed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert NaN into inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.0007992  0.00949341 0.01229813 0.00716891 0.00225313]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.00099776 0.00943848 0.01227802 0.00673182 0.0024643 ]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.00122693 0.00931066 0.01230152 0.00635852 0.00266232]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.0014853  0.00913952 0.01236539 0.00602461 0.00284451]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.00177232 0.00892803 0.01246845 0.00572553 0.00301226]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.00208765 0.00866213 0.01261188 0.00546111 0.0031665 ]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.00243936 0.00833574 0.0128089  0.00524187 0.00330879]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.00283356 0.00794545 0.01305805 0.00506346 0.00343639]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.00327458 0.007493   0.01334589 0.00491525 0.00354506]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.00377057 0.00698245 0.01365503 0.00479042 0.00363245]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.00433633 0.0064154  0.01396669 0.00468592 0.00369911]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.00499533 0.00579448 0.01426213 0.00460059 0.00374883]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.00578049 0.00512913 0.01452293 0.00453325 0.00378862]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.00672892 0.0044437  0.01472991 0.00448027 0.00382752]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.00786413 0.00378265 0.01486542 0.00443438 0.00387378]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.00916824 0.00319834 0.01492634 0.00438584 0.00392847]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.01056747 0.00272668 0.0149386  0.00432547 0.00397982]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.01195421 0.00237331 0.01494937 0.00424631 0.00400753]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.0132329  0.00212002 0.01499812 0.00414404 0.00399557]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.01435069 0.00194003 0.01509764 0.00401727 0.00394031]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.01529534 0.00180917 0.01523796 0.00386691 0.00384906]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.01607797 0.00170968 0.01539917 0.0036953  0.0037341 ]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.01671764 0.00162995 0.01556026 0.00350563 0.00360738]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.01723138 0.00156283 0.01570222 0.00330225 0.00347748]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.01763003 0.00150417 0.01580851 0.00309064 0.00334953]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.01791679 0.00145224 0.01586556 0.00287534 0.00322669]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.01808829 0.00140727 0.01586233 0.00265816 0.00311061]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.01814569 0.00136987 0.01578884 0.0024408  0.0030016 ]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.01811293 0.00133892 0.01563784 0.00222931 0.00289945]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.01804175 0.0013109  0.01541133 0.00203402 0.00280426]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.0179986  0.00128078 0.01512439 0.00186587 0.00271657]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.01804446 0.00124385 0.01480201 0.0017324  0.00263679]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.01821782 0.00119762 0.01447116 0.00163553 0.0025646 ]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.01853094 0.0011426  0.01415345 0.00157227 0.00249893]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.01897607 0.00108148 0.01386129 0.00153719 0.00243827]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.01953252 0.00101793 0.01359779 0.00152418 0.00238107]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.02017199 0.00095568 0.01335899 0.00152699 0.00232593]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.02086218 0.00089801 0.0131364  0.00153899 0.00227181]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.02157056 0.0008475  0.0129188  0.00155303 0.00221812]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.0222685  0.00080595 0.01269372 0.00156185 0.00216488]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.02293526 0.00077443 0.0124495  0.00155892 0.00211263]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.02356007 0.00075337 0.01217703 0.00153957 0.00206193]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.02414197 0.0007425  0.01187034 0.00150166 0.00201288]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.02468787 0.00074087 0.01152634 0.00144566 0.00196515]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.02520945 0.00074709 0.0111446  0.00137413 0.00191855]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.02571919 0.0007596  0.01072753 0.00129087 0.00187351]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.0262275  0.00077696 0.0102808  0.00120018 0.00183093]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.02674331 0.00079793 0.00981366 0.00110615 0.0017915 ]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.02727276 0.0008214  0.00933861 0.00101221 0.00175534]\n",
      "\n",
      " x [-9.7094228  -6.30318087 -7.58721336  5.63902267 -2.27811707]\n",
      "y 0\n",
      "output [0.02780781 0.00084648 0.00887135 0.00092101 0.00172218]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "####\n",
    "#Give the index of the minimum element of a list\n",
    "####\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "learning_rate = 0.01\n",
    "hidden_units = 8\n",
    "input_vector_size = 5\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, input_vector_size])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "# Stacked dense layers example\n",
    "output = tf.layers.dense(x, hidden_units)\n",
    "output = tf.nn.tanh(output)\n",
    "output = tf.layers.dense(output, hidden_units)\n",
    "output = tf.nn.tanh(output)\n",
    "output = tf.layers.dense(output, input_vector_size)\n",
    "output = tf.nn.softmax(output, axis=0)\n",
    "\n",
    "y_onehot = tf.one_hot(y, input_vector_size)\n",
    "cross_entropy_loss = tf.reduce_mean(-tf.reduce_sum(y_onehot*tf.log(output+1e-10), reduction_indices=[1]))\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cross_entropy_loss)\n",
    "\n",
    "def generate_data(amount):\n",
    "    x = np.random.uniform(low=-10, high=10, size=(amount, input_vector_size))\n",
    "    y = np.argmin(x, axis=-1)\n",
    "    return x, y\n",
    "\n",
    "loss = []\n",
    "\n",
    "x_data, y_data = generate_data(100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for _ in range(50):\n",
    "        feed = {x:x_data, y:y_data}\n",
    "        loss_, _, probabilities = sess.run([cross_entropy_loss, train_op, output], feed)\n",
    "        loss.append(loss_)\n",
    "        feed = {x:[x_data[0]]}\n",
    "        print('\\n x', x_data[0])\n",
    "        print('y', y_data[0])\n",
    "        print('output', probabilities[0])\n",
    "\n",
    "plt.plot(np.arange(len(loss)),loss)\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 6.7200537 -3.104074   2.0084226 -4.9884796 -3.583696  -3.3608441\n",
      "   8.882267   1.2932125]\n",
      " [ 4.149077  -5.721452   2.8778791 -9.118938  -6.7176676 -5.8348165\n",
      "  11.175782   2.572101 ]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.9999971  -0.99598217  0.96461785 -0.9999071  -0.9984585  -0.9975939\n",
      "   0.99999994  0.85996556]\n",
      " [ 0.9995022  -0.99997854  0.99369097 -1.         -0.9999971  -0.9999829\n",
      "   1.          0.9884014 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.7446182   0.25787395 -0.34491152 -0.3939872   1.1907474   0.37052903\n",
      "   0.7758334   0.26336858]\n",
      " [-1.7878999   0.21960437 -0.370683   -0.4387458   1.2222266   0.37474382\n",
      "   0.7153457   0.26629022]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.9407599   0.2523059  -0.33185512 -0.37479246  0.83081055  0.35445437\n",
      "   0.6503087   0.25744358]\n",
      " [-0.94553846  0.21614096 -0.354589   -0.4126043   0.84030974  0.3581341\n",
      "   0.614018    0.2601695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n"
     ]
    }
   ],
   "source": [
    "x = output\n",
    "output = tf_print(output, [output], message='1')\n",
    "output = tf.layers.dense(output, hidden_units)\n",
    "output = tf_print(output, [output], message='2')\n",
    "output = tf.nn.tanh(output)\n",
    "output = tf_print(output, [output], message='3')\n",
    "output = tf.layers.dense(output, hidden_units)\n",
    "output = tf_print(output, [output], message='4')\n",
    "output = tf.nn.tanh(output)\n",
    "output = tf_print(output, [output], message='5')\n",
    "output = tf.layers.dense(output, 5)\n",
    "output = tf.nn.softmax(output, axis=0)\n",
    "output = tf_print(output, [output], message='6')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for _ in range(50):\n",
    "        feed = {x:[[float('NaN'),1,2,3,4], [6,2,6,2,5],[5,2,6,7,8]]}\n",
    "        probabilities = sess.run([output], feed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"gradients/dense/MatMul_grad/MatMul_1:0\", shape=(5, 32), dtype=float32) <tf.Variable 'dense/kernel:0' shape=(5, 32) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name dense/kernel:0 is illegal; using dense/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name dense/kernel:0_grad is illegal; using dense/kernel_0_grad instead.\n",
      "Tensor(\"gradients/dense/BiasAdd_grad/BiasAddGrad:0\", shape=(32,), dtype=float32) <tf.Variable 'dense/bias:0' shape=(32,) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name dense/bias:0 is illegal; using dense/bias_0 instead.\n",
      "INFO:tensorflow:Summary name dense/bias:0_grad is illegal; using dense/bias_0_grad instead.\n",
      "Tensor(\"gradients/dense_1/MatMul_grad/MatMul_1:0\", shape=(32, 32), dtype=float32) <tf.Variable 'dense_1/kernel:0' shape=(32, 32) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name dense_1/kernel:0 is illegal; using dense_1/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name dense_1/kernel:0_grad is illegal; using dense_1/kernel_0_grad instead.\n",
      "Tensor(\"gradients/dense_1/BiasAdd_grad/BiasAddGrad:0\", shape=(32,), dtype=float32) <tf.Variable 'dense_1/bias:0' shape=(32,) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name dense_1/bias:0 is illegal; using dense_1/bias_0 instead.\n",
      "INFO:tensorflow:Summary name dense_1/bias:0_grad is illegal; using dense_1/bias_0_grad instead.\n",
      "Tensor(\"gradients/dense_2/MatMul_grad/MatMul_1:0\", shape=(32, 5), dtype=float32) <tf.Variable 'dense_2/kernel:0' shape=(32, 5) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name dense_2/kernel:0 is illegal; using dense_2/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name dense_2/kernel:0_grad is illegal; using dense_2/kernel_0_grad instead.\n",
      "Tensor(\"gradients/dense_2/BiasAdd_grad/BiasAddGrad:0\", shape=(5,), dtype=float32) <tf.Variable 'dense_2/bias:0' shape=(5,) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name dense_2/bias:0 is illegal; using dense_2/bias_0 instead.\n",
      "INFO:tensorflow:Summary name dense_2/bias:0_grad is illegal; using dense_2/bias_0_grad instead.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "####\n",
    "#Give the index of the minimum element of a list\n",
    "####\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "learning_rate = 0.01\n",
    "hidden_units = 32\n",
    "input_vector_size = 5\n",
    "\n",
    "def get_grads_and_vars(cost):\n",
    "    tvars = tf.trainable_variables() #Get a list of all trainable variables\n",
    "    grads = tf.gradients(cost, tvars) #Get the gradients of all trainable variables wrt cost\n",
    "    grads_and_vars = list(zip(grads, tvars)) #list because zip iterator needs to be read twice\n",
    "    return grads_and_vars\n",
    "\n",
    "def create_summary(loss, grads_and_vars):                                             \n",
    "    tf.summary.scalar(\"loss\", loss) #Create scalar plot of loss\n",
    "    for g, v in grads_and_vars:                                                       \n",
    "        print(g,v)\n",
    "        tf.summary.histogram(v.name, v) #Plot histrogram of weights and bias                       \n",
    "        tf.summary.histogram(v.name + '_grad', g) #Plot histogram of gradients          \n",
    "    return tf.summary.merge_all() #Merge the summary operators into a single summary operator\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, input_vector_size], name='x')\n",
    "y = tf.placeholder(tf.int32, [None], name='y')\n",
    "\n",
    "# Stacked dense layers example\n",
    "output = tf.layers.dense(x, hidden_units)\n",
    "output = tf.nn.tanh(output)\n",
    "output = tf.layers.dense(output, hidden_units)\n",
    "output = tf.nn.tanh(output)\n",
    "output = tf.layers.dense(output, input_vector_size)\n",
    "output = tf.nn.softmax(output, axis=1)\n",
    "\n",
    "y_onehot = tf.one_hot(y, input_vector_size)\n",
    "cross_entropy_loss = tf.reduce_mean(-tf.reduce_sum(y_onehot*tf.log(output+1e-10), reduction_indices=[1]))\n",
    "\n",
    "grads_and_vars = get_grads_and_vars(cross_entropy_loss)\n",
    "#This line is changed because gradients are already calculated\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_op = optimizer.apply_gradients(grads_and_vars)\n",
    "\n",
    "summary_op = create_summary(cross_entropy_loss, grads_and_vars)\n",
    "writer = tf.summary.FileWriter('train', sess.graph) #Create a writer for tensorboard and save at ./train/\n",
    "\n",
    "\n",
    "def generate_data(amount):\n",
    "    x = np.random.uniform(low=-10, high=10, size=(amount, input_vector_size))\n",
    "    y = np.argmin(x, axis=-1)\n",
    "    return x, y\n",
    "\n",
    "x_data, y_data = generate_data(100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(50):\n",
    "        feed = {x:x_data, y:y_data}\n",
    "        #Set to trace all metadata for tensorboard\n",
    "        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "        run_metadata = tf.RunMetadata()\n",
    "        _, summary = sess.run([train_op, summary_op], feed,\n",
    "                               options=run_options,\n",
    "                               run_metadata=run_metadata)\n",
    "        #Get the summaries and log it as the 'i'th run\n",
    "        writer.add_summary(summary, i)\n",
    "        #Get metadata such as memory usage and computation time\n",
    "        writer.add_run_metadata(run_metadata, 'step %d' % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
