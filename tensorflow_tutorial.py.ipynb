{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n",
      "40.0\n",
      "40.0\n",
      "Tensor(\"add:0\", shape=(), dtype=float32)\n",
      "40.0\n",
      "40.0\n",
      "Tensor(\"zeros:0\", shape=(2, 2), dtype=float32)\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "w = tf.constant(5.0)\n",
    "x = tf.constant(6.0)\n",
    "b = tf.constant(10.0)\n",
    "\n",
    "#Important: c is a computation graph that connects w, x and b,\n",
    "#and outputs w*x + b.\n",
    "#c is a tensor and has no value until evaluation.\n",
    "c = w*x + b \n",
    "with tf.Session() as sess: \n",
    "    print(sess.run(w))  # 5.0\n",
    "    print(sess.run(c))  # 40.0\n",
    "    print(c.eval()) # 40.0\n",
    "    print(c) # Tensor(\"add_2:0\", shape=(), dtype=float32)\n",
    "\n",
    "sess = tf.Session()\n",
    "print(c.eval(session=sess))  #40.0\n",
    "print(sess.run(c)) # 40.0\n",
    "\n",
    "t = tf.zeros((2,2))\n",
    "with sess.as_default():\n",
    "    print(t) #Tensor(\"zeros:0\", shape=(2, 2), dtype=float32) \n",
    "    print(t.eval()) # [[ 0. 0.]\n",
    "                    #  [ 0. 0.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constant and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14159\n",
      "[ 9 25]\n",
      "Tensor(\"Const_4:0\", shape=(), dtype=int16)\n",
      "Tensor(\"sub:0\", shape=(), dtype=int16)\n",
      "180\n"
     ]
    }
   ],
   "source": [
    "#0-d tensors\n",
    "mammal = tf.constant(\"Elephant\", tf.string)\n",
    "ignition = tf.constant(451, tf.int16)\n",
    "floating = tf.constant(3.14159265359, tf.float32)\n",
    "\n",
    "#2-d tensors\n",
    "xor = tf.constant([[False, True],[True, False]], tf.bool)\n",
    "cool_numbers  = tf.constant([3.14159, 2.71828], tf.float32)\n",
    "squarish_squares = tf.constant([ [4, 9], [16, 25] ], tf.int32)\n",
    "\n",
    "pi = cool_numbers[0]\n",
    "my_column_vector = squarish_squares[:, 1]\n",
    "ss_shape = tf.shape(squarish_squares) # [2,2]\n",
    "\n",
    "print(pi.eval(session=sess)) # 3.14159\n",
    "print(my_column_vector.eval(session=sess)) #[ 9 25]\n",
    "\n",
    "print(ignition) # Tensor(\"Const_10:0\", shape=(), dtype=int16)\n",
    "ignition -= 271\n",
    "print(ignition) # Tensor(\"sub_1:0\", shape=(), dtype=int16)\n",
    "print(ignition.eval(session=sess)) # 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.4 3.3]\n",
      " [1.1 2.2]]\n",
      "[[[0 0 0]\n",
      "  [0 0 0]]]\n"
     ]
    }
   ],
   "source": [
    "#Discard tensors and variables previously defined\n",
    "#tf.reset_default_graph()\n",
    "#A name has to be given to variables.\n",
    "#The name can be used in tensorboard or variables manipulations\n",
    "#Inputs: get_variable(name, shape=None, dtype=None, initializer=None)\n",
    "w1 = tf.get_variable(\"floatypointfloat\", [2,2],\n",
    "                     initializer=tf.constant_initializer([[4.4, 3.3], [1.1, 2.2]]))\n",
    "w2 = tf.get_variable(\"zeros\", [1, 2, 3], dtype=tf.int32,\n",
    "                     initializer=tf.zeros_initializer)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #Variables have to be initialized before use.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(w1)) #[[4.3, 3.2], [1.1, 2.2]]\n",
    "    print(sess.run(w2)) #[[[0,0,0],[0,0,0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Lower level api for creating variable object\n",
    "# Use get_variable when possible \n",
    "i_am_zero_variable = tf.Variable(0.0)\n",
    "i_am_zero_variable2 = tf.get_variable('zero', [], initializer=tf.constant_initializer(0))\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(i_am_zero_variable.eval()) #0.0\n",
    "    print(i_am_zero_variable2.eval()) #0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_cant_be_changed = tf.constant(101)\n",
    "i_can_be_changed = tf.get_variable(\"can_be_changed\", [1],\n",
    "                                   dtype=tf.int32,\n",
    "                                   initializer=tf.constant_initializer([101]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101]\n",
      "<tf.Variable 'can_be_changed:0' shape=(1,) dtype=int32_ref>\n",
      "[5]\n"
     ]
    }
   ],
   "source": [
    "assign_op = i_can_be_changed.assign([5])\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(i_can_be_changed))\n",
    "    sess.run(assign_op)\n",
    "    print(i_can_be_changed)\n",
    "    print(sess.run(i_can_be_changed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'assign'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-11eae9e334b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0massign_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi_cant_be_changed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#AttributeError: 'Tensor' object has no attribute 'assign'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'assign'"
     ]
    }
   ],
   "source": [
    "assign_op = i_cant_be_changed.assign([5]) #AttributeError: 'Tensor' object has no attribute 'assign'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106\n"
     ]
    }
   ],
   "source": [
    "i_cant_be_changed = tf.constant(101)\n",
    "#A new tensor will be created.\n",
    "#The original tensor has not been modified\n",
    "i_cant_be_changed += 5\n",
    "with tf.Session() as sess:\n",
    "    print(i_cant_be_changed.eval()) #106"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs and Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1., 2., 3.], dtype=float32), array([1., 4., 9.], dtype=float32)]\n",
      "[ 0.  0. 25.]\n"
     ]
    }
   ],
   "source": [
    "# Define a placeholder that expects a vector of three floating-point values\n",
    "x = tf.placeholder(tf.float32, shape=[3]) #If the shape is None, you can feed a tensor of any shape.\n",
    "y = tf.square(x)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Value of 'y' depends on the value fed to x.\n",
    "    print(sess.run([x, y], {x: [1.0, 2.0, 3.0]}))  # [array([1., 2., 3.], dtype=float32),\n",
    "                                                   #  array([1., 4., 9.], dtype=float32)]\n",
    "    print(sess.run(y, {x: [0.0, 0.0, 5.0]}))       # [0., 0., 25.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape [3]\n\t [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[3], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\t [[Node: Square/_1 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_5_Square\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'Placeholder', defined at:\n  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/lib/python3.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/usr/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 523, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1758, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/usr/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n    handler_func(fileobj, events)\n  File \"/usr/lib/python3.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python3.7/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/lib/python3.7/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/lib/python3.7/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/lib/python3.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-9-95289326c574>\", line 2, in <module>\n    x = tf.placeholder(tf.float32, shape=[3]) #If the shape is None, you can feed a tensor of any shape.\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\", line 1735, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 4925, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape [3]\n\t [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[3], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\t [[Node: Square/_1 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_5_Square\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape [3]\n\t [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[3], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\t [[Node: Square/_1 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_5_Square\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7bd74b1ec0d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# a `tf.placeholder()` when evaluating a tensor that depends on it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1291\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape [3]\n\t [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[3], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\t [[Node: Square/_1 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_5_Square\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'Placeholder', defined at:\n  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/lib/python3.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/usr/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 523, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1758, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/usr/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n    handler_func(fileobj, events)\n  File \"/usr/lib/python3.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python3.7/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/lib/python3.7/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/lib/python3.7/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/lib/python3.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2901, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-9-95289326c574>\", line 2, in <module>\n    x = tf.placeholder(tf.float32, shape=[3]) #If the shape is None, you can feed a tensor of any shape.\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\", line 1735, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 4925, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"/usr/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape [3]\n\t [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[3], _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\t [[Node: Square/_1 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_5_Square\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "# Raises `tf.errors.InvalidArgumentError`, because you must feed a value for\n",
    "# a `tf.placeholder()` when evaluating a tensor that depends on it.\n",
    "with tf.Session() as sess:\n",
    "    sess.run(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape () for Tensor 'Placeholder:0', which has shape '(3,)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-4f33f7522bfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# of placeholder `x`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m37.0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1074\u001b[0m                              \u001b[0;34m'which has shape %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[0;32m-> 1076\u001b[0;31m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1077\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape () for Tensor 'Placeholder:0', which has shape '(3,)'"
     ]
    }
   ],
   "source": [
    "# Raises `ValueError`, because the shape of `37.0` does not match the shape\n",
    "# of placeholder `x`.\n",
    "with tf.Session() as sess:\n",
    "    sess.run(y, {x: 37.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight:[[0.96151656]], Bias:[[0.04617509]]\n",
      "Final Loss:192.0009307861328\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define number of data and learning rate\n",
    "n_samples = 1000\n",
    "learning_rate = 2E-5 \n",
    "\n",
    "# Generate some input data\n",
    "x_data = np.linspace(0,40*np.pi, n_samples)\n",
    "f = lambda x: x + 20*np.sin(x/10) \n",
    "y_data = f(x_data)\n",
    "\n",
    "# Reshape it to 1-dimension inputs\n",
    "x_data = np.reshape(x_data, (n_samples,1))\n",
    "y_data = np.reshape(y_data, (n_samples,1))\n",
    "\n",
    "# For logging the lost function\n",
    "loss_log = []\n",
    "\n",
    "# Define placeholders for input\n",
    "X = tf.placeholder(tf.float32, shape=(n_samples, 1))\n",
    "y = tf.placeholder(tf.float32, shape=(n_samples, 1))\n",
    "\n",
    "# Define variables to be learned\n",
    "w = tf.get_variable(\"weights\", (1, 1),\n",
    "    initializer=tf.random_normal_initializer())\n",
    "b = tf.get_variable(\"bias\", (1,1),\n",
    "    initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "# Define structure of the loss\n",
    "y_pred = w*X + b\n",
    "# Mean Squared Error\n",
    "loss = tf.reduce_mean((y - y_pred)**2)\n",
    "\n",
    "# Operator to minimize the loss\n",
    "train_opt = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize Variables in graph\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Gradient descent 500 times (500 is arbitrary)\n",
    "    for i in range(500):\n",
    "        # Do gradient descent step\n",
    "        _, loss_val = sess.run([train_opt, loss], feed_dict={X: x_data, y: y_data})\n",
    "        loss_log.append(loss_val)\n",
    "    pred = sess.run(y_pred, feed_dict={X: x_data}) \n",
    "    weight, bias = sess.run([w, b])\n",
    "    print('Weight:{}, Bias:{}'.format(weight, bias))\n",
    "    print('Final Loss:{}'.format(loss_log[-1]))\n",
    "\n",
    "# Plot the loss function\n",
    "plt.plot(np.arange(len(loss_log)),loss_log)\n",
    "plt.title('Linear Regresson Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.figure()\n",
    "# Plot the data and predictions\n",
    "plt.title('Linear Regresson Data and Fit')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.plot(x_data, y_data)\n",
    "plt.plot(x_data, pred)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Higher Level Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight:[[0.9366367]], Bias:[2.159174]\n",
      "Final Loss:183.09938049316406\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_samples = 1000\n",
    "learning_rate = 2E-2\n",
    "\n",
    "x_data = np.linspace(0,40*np.pi, n_samples)\n",
    "f = lambda x: x + 20*np.sin(x/10) \n",
    "y_data = f(x_data)\n",
    "\n",
    "x_data = np.reshape(x_data, (n_samples,1))\n",
    "y_data = np.reshape(y_data, (n_samples,1))\n",
    "\n",
    "loss_log = []\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(n_samples, 1))\n",
    "y = tf.placeholder(tf.float32, shape=(n_samples, 1))\n",
    "\n",
    "# Create an object of dense layer \n",
    "dense = tf.layers.Dense(1) # 1 for output dimension\n",
    "y_pred = dense(X)\n",
    "loss = tf.reduce_mean((y - y_pred)**2)\n",
    "\n",
    "# ADAM Optimizer: a more advanced version of gradient descent\n",
    "train_opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(500):\n",
    "      _, loss_val = sess.run([train_opt, loss], feed_dict={X: x_data, y: y_data})\n",
    "      loss_log.append(loss_val)\n",
    "    pred = sess.run(y_pred, feed_dict={X: x_data}) \n",
    "    weight, bias = sess.run([dense.kernel, dense.bias])\n",
    "    print('Weight:{}, Bias:{}'.format(weight, bias))\n",
    "    print('Final Loss:{}'.format(loss_log[-1]))\n",
    "\n",
    "plt.plot(np.arange(len(loss_log)),loss_log)\n",
    "plt.title('Linear Regresson Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.figure()\n",
    "\n",
    "plt.title('Linear Regresson Data and Fit')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.plot(x_data, y_data)\n",
    "plt.plot(x_data, pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minibatch and rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Loss 0.15744348\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3Xl8XHW9//HXZ2ayNFvXtHQlLS2bCAULVLYLCIiAbCrCBUXhWhdwQe/1iveHev1dr9yr4C6KsnivgqgFQUQWQUQqSzcopQvdaeiStE2aPZmZfO4fcyadJNM2TTMzSeb9fDzmkTnfc2byOdM073y/55zvMXdHRESkp1CuCxARkcFJASEiImkpIEREJC0FhIiIpKWAEBGRtBQQIiKSlgJCRETSUkCI9JGZbTSzc3Jdh0i2KCBERCQtBYTIQTKzj5nZWjPbZWaPmNmkoN3M7DtmVmNmu81smZkdE6y7wMxWmFmjmb1lZv+c270Q6U0BIXIQzOxs4JvAFcBEYBPw62D1ecAZwOHAKOCDwM5g3V3Ax929HDgGeCaLZYv0SSTXBYgMcVcDd7v7EgAzuxmoM7MqIAqUA0cCL7v7ypTXRYGjzexVd68D6rJatUgfqAchcnAmkeg1AODuTSR6CZPd/Rngh8CPgO1mdqeZVQSbvg+4ANhkZn81s3dmuW6R/VJAiBycLcChyQUzKwXGAm8BuPv33f0dwNtIDDX9S9C+0N0vAcYDvwd+k+W6RfZLASFyYArMrDj5IPGL/aNmNtvMioD/BF5y941mdqKZnWxmBUAz0AbEzazQzK42s5HuHgUagHjO9khkLxQQIgfmMaA15XE6cAswH9gKHAZcGWxbAfyMxPGFTSSGnr4drPsQsNHMGoBPANdkqX6RPjPdMEhERNJRD0JERNJSQIiISFoKCBERSUsBISIiaQ3pK6nHjRvnVVVVuS5DRGRIWbx48Q53r9zfdkM6IKqqqli0aFGuyxARGVLMbNP+t9IQk4iI7IUCQkRE0lJAiIhIWgoIERFJSwEhIiJpKSBERCQtBYSIiKSV9wHxyuZ6lr+1O9dliIgMOkP6QrmBcOmPFgCw8dYLc1yJiMjgkrEehJndbWY1ZrY8pe0BM3sleGw0s1eC9ioza01Z95NM1SUiIn2TyR7EvSRu2P4/yQZ3/2DyuZndBqSO7axz99kZrEdERA5AxgLC3Z8zs6p068zMgCuAszP1/UVE5ODk6iD16cB2d1+T0jbdzJaa2V/N7PS9vdDM5pnZIjNbVFtbm/lKRUTyVK4C4irg/pTlrcA0dz8e+Dxwn5lVpHuhu9/p7nPcfU5l5X5nqxURkX7KekCYWQS4HHgg2ebu7e6+M3i+GFgHHJ7t2kREZI9c9CDOAVa5e3WywcwqzSwcPJ8BzALW56A2EREJZPI01/uBF4AjzKzazK4PVl1J9+ElgDOAZWb2KvA74BPuvitTtYmIyP5l8iymq/bS/pE0bfOB+ZmqRUREDlzeT7UhIiLpKSBERCQtBYSIiKSlgBARkbTyOiDcPdcliIgMWnkdEJ3KBxGRvcrrgIh1dua6BBGRQSuvAyKuLoSIyF7ldUDEFBAiInuV1wHxH4+uyHUJIiKDVt4GRLzT+c2i6v1vKCKSp/I2IDpiOkAtIrIveRsQ7bF4rksQERnU8jgg1IMQEdmXvA2Itqh6ECIi+5K3AaEehIjIvuVvQEQVECIi+5K/AaGD1CIi+5TJe1LfbWY1ZrY8pe1rZvaWmb0SPC5IWXezma01s9Vm9u5M1ZWkISYRkX3LZA/iXuD8NO3fcffZweMxADM7GrgSeFvwmh+bWTiDtekgtYjIfmQsINz9OWBXHze/BPi1u7e7+wZgLXBSpmqD3j0I3RtCRKS7XByDuNHMlgVDUKODtsnA5pRtqoO2jNhS38o3/7SyW5vyQUSku2wHxB3AYcBsYCtwW9BuabZN+yvbzOaZ2SIzW1RbW9uvIupaOti8q7VbW1wJISLSTVYDwt23u3vc3TuBn7FnGKkamJqy6RRgy17e4053n+PucyorK/tVx7iyol5tnQoIEZFushoQZjYxZfEyIHmG0yPAlWZWZGbTgVnAy5mqY0xpYa825YOISHeRTL2xmd0PnAmMM7Nq4KvAmWY2m8Tw0Ubg4wDu/rqZ/QZYAcSAG9w9Y6cZFYR756LuLici0l3GAsLdr0rTfNc+tv8G8I1M1bM/GmISEekub6+k7kkdCBGR7hQQAV0HISLSXcaGmAa7O64+gaWb69nZ1MH8JdU6BiEi0kPe9iDe8/aJfPmCo5g9bRSgISYRkZ7yNiCSQsElehpiEhHpTgFhiYRQD0JEpDsFRNCD0FQbIiLdKSCSPQh1IUREulFABAGhDoSISHcKiOAT0JXUIiLdKSCCHoSOQYiIdKeA6BpiUkCIiKRSQOg0VxGRtBQQydNclRAiIt3kfUBY0IN4z/f+xr/+blmOqxERGTzyPiDCoT23w35g0eYcViIiMrjkfUCk5IOIiKRQQJgSQkQkHQWEuhAiImllLCDM7G4zqzGz5Slt3zKzVWa2zMweMrNRQXuVmbWa2SvB4yeZqqunnvmw9M26bH1rEZFBLZM9iHuB83u0PQUc4+7HAm8AN6esW+fus4PHJzJYVzc9h5gu+/Hfs/WtRUQGtYwFhLs/B+zq0faku8eCxReBKZn6/n2V7hCErqoWEcntMYjrgD+lLE83s6Vm9lczO31vLzKzeWa2yMwW1dbWHnQR4TQJUdvYftDvKyIy1OUkIMzs34AY8KugaSswzd2PBz4P3GdmFele6+53uvscd59TWVl50LWkO0i9aVfLQb+viMhQl/WAMLNrgYuAqz0Yy3H3dnffGTxfDKwDDs9GPelOYtqsgBARyW5AmNn5wL8CF7t7S0p7pZmFg+czgFnA+izV1KutNRrPxrcWERnUIpl6YzO7HzgTGGdm1cBXSZy1VAQ8FfxifjE4Y+kM4OtmFgPiwCfcfVfaNx5g6Y5BaOI+EZEMBoS7X5Wm+a69bDsfmJ+pWvYl3ZXUCggREV1JnfY0VwWEiIgCQj0IEZG9yPuASEf3pxYRUUCwq7mjV1unehAiIgqIEw4dxQnTRnHc1FFdbfHOHBYkIjJI5H1AlBRGePBTp3LEhLKutninEkJEJO8DIin1sIOOQYiIKCC6pEaChphERBQQXTpTeg2d6kGIiCgguqRkQiyugBARUUAEUiNBPQgREQVEl9RQ0JXUIiIKiC6pnYaYAkJERAGR1O0gtQJCREQBkVRevGfmc10HISKSwftBDDVfvuAopowu4b6X3lQPQkQE9SC6lBcXcMNZMykIm45BiIiggOglFDINMYmIoIDoJWymISYRETIcEGZ2t5nVmNnylLYxZvaUma0Jvo4O2s3Mvm9ma81smZmdkMna9iYcMl0HISJC5nsQ9wLn92j7EvC0u88Cng6WAd4DzAoe84A7MlxbWgoIEZGEjAaEuz8H7OrRfAnwi+D5L4BLU9r/xxNeBEaZ2cRM1pdOWMcgRESA3ByDmODuWwGCr+OD9snA5pTtqoO2bsxsnpktMrNFtbW1A15cyNSDEBGBwXWQ2tK09fpN7e53uvscd59TWVk54EVENMQkIgLkJiC2J4eOgq81QXs1MDVluynAlizXljjNVQEhIpKTgHgEuDZ4fi3wcEr7h4OzmeYCu5NDUdkUNtN03yIi9DEgzOwwMysKnp9pZp8xs1F9eN39wAvAEWZWbWbXA7cC55rZGuDcYBngMWA9sBb4GfCpA96bARAJqwchIgJ9n4tpPjDHzGYCd5H4a/8+4IJ9vcjdr9rLqnel2daBG/pYT8boILWISEJfh5g63T0GXAZ8191vArJ+Cmo26DRXEZGEvgZE1MyuInHM4NGgrSAzJeVWogeR6ypERHKvrwHxUeCdwDfcfYOZTQd+mbmycicS0lxMIiLQx2MQ7r4C+AxAMHdSubvfuu9XDU3hkBHrVBdCRKSvZzE9a2YVZjYGeBW4x8xuz2xpuREKGepAiIj0fYhppLs3AJcD97j7O4BzMldW7oQNncUkIkLfAyISXPV8BXsOUg9L4VCIN3e18ONn1+a6FBGRnOprQHwdeAJY5+4LzWwGsCZzZeVOOPhE/vvx1bktREQkx/p6kPq3wG9TltcD78tUUblkaecMFBHJP309SD3FzB4K7g633czmm9mUTBeXC63ReK5LEBEZFPo6xHQPiek1JpG4R8MfgrZhp7k9lusSREQGhb4GRKW73+PuseBxLzDwN2MYBJoUECIiQN8DYoeZXWNm4eBxDbAzk4XligJCRCShrwFxHYlTXLcBW4H3k5h+Y9hp6dhzDEJTbohIPutTQLj7m+5+sbtXuvt4d7+UxEVzw05qD2JzXQuPL8/6PYtERAaFg7mj3OcHrIpBpKltT0Bcd+9CPvHLJdQ0tOWwIhGR3DiYgBiWFwxcddK0rufrapsB2NHUkatyRERy5mACYlgO0N9y0VH8/MNzurXVNrXnqBoRkdzZ55XUZtZI+iAwYERGKsoxM6O0KPGxFISNaNw1xCQieWmfAeHu5QP9Dc3sCOCBlKYZwFeAUcDHgNqg/cvu/thAf/++KIwkOlaF4RDReJyaRvUgRCT/9GkupoHk7quB2QBmFgbeAh4icdrsd9z929muqaeiICDaYokbB9UqIEQkDx3MMYiB8C4SM8RuynEd3SQDInlfCB2DEJF8lOuAuBK4P2X5RjNbZmZ3B7c27cXM5pnZIjNbVFtbm26Tg5YcYkr647KtvO+Ov2fke4mIDFY5CwgzKwQuZs804ncAh5EYftoK3Jbude5+p7vPcfc5lZWZmQ6qKBLu1bZ4U11GvpeIyGCVyx7Ee4Al7r4dwN23u3vc3TuBnwEn5aqwnj0IEZF8lMvfhFeRMrwU3NI06TJgedYrCiggRERycBYTgJmVAOcCH09p/m8zm03iuouNPdZlVZECQkQkNwHh7i3A2B5tH8pFLelEQoYZeI9LBN0ds2E5w4iISC/6UzkNMyOUJghimv5bRPKIAmIv4mnCoCO4cE5EJB8oIPZjXFlR1/NoXAEhIvlDAbEf48v3BESHAkJE8ogCYj8mVKQEhIaYRCSPKCD2Y2y3ISYdpBaR/KGA2I+K4oKu5zoGISL5RAGxHxUj9lwqoiEmEcknCoj9KCncM3HfRT94nvoW3Z9aRPKDAmI/el5N/eaultwUIiKSZQqIA6SLqUUkX+RkLqah4MmbzmB3a5TCcPcMvfRHC3j8c6dz5CEVOapMRCQ71IPYi8MnlHNi1RiOmzqKP9x4Wrd1n75vaY6qEhHJHgVEHxQXdP+Y1tQ05agSEZHsUUD0QbobCHXqYISIDHMKiD4oCPf+mOpbozmoREQkexQQfZAuIK67dyHX37swB9WIiGSHAqIP0g0xvbK5nqdX1XDDr5bw0NLqHFQlIpJZOTvN1cw2Ao1AHIi5+xwzGwM8AFSRuC/1Fe5el6sak3qe6prqj69t5aUNu6goLuDkGWMpK9KZwyIyPOS6B3GWu8929znB8peAp919FvB0sJxzBeF934d6TGkB1/9iETf8akmWKhIRybxcB0RPlwC/CJ7/Arg0h7V0iYRD/HreXD52+nRuuehoUm9XXRgOUdPYDsBLG3bmqEIRkYGXy/EQB540Mwd+6u53AhPcfSuAu281s/E5rK+buTPGMnfGWAC+89QbNLXHeO9xkyiKhPjd4sQxiLaoZnsVkeEjlwFxqrtvCULgKTNb1ZcXmdk8YB7AtGnTMlnfXhVGQtCeGHoaW1qYkxpERDItZ0NM7r4l+FoDPAScBGw3s4kAwdeaNK+7093nuPucysrKbJbcJXnQujAcYkyPgPCe07+KiAxROQkIMys1s/Lkc+A8YDnwCHBtsNm1wMO5qG9/kqe9FqQJiJaOeC5KEhEZcLkaYpoAPGSJo70R4D53f9zMFgK/MbPrgTeBD+Sovn1KDYjRJd0DorkjRqlOdRWRYSAnv8ncfT1wXJr2ncC7sl/RgUkOMRVEjJKicLd1Le1xKM9FVSIiA2uwneY6JCR7EIXhECWF3TO2uSOWi5JERAacAqIfIqHEhRAF4VC3e1YDXPj951lWXZ+LskREBpQCoh+SF8oVhEOMKAj3Wr9grS6YE5GhTwHRD0ayB2GMKOwdEKVFvdtERIYaBUR/BD2IwkjvISag13EJEZGhSAHRD8mpmArCIYojvQMieYxCRGQoU0D0Q/IYRCRkhNKEQUdMczKJyNCngOiH5DGIdDcSAmiPKyBEZOhTQPRD6llM6agHISLDgQKiH0K25zqIdBQQIjIcKCD6oesYRI87zf37xW8DFBAiMjwoIA5Cz8PT155SRThkdMQ1o6uIDH0KiH4IZqEl3a0fCsMh9SBEZFjQFV39kDyz1UkkxMM3nEpjW2KSvqICBYSIDA8KiH5IDi0lexDHTR3Vta4wHKJDp7mKyDCgIaZ+2OcQUyREe6yT1o44z6zazm1Prs5ydSIiA0M9iH744vlHUNPYxtzDxvZaVxgJ8eCSt3jsta20RRM9iS+cd0S2SxQROWgKiH448pAKHv306WnXtQX3pE6GA0B7LE5RmjmbREQGMw0xDbDdrdHebS2920REBrusB4SZTTWzv5jZSjN73cw+G7R/zczeMrNXgscF2a5tIDR39L4Goj5NaIiIDHa56EHEgC+4+1HAXOAGMzs6WPcdd58dPB7LQW0DZnRJQdfzuuYOFqzdwf9/dEUOKxIROTBZDwh33+ruS4LnjcBKYHK268i0606d3vW8vjXK1T9/ibue34CnO/VJRGQQyukxCDOrAo4HXgqabjSzZWZ2t5mN3str5pnZIjNbVFtbm6VKD9ynzprJVy5KdIzqWzq62tt1EZ2IDBE5CwgzKwPmA59z9wbgDuAwYDawFbgt3evc/U53n+PucyorK7NW74EKh4wPnjgVgH+d/1pXe1N7LFcliYgckJyc5mpmBSTC4Vfu/iCAu29PWf8z4NFc1HawHvrUKWypbwNIe7/q5vYY48qKsl2WiMgBy8VZTAbcBax099tT2iembHYZsDzbtQ2E46eN5sJjE7tiZqz8+vlMGlnctf4fvvUsO5vac1WeiEif5WKI6VTgQ8DZPU5p/W8ze83MlgFnATfloLYBN6IwzO9vPJXbrziuq+1Py7flsCIRkb7J+hCTuz9P71spAAzp01r3ZXx5MYeOLe1abmjTdREiMvjpSuosKS3aczyitlFDTCIy+CkgsqS0cE9nbdPOFr7/9BqefH3vQ00/eHoNizbuykZpIiJpabK+LCkt2vNRb6lv5ZlVNQD8vwuP4p9On9Fr+9ueegOego23Xpi1GkVEUqkHkSV7G2L6jz+u7LVtTDccEpFBQAGRJYXhPR/1zuaObuuae1w81xLtPeGfiEi2KSCyJHkXunTe3NXSbbk1zYywIiLZpoDIopVfP59bL397r3YFhIgMRgqILBpRGGZ8Re9pNr79xGqeWdU10wgtCggRGQQUEFk2uqSwV9uamiauu3dR13JrVBP6iUju6TTXLBtT2jsgkr7wm1eZOLKYE6ePyWJFIiLpKSCybOroEj5+xgzOPGI8z75RwwMLN1Mf3LN6/pJqAD7ph+WyRBERQAGRdaGQcfMFRwHwzsPGctrMcXz7yTf4yCmH8lp1A3cv2MDDS9/q03vFO527n9/AlSdNpby4YP8vEBE5AAqIHDt9ViWnz0rc+Oiy42HB2h2s3t7Ytf6RV7fwk2fXcc9HT+Tx5dv46iOv85Nr3sHP/7aeRZvqAFiwbgf3fOTEbqfSvrK5noeWVPO1i9+2z1NsRUT2xobyPZLnzJnjixYt2v+GQ8iqbQ2c/92/dS2ffeR4nllVQyRkxDr3/m/15QuOZN4Ze4amjrrlcVqjcZbeci6j93HcQ0Tyj5ktdvc5+9tOZzENMkceUtFteU1NI+XFEQoj+/6nWrixrut5Y1uU1uBq7LfqW7ttN5T/IBCR7NIQ0yC3eVcrl58wmaMOqeAbj/WetwngjMMreX7NDn6zcDO3PbWa7Q175nq66AfPM/+Tp9AeixMy42P/s4ibzjmc606bTizeyeJNdZw8Y2yf63mtejd/W1vLydPH0NAa4y+razhm0kiuCO6/LSLDhwJiEDp2ykiWVe/uWo53OlXjSve6/TGTKnjujVq+OH9Z2vVPvL6NO59b37X89UdXcN1p0/npc+v51hOruevaORw6toTRJYWMTXO/7A07mqkaW4KZccmPnqfT4chDytlS30pDW+KaDQWEyPCjIaZB6Nfz5vKpM/ccT7hk9iSmjB7RbZsjDynven7O0RP2+X5/XV3bq+22J1fzvy9sAuB/X9zEObc/xz/+7KVe2y19s46zvv0sv3zpTdyd5GGQVdsau8IhnXinc++CDTT28e5523a38dL6nX3aVkSyQwExCJUURvj8uYfz0KdOYcM3L+DsIycwa3wZl8yeRElhmH84vJLHP3dG1/YnTBvNxlsv5LHPnA7AuB69gORZUd+8/O0s/LdzKIqE+MEza9nW0EZ5UYRngwBJbtfaEefHz67lvT94nlseXg7AbxdtTjs1eZK785tFm/nWE6vY0dTOUyu287U/rODiHy5g085mADbuaOaSHz5PdV1Lr9e//yd/54N3vkh7bN/TjLRF47y4fif3LtjQq/2Tv1zM8rd27+WV/dPZ6fxucTVN7TEa26Is3rSLmx9cRuc+ThgQGS4G3VlMZnY+8D0gDPzc3W/d27bD8Sym/Un+e5kZVV/6I9D9pkJv7mxhdGkBZUURbrhvCaNLClm4cRcXHzeJG8+eBcCy6no27Ghm5vgyWjrifOAnL3S9/j8uPYYnV2znuTd69zpgz1lVkAiiHU2J4x2fP/dwbn/qjb3W/dhnTufyOxbQFu3kijlTmHfGYUweNYL/enwV5x49gat/nui9zP/kKbyxvZH//ONKzj5qPP983hHEO51P37+UeWfM4Od/W8+rwfDbS19+F+XFEWKdzuJNdXz0noWEQ8YLN59NR6yTexZspKE1yskzxvL+d0xJfFY7mhlfUcSW+lZeWL+LD809tFetH777ZcqLIvzo6hP447Kt3HDfEg6rLGXjzhbiQTD8+fNnMHN8Oe7edRrxkjfrKAyHOGbyyK73enH9TiaO7H5PckgEzxOvb+OsI8dTXBBmX2LxTh5dtpXzjzmk27aPvbaV5vYYH5hzcMN7u1uj/H3tDs4/5hDMjO0NbZhBcUGYSMi4+cHX+PgZh3H0pIr9v1mGReOdFIQP/u/aeKcTDqU//buxLUq80xmVZlqc4aKvZzENqoAwszDwBnAuUA0sBK5y9xXpts/HgEj142fXMqakkCtPmtbv93B3HlzyFnUtHWl7CGNLC+mIddIY3LNi460Xcu+CDXztDyt4YN5cHli4mQdTLuwrioRoj/X/hkenzxrH39bs6PfrAUIGPf/AP2piBTPHl/Hosi28ffKeYzw/uOp4Tp4+hu89vYbn1+5gXFkRi4PrS97/jik8vXI7dS29h8nmnTEDd+e3i6t5zzETqRgR4ad/Xd+1D/940jTaY5187oFXgMSw4WGVZfx93Q621LdRVhTmlodf58oTp3LDWTP5zK+X0tgW430nTOHsI8fz55XbaYvGOayyjH//w+vUtUQ5sWo018w9lHDIqCgu4MN3vwzA966czTGTR3L7U2/Q3B6jtDDCcVNHcuyUUTS1xYi7UxA2vvvnNdx07uHMnjKK7Y1t1DS0U1lexG1PvsGfV27njqtPoKk9xr/8bhmFkRDuzsgRBexo6uC4qaN48JOn8NDSt3j75JGUFIZZW9PEAws384E5Uzh2yijGlhbyh2Vb+N3iau645h2UFUWIdzpraho5rLKMO59bT3lxhA/NPZQX1u1kXW0TFSMKeNdRE2iLxnmtejcnHDqa2sZ2bvn9cqrGlXDTuYfT3B6nojjCY8u38V9/WsUd15zA8dNGs6OxnfU7mmiLdjK2tJCSwggjRxTQEY8zrqyIlzbsoigS4swjxtMWjbNyawNTRpcAcPEPn+eauYfywROn8uirW3hh/U7OO/oQlm/ZzZOvb6exLcorXzmPhuCMwIkjR9DZ6fxp+TaOmVzBxJEjKIyEaIvG+fGz67js+MlMD44TtkXjhENGS0ecVzfXc9rMcUQ7O1mxpYGQGUdNrKAwEmJ7QxtjSgspCIdYV9vEX1bV8JFTqgiHDDOjLRrnyw+9xhmzKrlk9iQAdjV3EO90CiOhgwqwoRoQ7wS+5u7vDpZvBnD3b6bbPt8DYiB1djrPr91BJGTUNrVTVhRh484Wrju1iuaOOMd89Qnee9wkfnDV8UDiB3VMaSFN7TG+/cRqTps5jjFlhVSNLeXVzfUsWLuDihEFPPLqFj599kw+++vEL8r3HjeJjTuaee2t3Rw+oYwbzprJs6treWjpW5QVRWhqjzFxZDEfPbWK7z+9lqb23sc5/vTZ03lqxXbW1DTxh1e39Fo/cWQxW3e3cfkJk/n02bO48s4X6HRoaosxvqKIzbtaegXI3px1RCWHji3l3r9v5MwjKlmyqa7bsZeDDcSBFgr+8h+oGYEPZP96BnNpYZho3OmId2IGyV81BWEjGs/e753igsQ+HOivuvHlRdS1dBCNJ34hd6R8DmZQWVZETcrdIceXF2EGdc1RzBK9lFinU1YUIRwydrcm/tAoDIdwnGjcKS4IETajOeXfqygSImRGNN7Zde2TGURC3T+3y0+YzO1XzO7PRzJkA+L9wPnu/k/B8oeAk939xpRt5gHzAKZNm/aOTZs25aTWfLNtdxujSgr2OxyyNw1tUcqLIl3DMW3ROIXhEKGgm79xRzPTxpRQ19JBxYgCCsIhdrdGaWyLUtPYzrGTR9LUHuNva3bw3uMmdb2vu9PSkTiFtyPeSUVx4nts3tXCpFEjCIesaxgo+bWmoY2m9hhb6tvY1tBGQ2uUyaNHsKu5gxOrxlDf0kFxQZjV2xq5/ITJidc0tjFyRAEFoRArtjbwxvZG3BO9hSVv1lNWFOGQkcW8uauZt08exetbdrOutpmJI4sZXVLI6m0NtETjzKwsY+LIEbxSXc/MyjI27WymprGdIw4ppy0aZ21NE41tMaaNKWF0aQEbapvzug+VAAAIT0lEQVS58NhJjCopYPGmOlo74jiJnl1rNE5pUYRIyFizvZGJo0bw7rcdwq7mDtbWNNHcHqO8OMKOpg5aozEqy4pZ+mYdpUURRpcmPuNY3FlX28SJVWNY+mY9J00fw9wZY1i6uZ7drVGmji5hzfZGXt64i5LCMBMqionFnXhnolcytqyIXc0dROOdvFXfSizuzBxfxoYdieNOhZEQ48uLaGiNUl5cwIjCMNV1rUTjnZQWRTASv/iaO+KMKAgzojDEiIIwVeNKWbGlgZaOOCWFYQrCIcZXFHHEhHIWbqyjsS1KrNOZMnoEU8eUsKG2mbg7BoTM2NXcwYSRxWytb2V7QztlRWFmTSinprGd2sb2rp+1aLyT844+hE53Xli3k9nTRrFoYx1bd7fS2pH4fIsLwjhOJGREQomeFWZsDa4xGldeREt7rCtIRxSGMQwnESxtHXGa2uMUF4RwwILPJfmrNxIyigvChENGXUsH4ZARMqMgHOLoSRVs391GQ1uUjlgn5cWJejrinUwaOYJLj5/cr/+PQzUgPgC8u0dAnOTun063vXoQIiIHbqheSV0NpB5xmwL0HkMQEZGMG2wBsRCYZWbTzawQuBJ4JMc1iYjkpUF1JbW7x8zsRuAJEqe53u3ur+e4LBGRvDSoAgLA3R8DHst1HSIi+W6wDTGJiMggoYAQEZG0FBAiIpKWAkJERNIaVBfKHSgzqwUO5lLqccDBTfwz9Gif84P2OT/0d58PdffK/W00pAPiYJnZor5cTTicaJ/zg/Y5P2R6nzXEJCIiaSkgREQkrXwPiDtzXUAOaJ/zg/Y5P2R0n/P6GISIiOxdvvcgRERkLxQQIiKSVl4GhJmdb2arzWytmX0p1/UMFDO728xqzGx5StsYM3vKzNYEX0cH7WZm3w8+g2VmdkLuKu8/M5tqZn8xs5Vm9rqZfTZoH7b7bWbFZvaymb0a7PO/B+3TzeylYJ8fCKbMx8yKguW1wfqqXNZ/MMwsbGZLzezRYHlY77OZbTSz18zsFTNbFLRl7Wc77wLCzMLAj4D3AEcDV5nZ0bmtasDcC5zfo+1LwNPuPgt4OliGxP7PCh7zgDuyVONAiwFfcPejgLnADcG/53De73bgbHc/DpgNnG9mc4H/Ar4T7HMdcH2w/fVAnbvPBL4TbDdUfRZYmbKcD/t8lrvPTrneIXs/2+6eVw/gncATKcs3Azfnuq4B3L8qYHnK8mpgYvB8IrA6eP5T4Kp02w3lB/AwcG6+7DdQAiwBTiZxRW0kaO/6OSdxf5V3Bs8jwXaW69r7sa9Tgl+IZwOPkri983Df543AuB5tWfvZzrseBDAZ2JyyXB20DVcT3H0rQPB1fNA+7D6HYBjheOAlhvl+B0MtrwA1wFPAOqDe3WPBJqn71bXPwfrdwNjsVjwgvgt8EegMlscy/PfZgSfNbLGZzQvasvazPehuGJQFlqYtH8/1HVafg5mVAfOBz7l7g1m63UtsmqZtyO23u8eB2WY2CngIOCrdZsHXIb/PZnYRUOPui83szGRzmk2HzT4HTnX3LWY2HnjKzFbtY9sB3+d87EFUA1NTlqcAW3JUSzZsN7OJAMHXmqB92HwOZlZAIhx+5e4PBs3Dfr8B3L0eeJbE8ZdRZpb8oy91v7r2OVg/EtiV3UoP2qnAxWa2Efg1iWGm7zK89xl33xJ8rSHxh8BJZPFnOx8DYiEwKzj7oRC4EngkxzVl0iPAtcHza0mM0SfbPxyc+TAX2J3stg4llugq3AWsdPfbU1YN2/02s8qg54CZjQDOIXHg9i/A+4PNeu5z8rN4P/CMB4PUQ4W73+zuU9y9isT/2Wfc/WqG8T6bWamZlSefA+cBy8nmz3auD8Lk6MDPBcAbJMZt/y3X9Qzgft0PbAWiJP6auJ7EuOvTwJrg65hgWyNxNtc64DVgTq7r7+c+n0aiG70MeCV4XDCc9xs4Flga7PNy4CtB+wzgZWAt8FugKGgvDpbXButn5HofDnL/zwQeHe77HOzbq8Hj9eTvqmz+bGuqDRERSSsfh5hERKQPFBAiIpKWAkJERNJSQIiISFoKCBERSUsBIXnNzJqCr1Vm9o8D/N5f7rH894F8f5FMU0CIJFQBBxQQwczA+9ItINz9lAOsSSSnFBAiCbcCpwfz7t8UTIb3LTNbGMyt/3EAMzvTEvefuI/ExUiY2e+DydReT06oZma3AiOC9/tV0JbsrVjw3suDuf4/mPLez5rZ78xslZn9KrhSHDO71cxWBLV8O+ufjuSlfJysTySdLwH/7O4XAQS/6He7+4lmVgQsMLMng21PAo5x9w3B8nXuviuY9mKhmc139y+Z2Y3uPjvN97qcxH0cjgPGBa95Llh3PPA2EnPoLABONbMVwGXAke7uyWk2RDJNPQiR9M4jMa/NKySmDx9L4kYsAC+nhAPAZ8zsVeBFEpOlzWLfTgPud/e4u28H/gqcmPLe1e7eSWLakCqgAWgDfm5mlwMtB713In2ggBBJz4BPe+JOXrPdfbq7J3sQzV0bJaaePofEzWmOIzFHUnEf3ntv2lOex0ncDCdGotcyH7gUePyA9kSknxQQIgmNQHnK8hPAJ4OpxDGzw4MZNXsaSeLWli1mdiSJabeTosnX9/Ac8MHgOEclcAaJCeXSCu51MdLdHwM+R2J4SiTjdAxCJGEZEAuGiu4FvkdieGdJcKC4lsRf7z09DnzCzJaRuMXjiynr7gSWmdkST0xNnfQQidtjvkpiJtovuvu2IGDSKQceNrNiEr2Pm/q3iyIHRrO5iohIWhpiEhGRtBQQIiKSlgJCRETSUkCIiEhaCggREUlLASEiImkpIEREJK3/A3lpfnNopYx+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum from RNN [[14.615639]]\n",
      "True sum 15\n",
      "Sum from RNN [[11.502426]]\n",
      "True sum 16\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "###\n",
    "# Calculates the sum of a list\n",
    "###\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def generate_data(amount):\n",
    "    x = np.random.uniform(low=-10, high=10, size=(amount,5,1))\n",
    "    y = np.sum(x, 1)\n",
    "    return x, y\n",
    "\n",
    "def read_batch(stream, batch_size):\n",
    "    head = 0\n",
    "    while True:\n",
    "        batch = []\n",
    "        for item in stream:\n",
    "            batch.append(item[head:head+batch_size])\n",
    "        yield batch\n",
    "        head += batch_size\n",
    "        head %= len(stream[0])-batch_size\n",
    "\n",
    "class SumRNN(object):\n",
    "    def __init__(self):\n",
    "        self.lstm_size = 32\n",
    "        self.learning_rate = 0.01\n",
    "        self.data_amount = 10000\n",
    "        self.batch_size = 100\n",
    "        self.loss_logs = []\n",
    "        self.input_data, self.target_data = generate_data(self.data_amount)\n",
    "        self.create_model()\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def create_rnn(lstm_size, data):\n",
    "        #Create a LSTMCell with hidden size lstm_size\n",
    "        rnn_cell = tf.contrib.rnn.LSTMCell(lstm_size) \n",
    "        #Get the zero state of rnn_cell\n",
    "        initial_state = rnn_cell.zero_state(tf.shape(data)[0], tf.float32)\n",
    "        #Create a rnn with the cell.\n",
    "        #Dynamic means the input length can be a variable\n",
    "        outputs, state = tf.nn.dynamic_rnn(rnn_cell, data, \n",
    "                                           initial_state=initial_state,\n",
    "                                           dtype=tf.float32)\n",
    "        #Get the last output only and pass it through a dense layer\n",
    "        outputs = tf.layers.dense(outputs[:,-1], 1)\n",
    "        return outputs\n",
    "\n",
    "    def create_placeholders(self):\n",
    "        self.inputs = tf.placeholder(dtype=tf.float32, shape=[None, None, 1])\n",
    "        self.targets = tf.placeholder(dtype=tf.float32, shape=[None, 1])\n",
    "        \n",
    "    @staticmethod\n",
    "    def calculate_loss(predictions, targets):\n",
    "        #L2 loss\n",
    "        loss = tf.reduce_mean(tf.square(predictions-targets))\n",
    "        return loss\n",
    "    \n",
    "    def create_model(self):\n",
    "        self.create_placeholders()\n",
    "        self.outputs = self.create_rnn(self.lstm_size, self.inputs)\n",
    "        self.loss = self.calculate_loss(self.outputs, self.targets)\n",
    "        self.train_opt = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(self.loss)\n",
    "        \n",
    "    @staticmethod\n",
    "    def plot_results(loss_logs):\n",
    "        plt.plot(np.arange(len(loss_logs)), loss_logs)\n",
    "        plt.title('Loss')\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.show()\n",
    "        \n",
    "    def train(self):\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            #Create a iterator that feed minibatches\n",
    "            batch_feeder = read_batch([self.input_data, self.target_data], self.batch_size)\n",
    "            for _ in range(500):\n",
    "                batch_inputs, batch_targets = next(batch_feeder)\n",
    "                feed_dict = {self.inputs: batch_inputs,\n",
    "                             self.targets: batch_targets}\n",
    "                loss, _ = sess.run([self.loss, self.train_opt], feed_dict=feed_dict)\n",
    "                self.loss_logs.append(loss)\n",
    "            print('Final Loss', loss)\n",
    "            #The noisy loss comes from the minibatch\n",
    "            self.plot_results(self.loss_logs)\n",
    "\n",
    "            #Test the rnn\n",
    "            self.infer(sess, [1,2,3,4,5])\n",
    "            #The rnn does not generalize well to lengths other than 5\n",
    "            self.infer(sess, [1,2,3,4,5,1])\n",
    "\n",
    "                \n",
    "    def infer(self, sess, test_input):            \n",
    "            test_input = np.reshape(test_input, (1,len(test_input),1))\n",
    "            print('Sum from RNN', sess.run(self.outputs, feed_dict={self.inputs:test_input}))\n",
    "            print('True sum', test_input.sum())\n",
    "            \n",
    "model = SumRNN()\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ij,j->i:  [26 62]\n",
      "ij,j->ij:  [[ 3  8 15]\n",
      " [12 20 30]]\n",
      "ii->:  21\n"
     ]
    }
   ],
   "source": [
    "#Almost the same as np.einsum (See: https://github.com/tensorflow/tensorflow/issues/4722)\n",
    "#Does einsum summation convention\n",
    "import tensorflow as tf\n",
    "a = tf.constant([[1,2,3], [4,5,6]])\n",
    "b = tf.constant([3,4,5])\n",
    "#Tensor contraction a_ij*b^j -> c_i or matrix multiplication\n",
    "c = tf.einsum('ij,j->i', a, b)\n",
    "#broadcasted multiplication a_ij*b^j -> c_ij (matrix multiplication without summing)\n",
    "d = tf.einsum('ij,j->ij', a, b)\n",
    "#Trace of a\n",
    "e = tf.einsum('ii->', a)\n",
    "with tf.Session() as sess:\n",
    "    print('ij,j->i: ', c.eval())  #ij,j->i:  [26 62]\n",
    "    print('ij,j->ij: ', d.eval()) #ij,j->ij:  [[ 3  8 15]\n",
    "                                  #            [12 20 30]]\n",
    "    print('ii->: ', e.eval())     #ii->:  21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Restore Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in path: /tmp/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Create some variables.\n",
    "v1 = tf.get_variable(\"v1\", shape=[3], initializer = tf.zeros_initializer)\n",
    "v2 = tf.get_variable(\"v2\", shape=[5], initializer = tf.zeros_initializer)\n",
    "\n",
    "inc_v1 = v1.assign(v1+1)\n",
    "dec_v2 = v2.assign(v2-1)\n",
    "\n",
    "# Add an op to initialize the variables.\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# Add ops to save and restore ALL the variables.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Later, launch the model, initialize the variables, do some work, and save the\n",
    "# variables to disk.\n",
    "with tf.Session() as sess:\n",
    "  sess.run(init_op)\n",
    "  # Do some work with the model.\n",
    "  inc_v1.op.run()\n",
    "  dec_v2.op.run()\n",
    "  # Save the variables to disk.\n",
    "  save_path = saver.save(sess, \"/tmp/model.ckpt\")\n",
    "  print(\"Model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "Model restored.\n",
      "v1 : [1. 1. 1.]\n",
      "v2 : [-1. -1. -1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Create some variables.\n",
    "v1 = tf.get_variable(\"v1\", shape=[3])\n",
    "v2 = tf.get_variable(\"v2\", shape=[5])\n",
    "\n",
    "# Add ops to save and restore ALL the variables.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Later, launch the model, use the saver to restore variables from disk, and\n",
    "# do some work with the model.\n",
    "with tf.Session() as sess:\n",
    "  # Restore variables from disk.\n",
    "  saver.restore(sess, \"/tmp/model.ckpt\")\n",
    "  print(\"Model restored.\")\n",
    "  # Check the values of the variables\n",
    "  print(\"v1 : %s\" % v1.eval()) #v1 : [1. 1. 1.]\n",
    "  print(\"v2 : %s\" % v2.eval()) #v2 : [-1. -1. -1. -1. -1.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trump Tweets Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-18-718c3b92dadf>:89: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/trump_tweets.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-718c3b92dadf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-718c3b92dadf>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0mlm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCharRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m     \u001b[0mlm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-718c3b92dadf>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m                 \u001b[0mbatch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-718c3b92dadf>\u001b[0m in \u001b[0;36mread_batch\u001b[0;34m(stream, batch_size)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-718c3b92dadf>\u001b[0m in \u001b[0;36mread_data\u001b[0;34m(filename, vocab, window, overlap)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverlap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/trump_tweets.txt'"
     ]
    }
   ],
   "source": [
    "\"\"\" A clean, no_frills character-level generative language model.\n",
    "\n",
    "CS 20: \"TensorFlow for Deep Learning Research\"\n",
    "cs20.stanford.edu\n",
    "Danijar Hafner (mail@danijar.com)\n",
    "& Chip Huyen (chiphuyen@cs.stanford.edu)\n",
    "Lecture 11\n",
    "\"\"\"\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import random\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def safe_mkdir(path):\n",
    "    \"\"\" Create a directory if there isn't one already. \"\"\"\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "def vocab_encode(text, vocab):\n",
    "    return [vocab.index(x) for x in text if x in vocab]\n",
    "\n",
    "def vocab_decode(array, vocab):\n",
    "    return ''.join([vocab[x] if x>=0 else 'PAD' for x in array])\n",
    "\n",
    "def read_data(filename, vocab, window, overlap):\n",
    "    lines = [line.strip() for line in open(filename, 'r').readlines()]\n",
    "    while True:\n",
    "        random.shuffle(lines)\n",
    "\n",
    "        for text in lines:\n",
    "            text = vocab_encode(text, vocab)\n",
    "            for start in range(0, len(text), overlap):\n",
    "                chunk = text[start: start + window]\n",
    "                chunk += [-1]*(window-len(chunk))\n",
    "                yield chunk\n",
    "\n",
    "def read_batch(stream, batch_size):\n",
    "    batch = []\n",
    "    for element in stream:\n",
    "        batch.append(element)\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    yield batch\n",
    "\n",
    "class CharRNN(object):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.path = 'data/' + model + '.txt'\n",
    "        if 'trump' in model:\n",
    "            self.vocab = (\" $%'()+,-./123456790:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "                    \"'\\\"_abcdefghijklmnopqrstuvwxyz{|}@#➡📈\")\n",
    "        else:\n",
    "            self.vocab = (\" $%'()+,-./0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "                    \"\\\\^_abcdefghijklmnopqrstuvwxyz{|}\")\n",
    "\n",
    "        self.seq = tf.placeholder(tf.int32, [None, None])\n",
    "        self.temp = tf.constant(.5)\n",
    "        self.hidden_sizes = [128, 256]\n",
    "        self.batch_size = 128\n",
    "        self.lr = 0.0003\n",
    "        self.skip_step = 50 # number of training per infer\n",
    "        self.num_steps = 50 # number of chars for RNN unrolled\n",
    "        self.len_generated = 200\n",
    "        self.gstep = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "    def create_rnn(self, seq):\n",
    "        layers = [tf.nn.rnn_cell.GRUCell(size) for size in self.hidden_sizes]\n",
    "        cells = tf.nn.rnn_cell.MultiRNNCell(layers)\n",
    "        batch = tf.shape(seq)[0]\n",
    "        zero_states = cells.zero_state(batch, dtype=tf.float32)\n",
    "        self.in_state = tuple([tf.placeholder_with_default(state, [None, state.shape[1]]) \n",
    "                                for state in zero_states])\n",
    "\n",
    "        length = tf.reduce_sum(tf.reduce_max(seq, 2), 1)\n",
    "        self.output, self.out_state = tf.nn.dynamic_rnn(cells, seq, length, self.in_state)\n",
    "\n",
    "    def create_model(self):\n",
    "        seq = tf.one_hot(self.seq, len(self.vocab))\n",
    "        self.create_rnn(seq)\n",
    "        self.logits = tf.layers.dense(self.output, len(self.vocab), None)\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits[:, :-1], \n",
    "                                                        labels=seq[:, 1:])\n",
    "        self.loss = tf.reduce_sum(loss)\n",
    "        self.sample = tf.multinomial(self.logits[:, -1] / self.temp, 1)[:, 0] \n",
    "        self.opt = tf.train.AdamOptimizer(self.lr).minimize(self.loss, global_step=self.gstep)\n",
    "\n",
    "    def train(self):\n",
    "        saver = tf.train.Saver()\n",
    "        start = time.time()\n",
    "        min_loss = None\n",
    "        with tf.Session() as sess:\n",
    "            #To log the graph of the neural network to tensorboard\n",
    "            writer = tf.summary.FileWriter('graphs/gist', sess.graph)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            #Restore model\n",
    "            ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/' + self.model + '/checkpoint'))\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            \n",
    "            iteration = self.gstep.eval()\n",
    "            stream = read_data(self.path, self.vocab, self.num_steps, overlap=self.num_steps//2)\n",
    "            data = read_batch(stream, self.batch_size)\n",
    "            while True:\n",
    "                batch = next(data)\n",
    "                batch_loss, _ = sess.run([self.loss, self.opt], {self.seq: batch})\n",
    "                if (iteration + 1) % self.skip_step == 0:\n",
    "                    print('Iter {}. \\n    Loss {}. Time {}'.format(iteration, batch_loss, time.time() - start))\n",
    "                    self.online_infer(sess)\n",
    "                    start = time.time()\n",
    "                    checkpoint_name = 'checkpoints/' + self.model + '/char-rnn'\n",
    "                    if min_loss is None:\n",
    "                        saver.save(sess, checkpoint_name, iteration)\n",
    "                    elif batch_loss < min_loss:\n",
    "                        saver.save(sess, checkpoint_name, iteration)\n",
    "                        min_loss = batch_loss\n",
    "                iteration += 1\n",
    "\n",
    "    def online_infer(self, sess):\n",
    "        \"\"\" Generate sequence one character at a time, based on the previous character\n",
    "        \"\"\"\n",
    "        for seed in ['Hillary', 'I', 'R', 'T', '@', 'N', 'M', '.', 'G', 'A', 'W']:\n",
    "            sentence = seed\n",
    "            state = None\n",
    "            for _ in range(self.len_generated):\n",
    "                batch = [vocab_encode(sentence[-1], self.vocab)]\n",
    "                feed = {self.seq: batch}\n",
    "                if state is not None: # for the first decoder step, the state is None\n",
    "                    for i in range(len(state)):\n",
    "                        feed.update({self.in_state[i]: state[i]})\n",
    "                index, state = sess.run([self.sample, self.out_state], feed)\n",
    "                sentence += vocab_decode(index, self.vocab)\n",
    "            print('\\t' + sentence)\n",
    "\n",
    "def main():\n",
    "    model = 'trump_tweets'\n",
    "    safe_mkdir('checkpoints')\n",
    "    safe_mkdir('checkpoints/' + model)\n",
    "\n",
    "    lm = CharRNN(model)\n",
    "    lm.create_model()\n",
    "    lm.train()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.Print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def tf_print(op, tensors, message=''): \n",
    "    #redirective output from stderr to stdout for demonstrative purpose\n",
    "    #ipython notebook doesnt print stderr\n",
    "    #and tf.Print outputs to stderr\n",
    "    def print_message(x):\n",
    "        sys.stdout.write(message + \" %s\\n\" % x)\n",
    "        return x\n",
    "\n",
    "    prints = [tf.py_func(print_message, [tensor], tensor.dtype) for tensor in tensors]\n",
    "    with tf.control_dependencies(prints):\n",
    "        op = tf.identity(op)\n",
    "    return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y:  9.0\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(shape=[], dtype=tf.float32, name='x')\n",
    "y = tf.square(x)\n",
    "y = tf_print(y, [y], message='y: ')\n",
    "z = tf.sqrt(y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(z, feed_dict={x:-3}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for missing connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "dense/kernel:0 not changed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-aaf2a98ee2d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mfeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mtest_all_trainables_changed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-aaf2a98ee2d2>\u001b[0m in \u001b[0;36mtest_all_trainables_changed\u001b[0;34m(sess, train_op, feed)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbefore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mafter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Make sure something changed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'{} not changed'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainable_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: dense/kernel:0 not changed"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def test_all_trainables_changed(sess, train_op, feed):\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #Get a list of values of all trainable variables\n",
    "    before = sess.run(tf.trainable_variables())\n",
    "    #Gradient descent the variables once\n",
    "    _ = sess.run(train_op, feed)\n",
    "    #Get the values after being trained once\n",
    "    after = sess.run(tf.trainable_variables())\n",
    "    #Get a list of names of all trainable variable\n",
    "    trainable_names = [v.name for v in tf.trainable_variables()]\n",
    "    #Check for each trainable variable\n",
    "    for i, (b, a) in enumerate(zip(before, after)):\n",
    "        # Make sure something changed.\n",
    "        assert (b != a).any(), '{} not changed'.format(trainable_names[i])\n",
    "\n",
    "learning_rate = 0.01\n",
    "hidden_units = 8\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 5])\n",
    "y = tf.placeholder(tf.float32, [None, 2])\n",
    "\n",
    "# Stacked dense layers example\n",
    "output = tf.layers.dense(x, hidden_units)\n",
    "output = tf.nn.relu(output)\n",
    "output = tf.layers.dense(x, hidden_units)\n",
    "output = tf.nn.relu(output)\n",
    "output = tf.layers.dense(x, hidden_units)\n",
    "output = tf.nn.relu(output)\n",
    "output = tf.layers.dense(x, 2)\n",
    "output = tf.nn.relu(output)\n",
    "\n",
    "loss = tf.reduce_mean((y - output)**2)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    feed = {x:[[1,2,3,4,5]], y:[[2,4]]}\n",
    "    test_all_trainables_changed(sess, train_op, feed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert NaN into inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00917893 0.01522764 0.01695455 0.02158627 0.01023958]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00868588 0.01436077 0.01758172 0.02264987 0.01018074]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00827915 0.01373275 0.01783936 0.02329575 0.01018119]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00792316 0.01324375 0.01781229 0.02370385 0.01019443]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00760175 0.01282398 0.01759456 0.02403679 0.01016209]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00731686 0.01242609 0.01726138 0.02440493 0.01003816]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00707333 0.01203015 0.01684997 0.02483529 0.00981578]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00686902 0.01162808 0.01637918 0.02531428 0.00951847]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00669658 0.01121841 0.01586411 0.02582889 0.00918439]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00654539 0.01080253 0.01531599 0.02638562 0.00885657]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00640468 0.0103829  0.01474056 0.02700109 0.00856862]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00626619 0.00996249 0.01414178 0.02768222 0.00833427]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00612402 0.00954448 0.01352468 0.02842137 0.00815317]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00597344 0.00913153 0.01289376 0.02920377 0.00802027]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00581071 0.00872567 0.01225215 0.03001352 0.00792937]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00563354 0.0083291  0.01160417 0.03083331 0.00787311]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00544124 0.00794495 0.01095948 0.03165016 0.00784225]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00523542 0.00757604 0.0103345  0.03247605 0.00782538]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00502081 0.00722336 0.00974547 0.03334041 0.00780919]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00480368 0.00688572 0.00920186 0.03426289 0.00777936]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00458965 0.00656065 0.00870778 0.03524935 0.00772261]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.0043826  0.0062456  0.00826452 0.03629736 0.00762923]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00418433 0.00593909 0.00787129 0.03739711 0.00749349]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00399472 0.00564134 0.00752569 0.03853112 0.00731175]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00381251 0.005354   0.00722502 0.03967666 0.00708259]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00363588 0.00507925 0.00696732 0.04081093 0.0068088 ]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00346276 0.00481914 0.00675148 0.04191391 0.00649785]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00329115 0.00457529 0.00657666 0.04296285 0.00616104]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00311948 0.0043486  0.00644168 0.04392691 0.00581208]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00294684 0.00413925 0.00634402 0.04478081 0.00546433]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00277282 0.00394676 0.00627909 0.04553801 0.00512755]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00259755 0.00376999 0.00624146 0.04626474 0.00480615]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00242169 0.00360693 0.00622579 0.04704296 0.00449983]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.0022471  0.00345515 0.00622651 0.04792224 0.00420573]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00207783 0.00331276 0.00623789 0.04890694 0.00392096]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00191955 0.00317917 0.00625468 0.04996966 0.00364442]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00177627 0.00305424 0.00627193 0.05106514 0.0033776 ]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00164856 0.00293679 0.0062845  0.05214033 0.00312429]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00153503 0.00282454 0.00628739 0.05314685 0.00288912]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00143369 0.0027154  0.00627575 0.05405231 0.00267526]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00134247 0.00260872 0.00624495 0.05484666 0.00248342]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.0012595  0.00250566 0.00619135 0.05553981 0.00231255]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00118339 0.0024083  0.00611303 0.05615033 0.00216101]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00111332 0.00231825 0.00601041 0.05669139 0.00202725]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00104889 0.00223591 0.00588698 0.05716294 0.00190982]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00098991 0.0021609  0.00574954 0.05754888 0.0018071 ]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00093615 0.00209265 0.00560731 0.05781265 0.00171723]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00088725 0.00203072 0.00546998 0.05789491 0.00163809]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00084281 0.00197472 0.00534539 0.0577245  0.00156742]\n",
      "\n",
      " x [ 3.12152608  7.8237832  -8.16996512 -8.52754604 -2.42803821]\n",
      "y 3\n",
      "output [0.00080237 0.00192427 0.00523828 0.05724721 0.00150295]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3Xd8VfX9x/HXJ4OEEcJImCEEWTIdhD1FRFSKCzdq1aq11j2q/bW2tXVUraLVWnHiqhOF4gIF2SJhb9kbEoZMgSR8fn/kmlIKIUhuTnLv+/l45ME993xz+By95J3v+Z7v95i7IyIiAhATdAEiIlJ2KBRERKSQQkFERAopFEREpJBCQURECikURESkkEJBREQKKRREjsDMVppZn6DrEClNCgURESmkUBA5RmZ2vZktNbOtZjbCzOqF3jcze8rMss1su5nNMbPWoX1nm9kCM9tpZuvM7O5gz0Lk8BQKIsfAzHoDjwAXA3WBVcA7od19gR5AM6AacAmwJbTvZeBGd08CWgNjSrFskWKLC7oAkXLmCuAVd58BYGb3A9vMLAPIBZKAE4Fv3X3hQd+XC7Q0s9nuvg3YVqpVixSTegoix6YeBb0DANx9FwW9gfruPgZ4FngO2GRmQ8ysaqjphcDZwCozG2dmnUu5bpFiUSiIHJv1QMMfN8ysMlATWAfg7s+4ezugFQWXke4JvT/N3c8FagEfA++Vct0ixaJQEClavJkl/vhFwQ/za8zsZDNLAB4Gprr7SjNrb2YdzSwe2A3sBfLNrIKZXWFmye6eC+wA8gM7I5EiKBREivYp8MNBX92B3wMfAhuAxsClobZVgRcpGC9YRcFlpSdC+64EVprZDuCXwKBSql/kmJgesiMiIj9ST0FERAopFEREpJBCQURECikURESkULmb0ZySkuIZGRlBlyEiUq5Mnz59s7unHq1duQuFjIwMsrKygi5DRKRcMbNVR2+ly0ciInIQhYKIiBRSKIiISCGFgoiIFFIoiIhIIYWCiIgUUiiIiEihqAmFhRt28NfPF6FVYUVEjixqQuGb5Vt4/utljFmUHXQpIiJlVtSEwqBODTkhtTIPfbKQ3PwDQZcjIlImRU0oxMfG8H9nt2D55t28MaVYs71FRKJO1IQCQO8Ta9GtSQpPf7WE7/fsD7ocEZEyJ6pCwcz4Xf8W7Nyby+AvlwRdjohImRNVoQBwYp2qXNohnTe/WcWynF1BlyMiUqZEXSgA3HlGMyrGx/LwJwuDLkVEpEyJylBIqZLAzb2b8NWibCYsyQm6HBGRMiMqQwHgmq4ZNKhRkb+MXEieblEVEQGiOBQS4mK5/6wWLN60k3ez1gRdjohImRC1oQBwVus6dMiowZOjvmPH3tygyxERCVxUh4KZ8fv+Ldm6Zz+DR+sWVRGRqA4FgDZpyVzWIZ2hU1ayaOOOoMsREQlU2EPBzGLNbKaZjTzMvnQzGxvaP8fMzg53PYdzT9/mVE2M44GP52sVVRGJaqXRU7gNONKEgN8B77n7KcClwD9KoZ7/Ub1yBX7T70S+XbmVj2etC6IEEZEyIayhYGZpwDnAS0do4kDV0OtkYH046ynKxZkNOKlBNR76ZJEGnUUkaoW7pzAYuBc40kSAPwKDzGwt8Clwy+EamdkNZpZlZlk5OeGZbBYTY/zl3NZs2b2Pp0Z/F5a/Q0SkrAtbKJhZfyDb3acX0ewy4DV3TwPOBt4ws/+pyd2HuHumu2empqaGqeKCQecrOqYzdPJKFqzXoLOIRJ9w9hS6AgPMbCXwDtDbzN48pM11wHsA7j4FSARSwljTUd3dtznVKlXggeHzNOgsIlEnbKHg7ve7e5q7Z1AwiDzG3Qcd0mw1cDqAmbWgIBQCXYyoWqUK3NfvRLJWbWPYDA06i0h0KfV5Cmb2oJkNCG3eBVxvZrOBfwE/9zLw6/nAdmmckl6NRz5byPYfNOgsItGjVELB3b929/6h1w+4+4jQ6wXu3tXdT3L3k919VGnUczQxMcafz23N1t37eezzRUGXIyJSaqJ+RvORtK6fzLVdG/HW1NVMXb4l6HJEREqFQqEId/ZtRnqNStw3bC57c/ODLkdEJOwUCkWoVCGORy9ow4rNu/VMZxGJCgqFo+jSJIVLMhvw4oTlzF27PehyRETCSqFQDL89pwU1K1fg3g/nkKuntIlIBFMoFENyxXj+fF5rFm7YwZDxy4MuR0QkbBQKxXRmqzqc06YuT3+1hKXZu4IuR0QkLBQKx+CPA1pRMT6W+z6cw4EDgc+xExEpcQqFY5CalMAD/VuStWobr09ZGXQ5IiIlTqFwjC44tT69mqfy6OeLdBlJRCKOQuEYmRmPXdiWShXiuP3dmezP091IIhI5FAo/Qa2qiTx6QRvmrdvBk3ogj4hEEIXCT9S3VR0u65DOC+OXMWWZ1kYSkcigUDgOv+/fgkY1K3Pne7PYvkdLbItI+adQOA6VKsQx+NKTydm5j99+PFdPahORck+hcJzaplXjjjOa8cmcDXpSm4iUewqFEvDLno3pkFGDP4yYz5qte4IuR0TkJ1MolIDYGOPJS07CDG59R7epikj5pVAoIWnVK/HIBW2Yufp7HvpkQdDliIj8JAqFEtS/bT2u69aIoVNW8dHMtUGXIyJyzBQKJey+s06kY6Ma3D9sLgvW7wi6HBGRYxL2UDCzWDObaWYjj7D/YjNbYGbzzeztcNcTbvGxMTx7+alUq1iBX745XfMXRKRcKY2ewm3AwsPtMLOmwP1AV3dvBdxeCvWEXWpSAv8YdCobtv/A7e/O1DLbIlJuhDUUzCwNOAd46QhNrgeec/dtAO6eHc56StOp6dV54GetGLs4h6e/WhJ0OSIixRLunsJg4F7gSPdoNgOamdkkM/vGzPodrpGZ3WBmWWaWlZOTE65aS9ygjulceGoaT3+1hDGLNgVdjojIUYUtFMysP5Dt7tOLaBYHNAV6AZcBL5lZtUMbufsQd89098zU1NSw1BsOZsZD57emVb2q3PbOLJZm7wy6JBGRIoWzp9AVGGBmK4F3gN5m9uYhbdYCw909191XAIspCImIkRgfywtXtiMxPparX5lG9o69QZckInJEYQsFd7/f3dPcPQO4FBjj7oMOafYxcBqAmaVQcDlpebhqCkpa9Uq8+vP2bNuzn2tem8aufXlBlyQiclilPk/BzB40swGhzS+ALWa2ABgL3OPuEflwgtb1k3nuilNZtHEnv3prBrn5WgpDRMoeK2/LPWdmZnpWVlbQZfxk705bzW8+nMtF7dJ4bGBbzCzokkQkCpjZdHfPPFq7uNIoRv7jkvbprP9+L09/tYT61Stye59mQZckIlJIoRCA2/s0Zf33PzD4yyXUS67Ixe0bBF2SiAigUAiEmfHwBW3YuGMv9380l2qV4unbqk7QZYmIaEG8oMTHxvD8oHa0rp/MzW/P4MsFmtwmIsFTKASoSkIcr1/bgRZ1q/Krt2Zo1rOIBE6hELDkivG8cW1HmtWpwi/fmMHYxRGz/JOIlEMKhTIguVI8b17Xkaa1q3DjG9MZ9135Wd9JRCKLQqGMqFapAm9e15HGqVW4/vUsJixRMIhI6VMolCHVK1fgrV905ISUyvxiaBbj1WMQkVKmUChjaoSCoVFKZa4bOo0Rs9cHXZKIRBGFQhlUs0oC797YmVMaVOfWf83k1Ukrgi5JRKKEQqGMSq4Yz+vXdaBvy9r86d8LePyLRZS3dapEpPxRKJRhifGx/OOKU7msQzrPjV3Gbz6cQ55WVxWRMNIyF2VcXGwMD5/fmtQqFXhmzFK27t7P3y87lYoVYoMuTUQikHoK5YCZcWff5jx4biu+WpTN5S99Q/ZOPcFNREqeQqEcuapzBs9fcSqLNuzk3GcnMW/d9qBLEpEIo1AoZ/q1rssHN3XGgIH/nMzIObplVURKjkKhHGpVL5nhv+5Gq3rJ/PrtmTw5ajEHDujOJBE5fgqFcio1KYG3r+/IxZlpPDNmKTe9NZ3d+/KCLktEyjmFQjmWEBfLXy9sy+/7t2T0gk1c+PxklufsCrosESnHwh4KZhZrZjPNbGQRbQaamZvZUR8qLf/NzLiuWyNeu6YDm3bsZcCzk/hkzoagyxKRcqo0egq3AQuPtNPMkoBbgamlUEvE6tEslU9u7U6z2lW4+e0Z/GH4PPbl5QddloiUM2ENBTNLA84BXiqi2Z+BxwDdeH+c6lWryLs3dub67o0YOmUVF/9zCmu27gm6LBEpR8LdUxgM3Ascdm0GMzsFaODuR7y0FGp3g5llmVlWTo6Wky5KfGwM/3dOS164sh3LN+/mnGcmMFrPfxaRYgpbKJhZfyDb3acfYX8M8BRw19GO5e5D3D3T3TNTU1NLuNLIdGarOnxyS3fSa1bi+tez+OOI+ezN1eUkESlaOHsKXYEBZrYSeAfobWZvHrQ/CWgNfB1q0wkYocHmkpNesxIf3tSlYCB68krOfXYSizbuCLosESnDwhYK7n6/u6e5ewZwKTDG3QcdtH+7u6e4e0aozTfAAHfPCldN0SghLpbf92/J0Gs7sHXPfgY8O4lXJq7QZDcROaxSn6dgZg+a2YDS/nujXc9mqXx+W3d6NE3hwZEL+Plr08jeobF9EflvVt4e3JKZmelZWepM/FTuzptTV/OXkQuonBDHIxe04cxWdYIuS0TCzMymu/tRL89rRnOUMTOu7NSQT27tRt3kRG58Yzp3vjeL7T/kBl2aiJQBCoUo1aRWEh/9qiu39m7C8Fnr6Td4PBOXbA66LBEJmEIhilWIi+HOvs358KYuVKoQy6CXp/LA8Hns2a+F9USilUJBOLlBNT65tTvXdm3E61NWcfbTE5i+amvQZYlIABQKAkBifCwP/Kwl/7q+E7n5zsB/TuGhTxZowptIlFEoyH/p3LgmX9zRg8s6pPPihBWc/cwEpq/aFnRZIlJKFAryP6okxPHw+W1487qO7Ms9wEX/nMzDny5Ur0EkCigU5Ii6NU3h89u7c0n7dIaMX845z0xgxmr1GkQimUJBipSUGM8jF7Thjes68MP+fAY+r16DSCRTKEixdG+ayhd39CjsNegOJZHIVKxQMLPGZpYQet3LzG41s2rhLU3Kmh97DW9e15F9eQcY+M8p/GXkAn7Yr16DSKQobk/hQyDfzJoALwONgLfDVpWUad2apvDFHT24vEM6L01cwVlPj2faSvUaRCJBcUPhgLvnAecDg939DqBu+MqSsq5KQhwPnd+Gt3/RkbwDzsUvTOGPI+ZrNrRIOVfcUMg1s8uAq4EfH50ZH56SpDzp0iSFL27vwZWdGvLa5JWcOXg8k5dpDSWR8qq4oXAN0Bl4yN1XmFkj4M2jfI9EicoJcTx4bmvevaETMWZc/uJU/u+juezap16DSHlzzM9TMLPqQAN3nxOekoqm5ymUbT/sz+dvoxbz8qQV1EuuyCMXtKFHMz1XWyRoJfo8BTP72syqmlkNYDbwqpk9ebxFSuSpWCGW3/VvyQe/7EJifAxXvfIt97w/m+179LwGkfKguJePkt19B3AB8Kq7twP6hK8sKe/aNazOJ7d251e9GjNs5jr6PDWOz+ZuCLosETmK4oZCnJnVBS7mPwPNIkVKjI/l3n4nMvzmrtRKSuCmt2Zw4xtZeja0SBlW3FB4EPgCWObu08zsBGBJ+MqSSNK6fjLDb+7Kb/qdyNeLczj9yXG8O2015e354CLR4JgHmo/5LzCLBbKAde7e/5B9dwK/APKAHOBad19V1PE00Fy+Lc/ZxX3D5vLtiq10OqEGD53fhsapVYIuSyTilfRAc5qZfWRm2Wa2ycw+NLO0YtZyG7DwCPtmApnu3hb4AHismMeUcuqE1Cq8c30nHj6/DfPX7+CswRN4+ssl7MvTUhkiZUFxLx+9CowA6gH1gX+H3itSKDjOAV463H53H+vue0Kb3wDFDRopx2JijMs7pvPVXT05s3UdnvryO85+egJTl28JujSRqFfcUEh191fdPS/09RpQnJvPBwP3AgeK0fY64LNi1iMRoFZSIn+/7BRevaY9+/IOcMmQb7j3g9ls270/6NJEolZxQ2GzmQ0ys9jQ1yCgyF/rzKw/kO3u04928NDxMoHHj7D/BjPLMrOsnJycYpYs5cVpzWsx+o6e/LJnYz6csY7TnxzHe9PWcOCABqJFSluxBprNLB14loKlLhyYDNzq7quL+J5HgCspGEROBKoCw9x90CHt+gB/B3q6e/bRatFAc2RbtHEHv/toHlmrttGuYXX+fG5rWtarGnRZIuVecQeaf/LdR2Z2u7sPLmbbXsDdh7n76BQKBpj7uXuxbnFVKES+AwecD2es5ZHPFrH9h1yu7pzBHWc0JSlRazCK/FQlevfREdz5U77JzB40swGhzceBKsD7ZjbLzEYcRz0SIWJijIsyGzDmrp5c2r4Br05ewel/G8eI2es1t0EkzI6np7DG3RuUcD1HpZ5C9Jm95nt+9/E85q7bTqcTavCnAa1pXicp6LJEypXS6CnoVzYpFSc1qMbHN3flL+e1ZuGGnZz9zAT+PHIBO/ZqkT2RkhZX1E4z28nhf/gbUDEsFYkcRmyMMahTQ85uU5cnRi3mlUkrGD5rPfefdSLnn1KfmBgLukSRiFBkT8Hdk9y96mG+kty9yEARCYcalSvw8PltGH5zV9KqV+Su92dz0QtTmLdue9CliUSE47l8JBKYtmnVGHZTFx67sC0rN+/mZ89O5P5hc9iya1/QpYmUawoFKbdiYoyL2zdgzN29uLZrI97PWkuvJ77m5YkryM0vziR6ETmUQkHKveSK8fy+f0s+v707p6RX588jF3DW0xOYsESz30WOlUJBIkaTWkkMvaY9L12VSW7+Aa58+Vt+MTSLFZt3B12aSLmhUJCIYmb0aVmbUXf04Df9TmTKss30fWocD3+6ULewihSDQkEiUkJcLDf1aszYe3px/in1eXHCcno/8TX/+nY1+VpoT+SIFAoS0WolJfLYwJMYcXM3GqVU5v5hc/nZ3ycyZZme3SByOAoFiQpt0pJ578bOPHv5KWz/IZfLXvyGG17PYqXGG0T+i0JBooaZ0b9tPb66qyf3nNmcSUs3c8ZT4/jLyAVs/0HjDSKgUJAolBgfy82nNWHs3QXjDS9PWkGvx8fy+pSV5Gl+g0Q5hYJErVpVC8Yb/v3rbjSvk8QDw+fT7+kJjF2UrSW6JWopFCTqta6fzL+u78QLV7YjL/8A17w2jate+ZZFG3cEXZpIqVMoiFAw3nBmqzqMuqMnD/RvyZy12zn76QncP2wO2Tv3Bl2eSKlRKIgcpEJcDNd2a8S4e3rx8y4F6ymd9vjXPDtmCXtz84MuTyTsFAoih1GtUgUe+FlLRt/Zk65NUnhi1Hec9sTXfDRzLQc0+U0imEJBpAiNUioz5KpM3rmhEylVErjj3dmc949JTF2uyW8SmRQKIsXQ6YSaDL+5K09efBI5O/dxyZBvuPENLbYnkUehIFJMMTHGBaemMeauXtx1RjMmLNnMGU+O48F/L2D7Hk1+k8gQ9lAws1gzm2lmIw+zL8HM3jWzpWY21cwywl2PyPGqWCGWW05vytd392JguzRenbyCnk+M5dVJeriPlH+l0VO4DVh4hH3XAdvcvQnwFPDXUqhHpETUqprIoxe25ZNbutOqXlX+9O8FnPnUeL5csEmT36TcCmsomFkacA7w0hGanAsMDb3+ADjdzCycNYmUtJb1qvLmdR15+epMMPjF61kMenkqCzdo8puUP+HuKQwG7gWO1KeuD6wBcPc8YDtQ89BGZnaDmWWZWVZOjh6xKGWPmXF6i9p8cXsP/jSgFfPX7+CcZyZw/7C5bN61L+jyRIotbKFgZv2BbHefXlSzw7z3P/1udx/i7pnunpmamlpiNYqUtPjYGK7uksG4u0/j6i4ZvJ+1htMe/5oh45exL0+T36TsC2dPoSswwMxWAu8Avc3szUParAUaAJhZHJAMbA1jTSKlIrlSPH/4WSs+v70H7RvV4OFPF9H3qfF8MX+jxhukTAtbKLj7/e6e5u4ZwKXAGHcfdEizEcDVodcDQ230L0YiRpNaVXjl5+0Zem0H4mNjuPGN6Vzx0lQttidlVqnPUzCzB81sQGjzZaCmmS0F7gTuK+16REpDz2apfH5b98LxhrOfnsADw+exbff+oEsT+S9W3n4xz8zM9KysrKDLEPnJtu3ez+Avv+PNqaupkhDHnWc044qO6cTFai6phI+ZTXf3zKO106dQpJRVr1yBP53bmk9v7U7r+lX5w4j5nP3MBCYu2Rx0aSIKBZGgNK+TxJvXdWTIle3Ym3uAQS9P5YbXs1i9ZU/QpUkUUyiIBMjM6NuqDqPu6ME9ZzZn4tLN9HlqHI9/sYjd+/KCLk+ikEJBpAxIjI/l5tOaMOauXpzTpi7PjV1G7799zccz1+kWVilVCgWRMqROciJPXXIyH97UmVpJidz+7iwG/nMKs9d8H3RpEiUUCiJlULuGNRh+c1ceu7Atq7bs5tznJnHne7PYtEPPi5bwUiiIlFExMcbF7Rsw9u5e3NjzBEbO3sBpT+h50RJeCgWRMi4pMZ77z2rB6Dt70L1pwfOiT//bOEbOWa/xBilxCgWRcqJhzcq8cGUmb1/fkaTEOH799kwufH4y01dtC7o0iSAKBZFypkvjFD65tTuPXtCGNdt+4MLnJ/Ort6azUs+LlhKgZS5EyrHd+/J4ccJyhoxfTm7+AQZ1asitvZtSvXKFoEuTMqa4y1woFEQiQPaOvTz15Xe8O20NlRPiuKlXY67p0oiKFWKDLk3KCIWCSBT6btNOHv1sEWMWZVO7agK3nd6MizPTtNieaEE8kWjUrHYSr/y8Pe/d2Jn61Sry24/m0vep8Xw2d4PuVJJiUSiIRKAOjWrw4U1dePGqTGJjjJvemsF5z01i8lKtxCpFUyiIRCgz44yWtfn89h48PrAtOTv3cflLU7nipW+YuVq3scrhaUxBJErszc3n7amreW7sUrbs3k/flrW5q29zmtdJCro0KQUaaBaRw9q1L49XJ65gyPjl7Nqfx3kn1+f2Pk1pWLNy0KVJGCkURKRI3+/Zz/PjljF08kry8p2LMhtwS+8m1KtWMejSJAwUCiJSLJt27OUfY5fy9rerMYzLO6bzq16NqVU1MejSpAQFHgpmlgiMBxKAOOADd//DIW3SgaFANSAWuM/dPy3quAoFkfBYu20Pz45ZyvvT1xIfa1zdOYMbezamhmZHR4SyEAoGVHb3XWYWD0wEbnP3bw5qMwSY6e7Pm1lL4FN3zyjquAoFkfBauXk3z3y1hI9mraNifCxXdc7g+u6NqFklIejS5DgEPnnNC+wKbcaHvg5NIAeqhl4nA+vDVY+IFE9GSmWevORkRt/Rg9Nb1OaF8cvo/thYHvl0IZt37Qu6PAmzsI4pmFksMB1oAjzn7r85ZH9dYBRQHagM9HH36Yc5zg3ADQDp6entVq1aFbaaReS/Lc3exbNjljBi9noqxMUwqGNDbuh5ArWSNOZQngR++eiQYqoBHwG3uPu8g96/M1TD38ysM/Ay0NrdDxzpWLp8JBKM5Tm7eHbsUobPWk9cjHFp+wb8ovsJNKhRKejSpBjKVCgAmNkfgN3u/sRB780H+rn7mtD2cqCTu2cf6TgKBZFgrdy8m+e/XsawmWs54DDgpHr8smdjTYIr4wIfUzCz1FAPATOrCPQBFh3SbDVweqhNCyARyAlXTSJy/DJSKvPXgW2ZcG9vrumSwRfzN3Lm4PFc99o0pq/aGnR5cpzCefdRWwpuN42lIHzec/cHzexBIMvdR4TuOHoRqELBoPO97j6qqOOqpyBStny/Zz+vT1nFq5NWsG1PLu0aVucX3RrRt1UdYmMs6PIkpMxdPiopCgWRsmnP/jzenbaGVyatYM3WH0ivUYlru2ZwUWYDKifEBV1e1FMoiEgg8g84o+Zv5MUJy5mx+nuqJsZxRaeGXN05gzrJumMpKAoFEQnc9FXbeHnicj6ft5EYM85qU5efd8ng1PRqFMxvldJS3FBQn05EwqZdw+q0a9iONVv3MHTySt7NWsO/Z6+nbVoyP++SwTlt65IQp+dIlyXqKYhIqdm9L49hM9fx2qQVLMvZTUqVClzeIZ3LOqZTN1mrs4aTLh+JSJnl7kxcuplXJ61k7OJsYszo06IWV3bKoEvjmsTorqUSp8tHIlJmmRndm6bSvWkqq7fs4a1vV/F+1lq+mL+JE1Iqc3nHdC5q14DkSvFBlxp11FMQkTJhb24+n83bwJvfrGb6qm0kxMVwTpu6XNohnfYZ1TUwfZx0+UhEyq0F63fw1tRVDJ+1nl378micWplL26dzwan1tYT3T6RQEJFyb8/+PEbO2cA7365mxurviY81+raqwyWZDejaJEUzpo+BQkFEIsp3m3byzrdrGDZzLd/vyaVuciID26UxsF0aDWtWDrq8Mk+hICIRaV9ePl8uyOb96WsY/10OBxw6NqrBRZkNOLtNHSpV0P0zh6NQEJGIt3H7Xj6csZb3s9awcsseKlWIpV/rOgw8NY1OJ+jW1oMpFEQkarg7Wau2MWzGWkbO3sDOfXnUS07kvFPqc8GpaTSpVSXoEgOnUBCRqLQ3N5/RCzYxbMZaxi/ZTP4B56S0ZM49uT4/O6keqUnRefeSQkFEol72zr2MmLWej2auY/76HcTGGF2bpHDeyfXo26oOVaJoSW+FgojIQZZm7+Tjmev5eNY61m77gcT4GM5oWYefta1Lz+apEb8wn0JBROQw3J3pq7bx8ax1fDp3I1t37ycpIY6+rerQ/6S6dGuSQnxs2J5UHBiFgojIUeTmH2Dysi2MnL2ez+dvZOfePKpXiqdf6zqc1bounRvXjJiAUCiIiByDfXn5TPhuM/+es57RCzaxZ38+yRXjOaNlbc5uU4euTVLK9SUmrZIqInIMEuJi6dOyNn1a1mZvbj7jv8vh83kb+WL+Rj6YvpakhDhOb1GLvq3q0KNZasQOUkfmWYmIHIfE+Fj6tqpD31Z12J93gEnLNvPZ3A2MWrCJj2etp0JsDF2a1OSMlrU5o0VtalWNnGdPh+3ykZklAuOBBArC5wN3/8Nh2l0M/BFwYLa7X17UcXX5SESCkpftEh2cAAAIpElEQVR/gKxV2xi9YBOjF2xi9dY9AJzUoBp9TqzFaSfWolW9qmVyme/AxxSs4L9KZXffZWbxwETgNnf/5qA2TYH3gN7uvs3Marl7dlHHVSiISFng7ny3aRej5m9k9MJNzFm7HYDaVRM4rXktejWvRbemKWXmMlPgYwpekDa7Qpvxoa9DE+h64Dl33xb6niIDQUSkrDAzmtdJonmdJG45vSnZO/cybnEOYxdn88mcDbwzbQ3xsUb7jBr0aJZKj6aptKibVCZ7EQcL691HZhYLTAeaUPDD/zeH7P8Y+A7oCsQCf3T3zw9znBuAGwDS09PbrVq1Kmw1i4gcr9z8A2St3MbYxdmMW5zD4k07AUhNSqB70xR6NkulW5OUUn1gUOCXjw4pphrwEXCLu8876P2RQC5wMZAGTABau/v3RzqWLh+JSHmzacdexn+Xw/glm5m4JIdte3IBaFWvKt2apNC1SQodGtUgMT58t7wGfvnoYO7+vZl9DfQD5h20ay3wjbvnAivMbDHQFJhWGnWJiJSG2lUTuSizARdlNiD/gDNv3XYmLMlh4tLNvDJpBS+MX06FuBgyG1anaygk2tRPDuTJcuEcaE4FckOBUBEYBfzV3Uce1KYfcJm7X21mKcBM4GR333Kk46qnICKRZM/+PKat3MbEJTlMXLqFhRt2AJCUGEenE2rStXFNujZJoUmtKsc1HlEWegp1gaGhcYUY4D13H2lmDwJZ7j4C+ALoa2YLgHzgnqICQUQk0lSqEEfPZqn0bJYKwOZd+5i8bAuTl25m0rLNjF6wCSgYj/jdOS049+T6Ya1Hy1yIiJRha7buYdLSzUxatoUrOqbT6YSaP+k4ZaGnICIix6lBjUpc2iGdSzukl8rfFxnL/4mISIlQKIiISCGFgoiIFFIoiIhIIYWCiIgUUiiIiEghhYKIiBRSKIiISKFyN6PZzHKAn7p2dgqwuQTLKS+i9bwhes9d5x1dinPeDd099WgHKnehcDzMLKs407wjTbSeN0Tvueu8o0tJnrcuH4mISCGFgoiIFIq2UBgSdAEBidbzhug9d513dCmx846qMQURESlatPUURESkCAoFEREpFDWhYGb9zGyxmS01s/uCridczOwVM8s2s3kHvVfDzEab2ZLQn9WDrDEczKyBmY01s4VmNt/Mbgu9H9HnbmaJZvatmc0OnfefQu83MrOpofN+18wqBF1rOJhZrJnNNLORoe2IP28zW2lmc81slpllhd4rsc95VIRC6DnRzwFnAS2By8ysZbBVhc1rQL9D3rsP+MrdmwJfhbYjTR5wl7u3ADoBN4f+H0f6ue8Derv7ScDJQD8z6wT8FXgqdN7bgOsCrDGcbgMWHrQdLed9mruffNDchBL7nEdFKAAdgKXuvtzd9wPvAOcGXFNYuPt4YOshb58LDA29HgqcV6pFlQJ33+DuM0Kvd1Lwg6I+EX7uXmBXaDM+9OVAb+CD0PsRd94AZpYGnAO8FNo2ouC8j6DEPufREgr1gTUHba8NvRctarv7Bij44QnUCriesDKzDOAUYCpRcO6hSyizgGxgNLAM+N7d80JNIvXzPhi4FzgQ2q5JdJy3A6PMbLqZ3RB6r8Q+53ElUGB5YId5T/fiRiAzqwJ8CNzu7jsKfnmMbO6eD5xsZtWAj4AWh2tWulWFl5n1B7LdfbqZ9frx7cM0jajzDunq7uvNrBYw2swWleTBo6WnsBZocNB2GrA+oFqCsMnM6gKE/swOuJ6wMLN4CgLhLXcfFno7Ks4dwN2/B76mYEylmpn9+EtfJH7euwIDzGwlBZeDe1PQc4j088bd14f+zKbgl4AOlODnPFpCYRrQNHRnQgXgUmBEwDWVphHA1aHXVwPDA6wlLELXk18GFrr7kwftiuhzN7PUUA8BM6sI9KFgPGUsMDDULOLO293vd/c0d8+g4N/zGHe/ggg/bzOrbGZJP74G+gLzKMHPedTMaDazsyn4TSIWeMXdHwq4pLAws38BvShYSncT8AfgY+A9IB1YDVzk7ocORpdrZtYNmADM5T/XmH9LwbhCxJ67mbWlYGAxloJf8t5z9wfN7AQKfoOuAcwEBrn7vuAqDZ/Q5aO73b1/pJ936Pw+Cm3GAW+7+0NmVpMS+pxHTSiIiMjRRcvlIxERKQaFgoiIFFIoiIhIIYWCiIgUUiiIiEghhYJEHTPbFfozw8wuL+Fj//aQ7ckleXyRcFMoSDTLAI4pFEIr7hblv0LB3bscY00igVIoSDR7FOgeWpf+jtDCco+b2TQzm2NmN0LB5KjQsxrepmByHGb2cWhBsvk/LkpmZo8CFUPHeyv03o+9Egsde15oLfxLDjr212b2gZktMrO3QrOzMbNHzWxBqJYnSv2/jkSlaFkQT+Rw7iM0ExYg9MN9u7u3N7MEYJKZjQq17QC0dvcVoe1r3X1raGmJaWb2obvfZ2a/dveTD/N3XUDB8w5OomC2+TQzGx/adwrQioJ1eiYBXc1sAXA+cKK7+49LWYiEm3oKIv/RF7gqtAz1VAqWYm4a2vftQYEAcKuZzQa+oWCxxaYUrRvwL3fPd/dNwDig/UHHXuvuB4BZFFzW2gHsBV4yswuAPcd9diLFoFAQ+Q8Dbgk90epkd2/k7j/2FHYXNipYa6cP0Dn0xLOZQGIxjn0kB6/Nkw/EhZ4J0IGCVV/PAz4/pjMR+YkUChLNdgJJB21/AdwUWoIbM2sWWonyUMnANnffY2YnUrBU9Y9yf/z+Q4wHLgmNW6QCPYBvj1RY6LkQye7+KXA7BZeeRMJOYwoSzeYAeaHLQK8BT1Nw6WZGaLA3h8M/1vBz4JdmNgdYTMElpB8NAeaY2YzQUs4/+gjoDMym4MEv97r7xlCoHE4SMNzMEinoZdzx005R5NholVQRESmky0ciIlJIoSAiIoUUCiIiUkihICIihRQKIiJSSKEgIiKFFAoiIlLo/wHg+9OVp16EDwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "####\n",
    "#Give the index of the minimum element of a list\n",
    "####\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "learning_rate = 0.01\n",
    "hidden_units = 8\n",
    "input_vector_size = 5\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, input_vector_size])\n",
    "y = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "# Stacked dense layers example\n",
    "output = tf.layers.dense(x, hidden_units)\n",
    "output = tf.nn.tanh(output)\n",
    "output = tf.layers.dense(output, hidden_units)\n",
    "output = tf.nn.tanh(output)\n",
    "output = tf.layers.dense(output, input_vector_size)\n",
    "output = tf.nn.softmax(output, axis=0)\n",
    "\n",
    "y_onehot = tf.one_hot(y, input_vector_size)\n",
    "cross_entropy_loss = tf.reduce_mean(-tf.reduce_sum(y_onehot*tf.log(output+1e-10), reduction_indices=[1]))\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cross_entropy_loss)\n",
    "\n",
    "def generate_data(amount):\n",
    "    x = np.random.uniform(low=-10, high=10, size=(amount, input_vector_size))\n",
    "    y = np.argmin(x, axis=-1)\n",
    "    return x, y\n",
    "\n",
    "loss = []\n",
    "\n",
    "x_data, y_data = generate_data(100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for _ in range(50):\n",
    "        feed = {x:x_data, y:y_data}\n",
    "        loss_, _, probabilities = sess.run([cross_entropy_loss, train_op, output], feed)\n",
    "        loss.append(loss_)\n",
    "        feed = {x:[x_data[0]]}\n",
    "        print('\\n x', x_data[0])\n",
    "        print('y', y_data[0])\n",
    "        print('output', probabilities[0])\n",
    "\n",
    "plt.plot(np.arange(len(loss)),loss)\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n",
      "1 [[nan  1.  2.  3.  4.]\n",
      " [ 6.  2.  6.  2.  5.]\n",
      " [ 5.  2.  6.  7.  8.]]\n",
      "2 [[       nan        nan        nan        nan        nan        nan\n",
      "         nan        nan]\n",
      " [ 0.9562753 -6.297241  -2.4002733  2.2407007 -4.961317   8.259984\n",
      "   3.9050207 -1.3673065]\n",
      " [ 2.6694674 -5.953225  -3.2578635  1.4549265 -6.230465   6.5282197\n",
      "   2.7216978 -3.1292498]]\n",
      "3 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [ 0.7426108  -0.9999932  -0.9836837   0.9776182  -0.9999019   0.9999999\n",
      "   0.999189   -0.8780769 ]\n",
      " [ 0.99044394 -0.9999865  -0.99704444  0.8966627  -0.99999225  0.9999957\n",
      "   0.9913877  -0.9961791 ]]\n",
      "4 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-1.4812379   1.2095535   0.84531546 -2.1312492   0.322084   -0.6971032\n",
      "  -0.04872903  1.0381573 ]\n",
      " [-1.5262551   1.1550012   0.90762305 -2.1803946   0.34639585 -0.7603657\n",
      "  -0.01932065  0.87294924]]\n",
      "5 [[        nan         nan         nan         nan         nan         nan\n",
      "          nan         nan]\n",
      " [-0.90169966  0.83654547  0.68861425 -0.97221726  0.31139007 -0.60252583\n",
      "  -0.0486905   0.7771594 ]\n",
      " [-0.9097816   0.81940407  0.7199894  -0.9747853   0.33317533 -0.64129233\n",
      "  -0.01931825  0.7028695 ]]\n",
      "6 [[nan nan nan nan nan]\n",
      " [nan nan nan nan nan]\n",
      " [nan nan nan nan nan]]\n"
     ]
    }
   ],
   "source": [
    "x = output\n",
    "output = tf_print(output, [output], message='1')\n",
    "output = tf.layers.dense(output, hidden_units)\n",
    "output = tf_print(output, [output], message='2')\n",
    "output = tf.nn.tanh(output)\n",
    "output = tf_print(output, [output], message='3')\n",
    "output = tf.layers.dense(output, hidden_units)\n",
    "output = tf_print(output, [output], message='4')\n",
    "output = tf.nn.tanh(output)\n",
    "output = tf_print(output, [output], message='5')\n",
    "output = tf.layers.dense(output, 5)\n",
    "output = tf.nn.softmax(output, axis=0)\n",
    "output = tf_print(output, [output], message='6')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for _ in range(50):\n",
    "        feed = {x:[[float('NaN'),1,2,3,4], [6,2,6,2,5],[5,2,6,7,8]]}\n",
    "        probabilities = sess.run([output], feed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"gradients/dense/MatMul_grad/MatMul_1:0\", shape=(5, 32), dtype=float32) <tf.Variable 'dense/kernel:0' shape=(5, 32) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name dense/kernel:0 is illegal; using dense/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name dense/kernel:0_grad is illegal; using dense/kernel_0_grad instead.\n",
      "Tensor(\"gradients/dense/BiasAdd_grad/BiasAddGrad:0\", shape=(32,), dtype=float32) <tf.Variable 'dense/bias:0' shape=(32,) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name dense/bias:0 is illegal; using dense/bias_0 instead.\n",
      "INFO:tensorflow:Summary name dense/bias:0_grad is illegal; using dense/bias_0_grad instead.\n",
      "Tensor(\"gradients/dense_1/MatMul_grad/MatMul_1:0\", shape=(32, 32), dtype=float32) <tf.Variable 'dense_1/kernel:0' shape=(32, 32) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name dense_1/kernel:0 is illegal; using dense_1/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name dense_1/kernel:0_grad is illegal; using dense_1/kernel_0_grad instead.\n",
      "Tensor(\"gradients/dense_1/BiasAdd_grad/BiasAddGrad:0\", shape=(32,), dtype=float32) <tf.Variable 'dense_1/bias:0' shape=(32,) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name dense_1/bias:0 is illegal; using dense_1/bias_0 instead.\n",
      "INFO:tensorflow:Summary name dense_1/bias:0_grad is illegal; using dense_1/bias_0_grad instead.\n",
      "Tensor(\"gradients/dense_2/MatMul_grad/MatMul_1:0\", shape=(32, 5), dtype=float32) <tf.Variable 'dense_2/kernel:0' shape=(32, 5) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name dense_2/kernel:0 is illegal; using dense_2/kernel_0 instead.\n",
      "INFO:tensorflow:Summary name dense_2/kernel:0_grad is illegal; using dense_2/kernel_0_grad instead.\n",
      "Tensor(\"gradients/dense_2/BiasAdd_grad/BiasAddGrad:0\", shape=(5,), dtype=float32) <tf.Variable 'dense_2/bias:0' shape=(5,) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name dense_2/bias:0 is illegal; using dense_2/bias_0 instead.\n",
      "INFO:tensorflow:Summary name dense_2/bias:0_grad is illegal; using dense_2/bias_0_grad instead.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "####\n",
    "#Give the index of the minimum element of a list\n",
    "####\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "learning_rate = 0.01\n",
    "hidden_units = 32\n",
    "input_vector_size = 5\n",
    "\n",
    "def get_grads_and_vars(cost):\n",
    "    tvars = tf.trainable_variables() #Get a list of all trainable variables\n",
    "    grads = tf.gradients(cost, tvars) #Get the gradients of all trainable variables wrt cost\n",
    "    grads_and_vars = list(zip(grads, tvars)) #list because zip iterator needs to be read twice\n",
    "    return grads_and_vars\n",
    "\n",
    "def create_summary(loss, grads_and_vars):                                             \n",
    "    tf.summary.scalar(\"loss\", loss) #Create scalar plot of loss\n",
    "    for g, v in grads_and_vars:                                                       \n",
    "        print(g,v)\n",
    "        tf.summary.histogram(v.name, v) #Plot histrogram of weights and bias                       \n",
    "        tf.summary.histogram(v.name + '_grad', g) #Plot histogram of gradients          \n",
    "    return tf.summary.merge_all() #Merge the summary operators into a single summary operator\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, input_vector_size], name='x')\n",
    "y = tf.placeholder(tf.int32, [None], name='y')\n",
    "\n",
    "# Stacked dense layers example\n",
    "output = tf.layers.dense(x, hidden_units)\n",
    "output = tf.nn.tanh(output)\n",
    "output = tf.layers.dense(output, hidden_units)\n",
    "output = tf.nn.tanh(output)\n",
    "output = tf.layers.dense(output, input_vector_size)\n",
    "output = tf.nn.softmax(output, axis=1)\n",
    "\n",
    "y_onehot = tf.one_hot(y, input_vector_size)\n",
    "cross_entropy_loss = tf.reduce_mean(-tf.reduce_sum(y_onehot*tf.log(output+1e-10), reduction_indices=[1]))\n",
    "\n",
    "grads_and_vars = get_grads_and_vars(cross_entropy_loss)\n",
    "#This line is changed because gradients are already calculated\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_op = optimizer.apply_gradients(grads_and_vars)\n",
    "\n",
    "summary_op = create_summary(cross_entropy_loss, grads_and_vars)\n",
    "writer = tf.summary.FileWriter('train', sess.graph) #Create a writer for tensorboard and save at ./train/\n",
    "\n",
    "\n",
    "def generate_data(amount):\n",
    "    x = np.random.uniform(low=-10, high=10, size=(amount, input_vector_size))\n",
    "    y = np.argmin(x, axis=-1)\n",
    "    return x, y\n",
    "\n",
    "x_data, y_data = generate_data(100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(50):\n",
    "        feed = {x:x_data, y:y_data}\n",
    "        #Set to trace all metadata for tensorboard\n",
    "        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "        run_metadata = tf.RunMetadata()\n",
    "        _, summary = sess.run([train_op, summary_op], feed,\n",
    "                               options=run_options,\n",
    "                               run_metadata=run_metadata)\n",
    "        #Get the summaries and log it as the 'i'th run\n",
    "        writer.add_summary(summary, i)\n",
    "        #Get metadata such as memory usage and computation time\n",
    "        writer.add_run_metadata(run_metadata, 'step %d' % i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
